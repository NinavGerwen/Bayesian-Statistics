---
title: "Final Assignment - Bayesian Statistics"
author: "Nina van Gerwen (1860852)"
date: "15/06/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Bayesian Statistics is a field of statistics that is based on another
interpretation of probabilities. In Bayesian Statistics, a probability can be
viewed as a quantification of personal belief in an event (source). In the current 
paper, we will investigate the predictive ability of Social Economical Status (SES) and
Verbal Ability (VA) on intelligence as measured by the Intelligence Quotient (IQ) through
Bayesian methods and statistics. 


```{r Loading the data, include=FALSE}
library(haven)

dat <- read_sav(file = "Week6Data2.sav")

dat <- dat[, 2:4]

dat$verbal <- dat$verbal - mean(dat$verbal)
```

### Research Questions

In this paper, we will aim to research whether IQ can be predicted by both
SES and VA. To answer this research question, we have come up with a total of
three hypotheses.

Hypotheses:
- IQ in students is predicted by both their Verbal Ability, while statistically
controlling for SES, and their SES, while controlling for their Verbal Ability, 
where people with a higher Verbal Ability or SES will have higher IQ scores.

$$ H_1 : \beta_1 > 0, \beta_2 > 0$$

- IQ in students is predicted only by their Verbal Ability, while statistically
controlling for SES, where people with a higher Verbal Ability
will have higher IQ scores.

$$ H_2: \beta_1 > 0, \beta_2 \approx 0 $$
- IQ in students is predicted only by their SES, while statistically controlling
for Verbal Ability, where people with a higher self-reported SES will have
higher predicted IQ scores.

$$ H_3: \beta_1 \approx 0, \beta_2 > 0 $$

OPTIONAL INTERACTION EFFECT
- The effect of Verbal Ability on IQ depends on self-reported SES, where people 
who have a higher self-reported SES will have a greater effect of Verbal Ability 
on IQ than people with lower self-reported SES scores.

$$ H_3: \beta_1, \beta_2, \beta_3 > 0 $$

## Method

### Description of the data

For our dataset, we chose one gained from Multivariate Statistics. The dataset 
contains a total of three variables from 400 high school students and contains
no missing data. The variables are Verbal Ability, which ranges from 2.26 to 4
(*M* = 3.39, *SD* = 0.38), self-reported SES on a scale from 0 to 60 
(*M* = 32.78, *SD* = 9.66) and IQ, which ranged from 85.5 to 143 (*M* = 118.2, *SD* = 11.92). 
When visually 
inspecting the histograms of the variables, we find that SES and IQ seem to follow
a normal distribution. Verbal Ability, however, seems to be slightly skewed to the
left which could be explained by a ceiling effect. For all analyses, 
we grand mean centered the variable of Verbal Ability in order
to aid convergence and interpretation. 

### Statistical Analyses

To answer our research questions and hypotheses, we will run a total of three Bayesian linear
regression analyses. In the first two analyses, we will use Gibbs Sampling with both
an uninformative (LR1) and informative (LR2) prior for the estimation of
our regression coefficients. For the final analysis, regression coefficients will be estimated
through an independent Metropolis-Hastings (MH) Sampler (LR3), where the proposal distribution is a
Student's *t*-distribution with 1 degree of freedom in order to maximize the uncertainty in the tails.

### Prior distributions

IQ (b1): known to be a variable that has a mean of 100, sd of 15.
SES (b2): check categorical distribution in society and try to mimic that in
the self-report way
variance (vari): 
intercept??

- nvm, need priors for mu/sigma.. not their distribution (LEL)

### Model Diagnostics

First, we will assess the convergence of all analyses through visual inspection
of both traceplots and autocorrelation plots. (Because the analyses consist of
only one chain, we can not use the Gelman-Rubin statistic. Furthermore, the MC
error is based on a test and visual tests are preferred)

Second, we will assess whether the different methods of sampling gave  
different results. If they do not seem to differ, we will try to choose
the best model to continue with.

-- other things to say perhaps
only continue 
with the results gained from Gibbs Samplers 
with informative priors. (reasons)

Finally, for the chosen model, we will assess whether the residuals of our model are normally
distributed through use of the posterior predictive *p*-value and discrepancy measures
with the following original test statistic: 

$$ T = (\mu - m_1)^2$$
where $\mu$ is the mean and $m_1$ is the median of the distribution of the residuals. 
The reasoning behind the statistic is that the more skewed a distribution is, the 
larger the difference between the mean and median will be. However, this difference
can be either negative (in a left-skewed distribution) or positive (in a right-skewed distribution). 
Hence, we square the difference in order to always gain a positive value. Now, the larger
the statistic is, the larger the absolute difference between mean and median, and
the more skewed a distribution supposedly is. 

To investigate whether our original statistic works, we will compare the results 
to a posterior predictive *p*-value with the known statistic of skewness:

$$ \tilde{\mu}_3 = \frac{\Sigma_i^N(X_i - \bar{X})^3}{(N - 1) \cdot \sigma^3} $$
where $\bar{X}$ is the mean of the residuals and $\sigma$ the standard deviation
of the residuals. Skewness is known to measure the asymmetry of a distribution. Therefore, we will use it 
to see whether the results are similar to the results of our original test statistic.

### Model comparison

Besides answering the hypotheses, we also want to see which hypothesis has the
most support. To do this, we will compare the hypotheses (and their corresponding
statistical models) through the use of the Bayes factor. When comparing the
hypotheses, their priors distributions will be fractional, where the size of the 
fraction is decided by the following formula: 

$$ b = \frac{J}{N} $$

where *b* is the denominator of the fraction, *J* is the number of independent constraints in the hypothesis and
*N* is sample size. However, we will conduct a sensitivity analysis with varying numbers of *J* to
see the influence it has on the Bayes Factor. 

```{r The Gibbs/MH Sampler function, include=FALSE}
## For the input of the Gibbs/MH Sampler, one should provide: 
        ## a vector of continuous variables that serves as your dependent variable
        ## two vectors of continuous variables that serve as your independent variables
        ## a burn.in period (by default 1000 iterations)
        ## a total amount of iterations (by default 10000)
        ## the initial values for every regression coefficient
        ## a method function that specifies per parameter whether they would 
        ## like to have it sampled through a Gibbs Sampler or Metropolis Hastings algorithm
          ## by default, it assumes a Gibbs sampler for every regression coefficient, 
          ## and if you want to redefine it to a MH, you should change the 0 to a 1
          ## for the corresponding regression coefficient you want to change
              ## Examples: 
                ## method = c(0, 1, 0) will make it so that b1 is done through MH
GibbsSampler <- function(Y, X1, X2, burn.in = 1000, iterations = 10000, 
                         initial.values = c(1, 1, 1), method = c("0", "0", "0")) {
  ## First, the function should require ggplot2 as it is used to create traceplots later
  library(ggplot2)
  ## First create space for all the regression coefficients
  b0 <- rep(NA, iterations)
  b1 <- rep(NA, iterations)
  b2 <- rep(NA, iterations)
  vari <- rep(NA, iterations)
  
  ## Then assume the specified initial values
  b0[1] <- initial.values[1]
  b1[1] <- initial.values[2]
  b2[1] <- initial.values[3]
  vari[1] <- 1
  N <- length(Y)
 
  ## Now, we will start the sampling, it should start at the second element
  ## because the first element will be the initial values
  for(i in 2:iterations) {
    
    ## In every loop, We first update the intercept:
    ## Check whether the intercept should be sampled through Gibbs or MH
    if(method[1] == "0") {
      
      ## If through Gibbs Sampler:
      
      ## First, we update the parameters of the intercept (mean and standard deviation) with the following formula:
    
      mean.0.post <- (sum(Y - (b1[i - 1]*X1 - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1] + 
                        (mu0 / sigma0)) / ((N / vari[i - 1]) + (1 / sigma0))
      sd.0.post <- 1 / ((N / vari[i - 1]) + (1 / sigma0))

      ## Then through rnorm we randomly sample from a normal distribution the next value for b0 with the
      ## above calculated mean and standard deviation
      b0[i] <- rnorm(n = 1, mean = mean.0.post, sd = sqrt(sd.0.post))
    } else {
      
      ## If through MH:
      ## First, we gain a proposal value through multiplying the lm estimate
      ## with a random sampled value of a T-distribution with df = 1
      proposal_value <- 115.45 * rt(1, df = 1)
      ## Then calculate the acceptance ratio
      accept_ratio <- (dnorm(proposal_value, mean = 115.45, sd = 19.96)/dnorm(b0[i - 1], mean = 115.45, sd = 19.96)) * 
        (dt(b0[i - 1], df = 1)/dt(proposal_value, df = 1))
      ## Then see whether we accept the proposed value
      if(runif(1,0,1) < accept_ratio) {
        b0[i] <- proposal_value } else { b0[i] <- b0[i - 1] }
    }
 
    ## Then, every loop, we will update the coefficient of the first independent variable
    
    ## Again, check whether Gibbs or MH:
    if(method[2] == "0") {
    
      ## If through Gibbs:
      
      ## We can update the mean and standard deviation of the regression coefficient for the first variable
      ## using the updated value of the intercept
      mean.1.post <- ((sum(X1*(Y - b0[i] - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1]) + 
                        (mu1 / sigma1)) / ((sum((X1^2), na.rm = TRUE)/vari[i - 1]) + (1 / sigma1))
      sd.1.post <- 1/((sum((X1^2), na.rm = TRUE) / vari[i - 1]) + (1 / sigma1))
    
      ## Then again we randomly sample a new b1 through rnorm
      b1[i] <- rnorm(n = 1, mean = mean.1.post, sd = sqrt(sd.1.post))
      
    } else {
      ## If through MH:
      ## First, we gain a proposal value in a similar fashion as above
      proposal_value <- 11.73 * rt(1, df = 1)
      ## Calculate acceptance ratio
      accept_ratio <- (dnorm(proposal_value, mean = 11.73, sd = 1.46)/dnorm(b1[i - 1], mean = 11.73, sd = 1.46)) * 
        (dt(b1[i - 1], df = 1)/dt(proposal_value, df = 1))
      ## Then see whether we accept the proposed value
      if(runif(1,0,1) < accept_ratio) {
        b1[i] <- proposal_value } else { b1[i] <- b1[i - 1] }
    }
    
    ## Then, every loop, we update the regression coefficient of the second variable
    
    ## First check whether Gibbs or MH:

    if(method[3] == "0") {
      
      ## If through Gibbs:
    
      ## We update the mean and standard deviation of the regression coefficient for the second variable
      ## using the updated value of both the intercept and b1
      mean.2.post <- (sum(X2*(Y - b0[i] - b1[i]*X1), na.rm = TRUE)/vari[i - 1] + 
                        (mu2 / sigma2)) / ((sum(X2^2, na.rm = TRUE)/vari[i - 1]) + (1 / sigma2))
      sd.2.post <- 1/((sum(X2^2, na.rm = TRUE) / vari[i - 1]) + (1 / sigma2))
   
      ## Then, we randomly sample a new b2
      b2[i] <- rnorm(n = 1, mean = mean.2.post, sd = sqrt(sd.2.post))
    } else {
      ## If through MH:
      ## First, we gain a proposal value
      proposal_value <- 0.08 * rt(1, df = 1)
      ## Calculate acceptance ratio
      accept_ratio <- (dnorm(proposal_value, mean = 0.08, sd = 0.06)/dnorm(b2[i - 1], mean = 0.08, sd = 0.06)) * 
        (dt(b2[i - 1], df = 1)/dt(proposal_value, df = 1))
      ## Then see whether we accept the proposed value
      if(runif(1,0,1) < accept_ratio) {
        b2[i] <- proposal_value } else { b2[i] <- b2[i - 1] }
    }
    
    ## Finally, we update the parameters for the distribution of our variance using all above updated values
    a.post <- (N / 2) + a.prior
    b.post <- (sum((Y - (b0[i] + b1[i]*X1 + b2[i]*X2))^2, na.rm = TRUE) / 2) + b.prior

    ## And randomly sample a new variance again through rgamma
    vari[i] <- 1/rgamma(n = 1, shape = a.post, rate = b.post)
      ## We use the inverse of the randomly sample because we want the value of the variance, and not the precision.
  }
  
  ## Then, we remove the values of the burn-in
  b0 <- b0[-c(1:burn.in)]
  b1 <- b1[-c(1:burn.in)]
  b2 <- b2[-c(1:burn.in)]
  vari <- vari[-c(1:burn.in)]
  
  ## We also want the posterior distributions (histograms) of all parameters
  par(mfrow = c(2,2))
  hist(b0, breaks = 50)
  abline(v = mean(b0), col = "blue")
  hist(b1, breaks = 50)
  abline(v = mean(b1), col = "blue")
  hist(b2, breaks = 50)
  abline(v = mean(b2), col = "blue")
  hist(vari, breaks = 50)
  
  ## We want a dataframe consisting of all the sampled parameter values
  data_frame <- as.data.frame(cbind(b0, b1, b2, vari, iter = 1:length(b0)))
  
  ## And we want traceplots for all the parameters to assess convergence
  traceplot1 <- ggplot(data_frame, aes(x=iter, y=b0)) + 
    geom_line() + 
    labs(title="b0 trace")
  traceplot1
  
  traceplot2 <- ggplot(data_frame, aes(x=iter, y=b1)) + 
    geom_line() + 
    labs(title="b1 trace")
  traceplot2
  
  traceplot3 <- ggplot(data_frame, aes(x=iter, y=b2)) + 
    geom_line() + 
    labs(title="b2 trace")
  traceplot3
  
  traceplot4 <- ggplot(data_frame, aes(x=iter, y=vari)) + 
    geom_line() + 
    labs(title="var trace")
  traceplot4
  
  ## We create a list of all the output we want
  list_of_output <- list(data_frame, traceplot1, traceplot2, traceplot3, traceplot4)
  
  ## And finally, we ask the function to return this list of output
  return(list_of_output)
}

## Note, the above function only works if you first set all priors
## (i.e., a.prior, b.prior, mu0, mu1, mu2, sigma0, sigma1, sigma2)
```

## Results

```{r include=FALSE}
## Analysis 1: Uninformative prior + Gibbs -------------------------------------

## First set a seed for reproducibility
set.seed(3)

## Then set all priors to uninformative
a.prior <- 0.001
b.prior <- 0.001

mu0 <- 1
sigma0 <- 10000

mu1 <- 1
sigma1 <- 10000

mu2 <- 1
sigma2 <- 10000

## Then run the GibbsSampler function with our data
set.seed(3)
Analysis.1<- GibbsSampler(dat$IQ, dat$verbal, dat$SES, burn.in = 2500, 
                            iterations = 12500, initial.values = c(1, 1, 1))

## Analysis 2: Informative prior + Gibbs ---------------------------------------

## Set priors to earlier decided upon values
a.prior <- 0.001
b.prior <- 0.001

mu0 <- 1
sigma0 <- 10000

mu1 <- 1
sigma1 <- 10000

mu2 <- 1
sigma2 <- 10000

Analysis.2 <- GibbsSampler(dat$IQ, dat$verbal, dat$SES, burn.in = 5000, 
                            iterations = 25000, initial.values = c(1, 1, 1))

## Analysis 3: Independent MH Sampler ------------------------------------------

Analysis.3 <- GibbsSampler(dat$IQ, dat$verbal, dat$SES, burn.in = 5000, 
                            iterations = 25000, initial.values = c(1, 1, 1),
                            method = c("1", "1", "1"))
```

### Parameter estimates

For our LR1 analysis, we found an intercept of 118.32 (95% *C.I.*: [114.49; 122.14]).
For the effect of Verbal Ability on IQ, we found a regression coefficient of 11.93
(95% *C.I.*: [9.08; 14.77]) and for the effect of SES on IQ, we found
a regression coefficient of 0.003 (95% *C.I.*: [-0.11; 0.12]).

-- interpretation --

For our LR2 analysis, we found XXX.

-- interpretation --

For the final LR3 analysis, we found very similar results to the first analysis 
with an intercept of 118.73 (95% *C.I.*: [114.49; 122.14]) and two regression
coefficients of 12.00 (95% *C.I*: [9.08; 14.77]) and 0.054 (95% *C.I.*: [-0.11; 0.12]).

-- interpretation -- 

```{r Para estimates, include=FALSE}
## FIRST ANALYSIS --------------------------------------------------------------
## First, subset the dataframe from the list of output
LR1.dataframe <- Analysis.1[[1]]
## Now, to get the expected value and credible intervals of every regression coefficient:
mean(LR1.dataframe$b0)
quantile(LR1.dataframe$b0, probs = c(0.025, 0.975))
mean(LR1.dataframe$b1)
quantile(LR1.dataframe$b1, probs = c(0.025, 0.975))
mean(LR1.dataframe$b2)
quantile(LR1.dataframe$b2, probs = c(0.025, 0.975))

## SECOND ANALYSIS -------------------------------------------------------------
LR2.dataframe <- Analysis.2[[1]]
## THIRD ANALYSIS --------------------------------------------------------------
LR3.dataframe <- Analysis.3[[1]]
mean(LR3.dataframe$b0)
quantile(LR1.dataframe$b0, probs = c(0.025, 0.975))
mean(LR3.dataframe$b1)
quantile(LR1.dataframe$b1, probs = c(0.025, 0.975))
mean(LR3.dataframe$b2)
quantile(LR1.dataframe$b2, probs = c(0.025, 0.975))
```
### Model diagnostics

```{r Autocorrelation plot, include=FALSE}
## Autocorrelation plot function:
    ## For input: the autocorrelation plot should require a vector
    ## and the total amount of lags they wish to have (by default 50)
autocorrelationplot <- function(V, lag = 50) {
  ## First, it creates space for the values of the autocorrelations
  autocors <- rep(NA, lag)
  ## Then, a for loop is created that lasts up to the specified lag
  for(i in 1:lag) {
    ## First, it creates two versions of the given vector, where
    ## in the first version, there are i * 0's added at the end
    ## and in the second version, there are i * 0's added at the beginning
    V1 <- c(V, rep(0, i))
    V2 <- c(rep(0, i), V)
    ## Then, the two versions are put into a matrix
    matrix <- cbind(V1, V2)
    ## Then, we remove the i elements at the top and the bottom
      ## This is done so that the two vectors are of the same length
      ## and the values for which there is no lagged version are removed
          ## example: the third sampled value can not have a lag greater than 3
    matrix <- head(matrix, -i)
    matrix <- tail(matrix, -i)
    ## Finally, we ask for the correlation between the two vectors
    ## and put it in the created space
    autocors[i] <- abs(cor(matrix[, 1], matrix[, 2])[1])
  }
  
  ## Then, the autocorrelation plot is created with the lag on x-axis
  ## and the correlation values on the y-axis
  autocorplot <- plot(1:lag, autocors, ylim = c(0, 1))
  return(autocorplot)
}
```

```{r Convergence checks, include=FALSE}
par(mfrow = c(2, 2))
## Convergence for the first analysis ------------------------------------------
    ## TRACEPLOTS
Analysis.1[[2]]
Analysis.1[[3]]
Analysis.1[[4]]
Analysis.1[[5]]
    ## AUTOCOR PLOTS
autocorrelationplot(V = LR1.dataframe$b0)
autocorrelationplot(V = LR1.dataframe$b1)
autocorrelationplot(V = LR1.dataframe$b2)
autocorrelationplot(V = LR1.dataframe$vari)

## Convergence for the second analysis -----------------------------------------
Analysis.2[[2]]
Analysis.2[[3]]
Analysis.2[[4]]
Analysis.2[[5]]
    ## AUTOCOR PLOTS
autocorrelationplot(V = LR2.dataframe$b0)
autocorrelationplot(V = LR2.dataframe$b1)
autocorrelationplot(V = LR2.dataframe$b2)
autocorrelationplot(V = LR2.dataframe$vari)

## Convergence for the third analysis ------------------------------------------
Analysis.3[[2]]
Analysis.3[[3]]
Analysis.3[[4]]
Analysis.3[[5]]
    ## AUTOCOR PLOTS
autocorrelationplot(V = LR3.dataframe$b0)
autocorrelationplot(V = LR3.dataframe$b1)
autocorrelationplot(V = LR3.dataframe$b2)
autocorrelationplot(V = LR3.dataframe$vari)
```

```{r PP P-values, include=FALSE}
## First, we set a seed for reproducibility
set.seed(3)
## Then, we create an empty matrix with equal rows to our original dataset
## and as many columns as we would like datasets
sim.data <- matrix(data = NA, nrow = 400, ncol = 1000)

## Then, we create space for the regression coefficients that we will sample
## so that we can use these to calculate the residuals later
b0_t <- rep(NA, 1000)
b1_t <- rep(NA, 1000)
b2_t <- rep(NA, 1000)
s2_t <- rep(NA, 1000)

## Now, we can start the data simulation through a for loop
for(i in 1:1000) {
  ## In every loop, we sample one value from all our regression coefficients
  b0_t[i] <- sample(LR1.dataframe$b0, 1)
  b1_t[i] <- sample(LR1.dataframe$b1, 1)
  b2_t[i] <- sample(LR1.dataframe$b2, 1)
  s2_t[i] <- sample(LR1.dataframe$vari, 1)

  ## Then, we create a column of simulated Y values that use the sampled
  ## regression coefficients through rnorm, where the mean is their 
  ## expected value and the sd is the sampled variance
  sim.data[, i] <- rnorm(400, mean = b0_t[i] + b1_t[i]*dat$verbal + b2_t[i]*dat$SES, sd = sqrt(s2_t[i]))
}

## Now, as a discrepancy measure, we calculate the residuals for every
## simulated Y values using their corresponding sampled regression coefficients
residuals_sim <- matrix(data = NA, nrow = 400, ncol = 1000)

for(i in 1:1000) {
  residuals_sim[, i] <- sim.data[, i] - b0_t[i] - b1_t[i]*dat$verbal - b2_t[i]*dat$SES
}

## For the observed dataset, we also calculate the residuals a 1000 times
## using the sampled regression coefficients
residuals_obs <- matrix(data = NA, nrow = 400, ncol = 1000)

for(i in 1:1000){
  residuals_obs[, i] <- dat$IQ - b0_t[i] - b1_t[i]*dat$verbal - b2_t[i]*dat$SES
}

## ORIGINAL TEST STATISTIC -----------------------------------------------------

## Then, for every column of residuals (simulated and observed), we calculate
## our original test statistic
sim_skewness <- rep(NA, 1000)
for(i in 1:1000) {
  sim_skewness[i] <- (mean(residuals_sim[, i]) - median(residuals_sim[, i]))^2
}

obs_skewness <- rep(NA, 1000)

for(i in 1:1000) {
  obs_skewness[i] <- (mean(residuals_obs[, i]) - median(residuals_obs[, i]))^2
}

## And we calculate the Posterior Predictive P-value by finding the proportion
## of times that our statistic is larger in the simulated residuals than 
## in the observed residuals
ppp_value <- sum(sim_skewness > obs_skewness)/1000
ppp_value

## SKEWNESS STATISTIC ----------------------------------------------------------

library(moments)

obs_skew <- rep(NA, 1000)
sim_skew <- rep(NA, 1000)

## Again, we calculate skewness a thousand times for both the observed
## and simulated residuals
for(i in 1:1000) {
  obs_skew[i] <- skewness(residuals_obs[, i])
  sim_skew[i] <- skewness(residuals_sim[, i])
}

## And again, we calculate the posterior predictive p-value by finding the
## proportion of times that the skewness for simulated residuals is larger
## than the skewness in the observed residuals
comparison_ppp <- sum(sim_skew > obs_skew)/1000
comparison_ppp
```

### Model comparison

```{r Bayes Factor Calculations, include=FALSE}
library(MASS)

## Create a linear mo
linearmodel <- lm(IQ ~ verbal + SES, data = dat)

## OPTIES VOOR SAMPLING:
    ## OPTIE 1
Means <- linearmodel$coefficients[2:3]
Sigma <- vcov(linearmodel)[2:3, 2:3]

    ## OPTIE 2
Means <- c(mean(LR1.dataframe$b1), mean(LR1.dataframe$b2))
Sigma <- cov(LR1.dataframe[2:3])

## Now, we will sample from the posterior and prior distributions

    ## First set a seed
set.seed(3)
    ## Then sample from the posterior, using the above specified means and sigma
posterior <- mvrnorm(100000, mu = Means, Sigma = Sigma)
    
    ## Calculate the fractional value
b <- 6/400 ## 3 regression coefficients: 3 means, 3 variances --> 6

    ## Then sample from the prior, using means = 0 and sigma = sigma/b
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/b)

## Finally, we can calculate the Bayes Factor for all our hypotheses
BF.H1 <- (sum(posterior[, 1] > 0 & posterior[, 2] > 0)/100000) /
  (sum(prior[, 1] > 0 & prior[, 2] > 0)/100000)
  
BF.H2 <- (sum(posterior[, 1] > 0 & posterior[, 2] < .1 & posterior[, 2] > -.1)/100000) / 
  (sum(prior[, 1] > 0 & prior[, 2] < .1 & prior[, 2] > -.1)/100000)

BF.H3 <- (sum(posterior[, 2] > 0 & posterior[, 1] < .1 & posterior[, 1] > -.1)/100000) / 
  (sum(prior[, 2] > 0 & prior[, 1] < .1 & prior[, 1] > -.1)/100000)
```



## Discussion

- Differences between Bayesian and Frequentist analyses (relevant to the context
of the research questions)