---
title: "Final Assignment - Bayesian Statistics"
author: "Nina van Gerwen (1860852)"
date: "15/06/2022"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
bibliography: references.bib
nocite: '@*'
csl: apa.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Bayesian Statistics is a field of statistics where the interpretation of
a probability differs from the interpretation that is used in
Frequentist Statistics, where a probability is seen as an event's
relative frequency over time (HÃ¡jek, 2019). Instead, in Bayesian
Statistics, a probability is viewed as a quantification of personal
belief in an event (de Finetti, 2017).

Methods used in the field of Bayesian statistics have seen a rise 21st
century due to more powerful software and algorithms like Markov Chain
Monte Carlo (Fienberg, 2006). Furthermore, Bayesian Statistics is also
often seen as a possible answer to the replication crisis (e.g., Cumming, 2014; Romero, 2019) 
that plagues multiple disciplinary fields - a few of which are psychology, medicine
and economics. This is partly because statistical issues that come from
Frequentist significance testing (e.g., *p*-hacking) are not present in
Bayesian Statistics.

The goal of the current paper is to get more acquainted with the
Bayesian framework by performing a multiple linear regression through
Bayesian methods. To be specific, we will investigate the predictive
ability of Social Economical Status (SES) and Verbal Ability on
intelligence as measured by the Intelligence Quotient (IQ).

-- hier nog even toevoegen wat er nog meer gedaan wordt -- 

```{r Loading the data, include=FALSE}
library(haven)

dat <- read_sav(file = "Week6Data2.sav")

dat <- dat[, 2:4]

dat$verbal <- dat$verbal - mean(dat$verbal)
```

### Hypotheses

In this paper, we will aim to research whether IQ can be predicted by
SES and Verbal Ability. To answer this research question, we have come
up with a total of three hypotheses.

-   IQ in students is predicted by both their Verbal Ability, while
    statistically controlling for SES, and their SES, while controlling
    for their Verbal Ability, where people with a higher Verbal Ability
    or higher SES are predicted to have higher IQ scores.

    $$ H_1 : \beta_1 > 0, \beta_2 > 0 $$

-   IQ in students is predicted only by their Verbal Ability, while
    statistically controlling for SES, where people with a higher Verbal
    Ability are predicted to have higher IQ scores.
    $$ H_2: \beta_1 > 0, \beta_2 \approx 0 $$

-   IQ in students is predicted only by their SES, while statistically
    controlling for Verbal Ability, where people with a higher
    self-reported SES are predicted to have higher IQ scores.
    $$ H_3: \beta_1 \approx 0, \beta_2 > 0 $$

## Method

### Description of the data

```{r Descriptive statistics, include = FALSE}
## For ranges and means:
summary(dat)
## For standard deviations:
sd(dat$verbal)
sd(dat$SES)
sd(dat$IQ)
## For visual inspection of the histograms:
hist(dat$verbal, breaks = 20)
hist(dat$SES)
hist(dat$IQ)
```

For our dataset, we chose one gained from Multivariate Statistics. The
dataset contains a total of three variables from 400 high school
students and contains no missing data. The variables are Verbal Ability,
which ranges from 2.26 to 4 (*M* = 3.39, *SD* = 0.38), self-reported SES
on a scale from 0 to 60 (*M* = 32.78, *SD* = 9.66) and IQ, which ranged
from 85.5 to 143 (*M* = 118.2, *SD* = 11.92). When visually inspecting
the histograms of the variables, we find that SES and IQ seem to follow
a normal distribution. Verbal Ability, however, seems to be slightly
skewed to the left which could be explained by a ceiling effect. For all
analyses, we grand mean centered the variable of Verbal Ability in order
to aid convergence and interpretation.

### Statistical analyses

To answer our research questions and test the hypotheses, we ran two
Bayesian linear regression analyses. In the first analysis, we used
Gibbs Sampling with uninformative prior distributions (LR1) for the
estimation of our regression coefficients. Below, the choice for our
prior distributions is further explained. For the second analysis,
regression coefficients were estimated through an independent
Metropolis-Hastings (MH) Sampler (LR2), where the proposal distribution
was a Student's *t*-distribution with 1 degree of freedom in order to
maximize uncertainty in the tails of the distribution. The means and
standard deviations of the proposal distributions were gained from the
Frequentist linear regression equivalent of our analyses.

#### Prior distributions

As stated before, we used uninformative priors for LR1. There were two
main reasons for this. First and foremost, the goal of the paper was to
get more acquainted with the Bayesian framework. Hence, the analyses
were of secondary importance and we did not wish to bias them with our
expectations. Secondly, although there is historical data available on
the effect of some of our independent variables on intelligence,
research in this area tends to be of a more psychometric nature. For
example, the effect of Verbal Ability on IQ can be found in factor
analyses such as the *g* factor model, where general mental ability is
divided in 7 subfactors of intelligence - one of which is verbal
comprehension (Spearman, 1904). However, due to the different methods,
we were unsure how to translate this information to the specification of
our priors. Instead, we chose for uninformative priors. This means that
for the intercept and regression coefficients, the prior distributions
had an extremely large variance and a mean that then becomes obsolete
(i.e., we simply set it to 1). Finally, for the variance coefficient,
which follows a gamma distribution, the scaling and rating parameter
were set close to 0.

### Model diagnostics

To investigate the quality of our analyses, we did the following
inspections. First, we assessed the convergence of all analyses through
visual inspection of both trace- and autocorrelation plots. Second, we
evaluated which of the two different sampling methods performed better.
To compare the two models and choose the best one, we used the Deviance
Information-Criterion (DIC).

Finally, for the chosen model, we gauged whether the residuals of our
model were normally distributed through use of the posterior predictive
*p*-value and discrepancy measures with the following original test
statistic:

$$ T = (\mu - m_1)^2$$ where $\mu$ is the mean and $m_1$ is the median
of the distribution of the residuals. The reasoning behind the statistic
is that the more skewed a distribution is, the larger the difference
between the mean and median is. However, this difference can be either
negative (in a left-skewed distribution) or positive (in a right-skewed
distribution). Hence, we square the difference in order to always gain a
positive value. Now, the larger the statistic is, the larger the
absolute difference between mean and median, and the more skewed a
distribution supposedly is.

To investigate whether our original statistic works correctly, we will
compare the results to a posterior predictive *p*-value that uses the
known statistic of skewness:

$$ \tilde{\mu}_3 = \frac{\Sigma_i^N(X_i - \bar{X})^3}{(N - 1) \cdot \sigma^3} $$
where $\bar{X}$ is the mean of the residuals, $\sigma$ the standard
deviation of the residuals and *N* is the total number of residuals.
Skewness is known to measure the asymmetry of a distribution. Therefore,
we used it to see whether the results are similar to the results of our
original test statistic.

### Hypothesis support

For our hypotheses, we researched which hypothesis had the most support.
To do this, we compared the three hypotheses through use of the Bayes'
Factor. When comparing the hypotheses through the Bayes' Factor, the
priors distributions were fractional, where the size of the fraction was
decided by the following formula:

$$ b = \frac{J}{N} $$

where *b* is the denominator of the fraction, *J* is the number of
independent constraints in the hypotheses and *N* is sample size.
Furthermore, we conducted a sensitivity analysis with varying numbers of
*J* to see the influence it has on the Bayes' Factor.

```{r The Gibbs/MH Sampler function, include=FALSE}
## For the input of the Gibbs/MH Sampler, one should provide: 
        ## a vector of continuous variables that serves as your dependent variable
        ## two vectors of continuous variables that serve as your independent variables
        ## a burn.in period (by default 1000 iterations)
        ## a total amount of iterations (by default 10000)
        ## the initial values for every regression coefficient
        ## a method function that specifies per parameter whether they would 
        ## like to have it sampled through a Gibbs Sampler or Metropolis Hastings algorithm
          ## by default, it assumes a Gibbs sampler for every regression coefficient, 
          ## and if you want to redefine it to a MH, you should change the 0 to a 1
          ## for the corresponding regression coefficient you want to change
              ## Examples: 
                ## method = c(0, 1, 0) will make it so that b1 is done through MH
GibbsSampler <- function(Y, X1, X2, burn.in = 1000, iterations = 10000, 
                         initial.values = c(1, 1, 1), method = c("0", "0", "0")) {
  ## First, the function should require ggplot2 as it is used to create traceplots later
  library(ggplot2)
  ## First create space for all the regression coefficients
  b0 <- rep(NA, iterations)
  b1 <- rep(NA, iterations)
  b2 <- rep(NA, iterations)
  vari <- rep(NA, iterations)
  
  ## Then assume the specified initial values
  b0[1] <- initial.values[1]
  b1[1] <- initial.values[2]
  b2[1] <- initial.values[3]
  vari[1] <- 1
  N <- length(Y)
 
  ## Now, we will start the sampling, it should start at the second element
  ## because the first element will be the initial values
  for(i in 2:iterations) {
    
    ## In every loop, We first update the intercept:
    ## Check whether the intercept should be sampled through Gibbs or MH
    if(method[1] == "0") {
      
      ## If through Gibbs Sampler:
      
      ## First, we update the parameters of the intercept (mean and standard deviation) with the following formula:
    
      mean.0.post <- (sum(Y - (b1[i - 1]*X1 - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1] + 
                        (mu0 / sigma0)) / ((N / vari[i - 1]) + (1 / sigma0))
      sd.0.post <- 1 / ((N / vari[i - 1]) + (1 / sigma0))

      ## Then through rnorm we randomly sample from a normal distribution the next value for b0 with the
      ## above calculated mean and standard deviation
      b0[i] <- rnorm(n = 1, mean = mean.0.post, sd = sqrt(sd.0.post))
    } else {
      
      ## If through MH:
      ## First, we gain a proposal value through adding the lm point estimate
      ## with the standard error of the estimate times 
      ## a random sampled value of a T-distribution with df = 1
      proposal_value <- 115.45920 + 1.95990 * rt(1, df = 1)
      ## Then calculate the acceptance ratio
      accept_ratio <- (dnorm(proposal_value, mean = 115.45, sd = 19.96)/dnorm(b0[i - 1], mean = 115.45, sd = 19.96)) * 
        (dt(b0[i - 1], df = 1)/dt(proposal_value, df = 1))
      ## Then see whether we accept the proposed value
      if(runif(1,0,1) < accept_ratio) {
        b0[i] <- proposal_value } else { b0[i] <- b0[i - 1] }
    }
 
    ## Then, every loop, we will update the coefficient of the first independent variable
    
    ## Again, check whether Gibbs or MH:
    if(method[2] == "0") {
    
      ## If through Gibbs:
      
      ## We can update the mean and standard deviation of the regression coefficient for the first variable
      ## using the updated value of the intercept
      mean.1.post <- ((sum(X1*(Y - b0[i] - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1]) + 
                        (mu1 / sigma1)) / ((sum((X1^2), na.rm = TRUE)/vari[i - 1]) + (1 / sigma1))
      sd.1.post <- 1/((sum((X1^2), na.rm = TRUE) / vari[i - 1]) + (1 / sigma1))
    
      ## Then again we randomly sample a new b1 through rnorm
      b1[i] <- rnorm(n = 1, mean = mean.1.post, sd = sqrt(sd.1.post))
      
    } else {
      ## If through MH:
      ## First, we gain a proposal value in a similar fashion as above
      proposal_value <- 11.73647 + 1.45593 * rt(1, df = 1)
      ## Calculate acceptance ratio
      accept_ratio <- (dnorm(proposal_value, mean = 11.73, sd = 1.46)/dnorm(b1[i - 1], mean = 11.73, sd = 1.46)) * 
        (dt(b1[i - 1], df = 1)/dt(proposal_value, df = 1))
      ## Then see whether we accept the proposed value
      if(runif(1,0,1) < accept_ratio) {
        b1[i] <- proposal_value } else { b1[i] <- b1[i - 1] }
    }
    
    ## Then, every loop, we update the regression coefficient of the second variable
    
    ## First check whether Gibbs or MH:

    if(method[3] == "0") {
      
      ## If through Gibbs:
    
      ## We update the mean and standard deviation of the regression coefficient for the second variable
      ## using the updated value of both the intercept and b1
      mean.2.post <- (sum(X2*(Y - b0[i] - b1[i]*X1), na.rm = TRUE)/vari[i - 1] + 
                        (mu2 / sigma2)) / ((sum(X2^2, na.rm = TRUE)/vari[i - 1]) + (1 / sigma2))
      sd.2.post <- 1/((sum(X2^2, na.rm = TRUE) / vari[i - 1]) + (1 / sigma2))
   
      ## Then, we randomly sample a new b2
      b2[i] <- rnorm(n = 1, mean = mean.2.post, sd = sqrt(sd.2.post))
    } else {
      ## If through MH:
      ## First, we gain a proposal value
      proposal_value <- 0.08399 + 0.05738 * rt(1, df = 1)
      ## Calculate acceptance ratio
      accept_ratio <- (dnorm(proposal_value, mean = 0.08, sd = 0.06)/dnorm(b2[i - 1], mean = 0.08, sd = 0.06)) * 
        (dt(b2[i - 1], df = 1)/dt(proposal_value, df = 1))
      ## Then see whether we accept the proposed value
      if(runif(1,0,1) < accept_ratio) {
        b2[i] <- proposal_value } else { b2[i] <- b2[i - 1] }
    }
    
    ## Finally, we update the parameters for the distribution of our variance using all above updated values
    a.post <- (N / 2) + a.prior
    b.post <- (sum((Y - (b0[i] + b1[i]*X1 + b2[i]*X2))^2, na.rm = TRUE) / 2) + b.prior

    ## And randomly sample a new variance again through rgamma
    vari[i] <- 1/rgamma(n = 1, shape = a.post, rate = b.post)
      ## We use the inverse of the randomly sample because we want the value of the variance, and not the precision.
  }
  
  ## Then, we remove the values of the burn-in
  b0 <- b0[-c(1:burn.in)]
  b1 <- b1[-c(1:burn.in)]
  b2 <- b2[-c(1:burn.in)]
  vari <- vari[-c(1:burn.in)]
  
  ## We also want the posterior distributions (histograms) of all parameters
  par(mfrow = c(2,2))
  hist(b0, breaks = 50)
  abline(v = mean(b0), col = "blue")
  hist(b1, breaks = 50)
  abline(v = mean(b1), col = "blue")
  hist(b2, breaks = 50)
  abline(v = mean(b2), col = "blue")
  hist(vari, breaks = 50)
  
  ## We want a dataframe consisting of all the sampled parameter values
  data_frame <- as.data.frame(cbind(b0, b1, b2, vari, iter = 1:length(b0)))
  
  ## And we want traceplots for all the parameters to assess convergence
  traceplot1 <- ggplot(data_frame, aes(x=iter, y=b0)) + 
    geom_line() + 
    labs(title="b0 trace")
  traceplot1
  
  traceplot2 <- ggplot(data_frame, aes(x=iter, y=b1)) + 
    geom_line() + 
    labs(title="b1 trace")
  traceplot2
  
  traceplot3 <- ggplot(data_frame, aes(x=iter, y=b2)) + 
    geom_line() + 
    labs(title="b2 trace")
  traceplot3
  
  traceplot4 <- ggplot(data_frame, aes(x=iter, y=vari)) + 
    geom_line() + 
    labs(title="var trace")
  traceplot4
  
  ## We create a list of all the output we want
  list_of_output <- list(data_frame, traceplot1, traceplot2, traceplot3, traceplot4)
  
  ## And finally, we ask the function to return this list of output
  return(list_of_output)
}

## Note, the above function only works if you first set all priors
## (i.e., a.prior, b.prior, mu0, mu1, mu2, sigma0, sigma1, sigma2)
```

## Results

```{r include=FALSE}
## Analysis 1: Uninformative prior + Gibbs -------------------------------------

## First set a seed for reproducibility
set.seed(3)

## Then set all priors to uninformative
a.prior <- 0.001
b.prior <- 0.001

mu0 <- 1
sigma0 <- 10000

mu1 <- 1
sigma1 <- 10000

mu2 <- 1
sigma2 <- 10000

## Then run the GibbsSampler function with our data

Analysis.1<- GibbsSampler(dat$IQ, dat$verbal, dat$SES, burn.in = 2500, 
                            iterations = 12500, initial.values = c(1, 1, 1))

## Analysis 2: Independent MH Sampler ------------------------------------------

Analysis.2 <- GibbsSampler(dat$IQ, dat$verbal, dat$SES, burn.in = 5000, 
                            iterations = 25000, initial.values = c(1, 1, 1),
                            method = c("1", "1", "1"))
```

### Parameter estimates

For analysis LR1, we found an intercept of 118.32 (95% *C.I.*: [114.47;
122.21]). For the effect of Verbal Ability on IQ, we found a regression
coefficient of 11.93 (95% *C.I.*: [9.11; 14.77]) and for the effect of
SES on IQ, we found a regression coefficient of 0.003 (95% *C.I.*:
[-0.11; 0.11]).

The intercept parameter tells us that when a student has a self-reported
SES of 0 and an average Verbal Ability, he has a predicted IQ of 118.
With the credible interval, we also know with 95% certainty that the IQ
of the student lies between 114.47 and 122.21.

For Verbal Ability, the results mean that for every one unit increase
above the mean a student scores on Verbal Ability, their predicted IQ
increases on average by 11.93. Furthermore, we can state that there is a
95% probability that the true effect of Verbal Ability on IQ lies
between 9.11 and 14.77.

As for the effect of self-reported SES on IQ, we found that the point
estimate is immensely close to 0 and there is a 95% probability that the
true parameter lies between -0.11 and 0.11. These results both indicate
that SES cannot predict IQ.

For our LR2 analysis, we found very similar results to the first
analysis with an intercept of 115.94 (95% *C.I.*: [105.75; 129.46]) and
two regression coefficients of 11.89 (95% *C.I*: [9.84; 14.09]) and 0.08
(95% *C.I.*: [-0.006; 0.17]). These values can be interpreted in the
same way as the estimated values in LR1.

```{r Parameter estimates & Credible Intervals, include=FALSE}
## FIRST ANALYSIS --------------------------------------------------------------
## First, subset the dataframe from the list of output
LR1.dataframe <- Analysis.1[[1]]
## Now, to get the expected value and credible intervals of every regression coefficient:
mean(LR1.dataframe$b0)
quantile(LR1.dataframe$b0, probs = c(0.025, 0.975))
mean(LR1.dataframe$b1)
quantile(LR1.dataframe$b1, probs = c(0.025, 0.975))
mean(LR1.dataframe$b2)
quantile(LR1.dataframe$b2, probs = c(0.025, 0.975))

## SECOND ANALYSIS -------------------------------------------------------------
LR2.dataframe <- Analysis.2[[1]]

mean(LR2.dataframe$b0)
quantile(LR2.dataframe$b0, probs = c(0.025, 0.975))
mean(LR2.dataframe$b1)
quantile(LR2.dataframe$b1, probs = c(0.025, 0.975))
mean(LR2.dataframe$b2)
quantile(LR2.dataframe$b2, probs = c(0.025, 0.975))
```

### Model diagnostics

```{r Autocorrelation plot, include=FALSE}
## Autocorrelation plot function:
    ## For input: the autocorrelation plot should require a vector
    ## and the total amount of lags they wish to have (by default 50)
autocorrelationplot <- function(V, lag = 50) {
  ## First, it creates space for the values of the autocorrelations
  autocors <- rep(NA, lag)
  ## Then, a for loop is created that lasts up to the specified lag
  for(i in 1:lag) {
    ## First, it creates two versions of the given vector, where
    ## in the first version, there are i * 0's added at the end
    ## and in the second version, there are i * 0's added at the beginning
    V1 <- c(V, rep(0, i))
    V2 <- c(rep(0, i), V)
    ## Then, the two versions are put into a matrix
    matrix <- cbind(V1, V2)
    ## Then, we remove the i elements at the top and the bottom
      ## This is done so that the two vectors are of the same length
      ## and the values for which there is no lagged version are removed
          ## example: the third sampled value can not have a lag greater than 3
    matrix <- head(matrix, -i)
    matrix <- tail(matrix, -i)
    ## Finally, we ask for the correlation between the two vectors
    ## and put it in the created space
    autocors[i] <- abs(cor(matrix[, 1], matrix[, 2])[1])
  }
  
  ## Then, the autocorrelation plot is created with the lag on x-axis
  ## and the correlation values on the y-axis
  autocorplot <- plot(1:lag, autocors, ylim = c(0, 1))
  return(autocorplot)
}
```

```{r Convergence checks, include=FALSE}
par(mfrow = c(2, 2))
## Convergence for the first analysis ------------------------------------------
    ## TRACEPLOTS
Analysis.1[[2]]
Analysis.1[[3]]
Analysis.1[[4]]
Analysis.1[[5]]
    ## AUTOCOR PLOTS
autocorrelationplot(V = LR1.dataframe$b0)
autocorrelationplot(V = LR1.dataframe$b1)
autocorrelationplot(V = LR1.dataframe$b2)
autocorrelationplot(V = LR1.dataframe$vari)

par(mfrow = c(2,2))
## Convergence for the second analysis -----------------------------------------
    ## TRACEPLOTS
Analysis.2[[2]]
Analysis.2[[3]]
Analysis.2[[4]]
Analysis.2[[5]]
    ## AUTOCOR PLOTS
autocorrelationplot(V = LR2.dataframe$b0)
autocorrelationplot(V = LR2.dataframe$b1)
autocorrelationplot(V = LR2.dataframe$b2)
autocorrelationplot(V = LR2.dataframe$vari)
```

#### Convergence

When investigating the convergence of the two analyses, we found the
following results. For LR1, the traceplots of all coefficients showed
very sound convergence. The autocorrelation plots showed two notable
results. For the intercept and SES regression coefficient, the
autocorrelation started at high values around .90 and then in a monotone
decreasing fashion reached 0 at lag 45. The other autocorrelation plots
remained constant at values around 0 for every lag. Although the
autocorrelation could be improved upon, we feel it is still safe to
assume the first model converged properly when also taking the
traceplots into account.

For the second analysis, LR2, the traceplots for the coefficients of the
independent variables showed sound convergence. The traceplot for the
intercept, while still acceptable, showed that sometimes quite extreme
values were accepted and convergence of this coefficient could be
improved. The traceplot of the variance, however, showed that there
might have been some issues as the plot shows only half of what you
would expect from a good convergence. This can be explained by the fact
that the algorithm somehow wanted to accept negative values, however,
due to a nature of a variance, this is impossible. Furthermore, some
very extreme values were accepted for variances, which might lead to
issues if we were to use this model for model assumption checks through
the posterior predictive *p*-value. In contrast to this result, the
autocorrelation plots all showed constant values close to 0. Hence, we
still assumed that the model converged adequately.

-- MISSCHIEN NOG mULTIPLE CHAINS TOEVOEGEN -- An important limitation to
keep in mind when assessing the convergence for the two analyses, is
that they all consisted of only one chain. This means that it is
possible that the analyses are stuck at a local maximum. However,
considering that the analyses gave similar results that seem to agree
with the results of a Frequentist linear regression, we feel safe to
assume that the analyses were not stuck at local maxima.

#### Model comparison

```{r DIC calculations, include=FALSE}
## To calculate the DIC, we create a function that as input
## requires the outcome variable, the independent variables and the
## posterior distribution (i.e., the gibbs sampled values)
DIC <- function(y, x1, x2, posterior) {
  ## First, we create a function that will give the log likelihood 
  ## with as input the sampled parameters
      ## The log likelihood is calculated by summing the results of the
      ## density function of all our outcome variables with the 
      ## predicted values as the mean and the variance coefficient
      ## as the standard deviation with log = TRUE, because we want the
      ## log likelihood, not the likelihood
  loglik <- function(parameters) {sum(dnorm(y, mean = parameters[1] + 
                                            parameters[2]*x1 + parameters[3]*x2, 
                                            sd = sqrt(parameters[4]), log = TRUE))}
  
  ## For the calculation of Dhat, we need to get the means of the
  ## Gibbs sampled values, we get these through colMeans and save them
  mean_parameters <- colMeans(posterior)
  
      ## Then, we can calculate Dhat by using our log likelihood function
      ## on the saved mean values and multiplying it by -2
  Dhat <- -2 * loglik(mean_parameters)
  
  ## For Dbar, which is the average likelihood over the posterior distribution
  ## we do the following:
      ## We get all individual log likelihoods for every row of sampled
      ## parameter values, and multiply these all by -2 and then
      ## take the mean of all the individual log likelihoods
  Dbar <- mean(- 2 * (apply(posterior, 1, loglik)))

  ## Then finally, we can calculate the DIC by taking -Dhat and
  ## adding 2 * Dbar to this!
  DIC_value <- -Dhat + 2 * Dbar
  
  ## And lastly, of course, the function should return this value
  return(DIC_value)
}

## Now, we can use the function on both our Gibbs and MH sampled analyses
Gibbs_DIC <- DIC(dat$IQ, dat$verbal, dat$SES, LR1.dataframe[1:4])
Gibbs_DIC

MH_DIC <- DIC(dat$IQ, dat$verbal, dat$SES, LR2.dataframe[1:4])
MH_DIC
```

Comparing the two models through use of the DIC, we calculated that LR1
had a DIC value of 3061.983, whereas LR2 had a DIC value of 3168.71
($\Delta_{DIC} = 106.73$. With these results, we can conclude that LR1,
the analysis that made use of a Gibbs Sampler, performed better than the
MH sampler as it has a DIC that is much lower. Hence, we only continue
discussing the results that were gained from LR1. This model was also
used for the calculation of the posterior predictive *p*-values, because
of this reason and the extreme outliers in the variance coefficient that
were present in the other model.

#### Distribution of the residuals

```{r PP P-values, include=FALSE}
## First, we set a seed for reproducibility
set.seed(3)
## Then, we create an empty matrix with equal rows to our original dataset
## and as many columns as we would like datasets
sim.data <- matrix(data = NA, nrow = 400, ncol = 5000)

## Then, we create space for the regression coefficients that we will sample
## so that we can use these to calculate the residuals later
b0_t <- rep(NA, 5000)
b1_t <- rep(NA, 5000)
b2_t <- rep(NA, 5000)
s2_t <- rep(NA, 5000)

## Now, we can start the data simulation through a for loop
for(i in 1:5000) {
  ## In every loop, we sample one value from all our regression coefficients
  b0_t[i] <- sample(LR1.dataframe$b0, 1)
  b1_t[i] <- sample(LR1.dataframe$b1, 1)
  b2_t[i] <- sample(LR1.dataframe$b2, 1)
  s2_t[i] <- sample(LR1.dataframe$vari, 1)

  ## Then, we create a column of simulated Y values that use the sampled
  ## regression coefficients through rnorm, where the mean is their 
  ## expected value and the sd is the sampled variance
  sim.data[, i] <- rnorm(400, mean = b0_t[i] + b1_t[i]*dat$verbal + b2_t[i]*dat$SES, sd = sqrt(s2_t[i]))
}

## Now, as a discrepancy measure, we calculate the residuals for every
## simulated Y values using their corresponding sampled regression coefficients
residuals_sim <- matrix(data = NA, nrow = 400, ncol = 5000)

for(i in 1:5000) {
  residuals_sim[, i] <- sim.data[, i] - b0_t[i] - b1_t[i]*dat$verbal - b2_t[i]*dat$SES
}

## For the observed dataset, we also calculate the residuals a 1000 times
## using the sampled regression coefficients
residuals_obs <- matrix(data = NA, nrow = 400, ncol = 5000)

for(i in 1:5000){
  residuals_obs[, i] <- dat$IQ - b0_t[i] - b1_t[i]*dat$verbal - b2_t[i]*dat$SES
}

## ORIGINAL TEST STATISTIC -----------------------------------------------------

## Then, for every column of residuals (simulated and observed), we calculate
## our original test statistic
sim_skewness <- rep(NA, 5000)
for(i in 1:5000) {
  sim_skewness[i] <- (mean(residuals_sim[, i]) - median(residuals_sim[, i]))^2
}

obs_skewness <- rep(NA, 5000)

for(i in 1:5000) {
  obs_skewness[i] <- (mean(residuals_obs[, i]) - median(residuals_obs[, i]))^2
}

## And we calculate the Posterior Predictive P-value by finding the proportion
## of times that our statistic is larger in the simulated residuals than 
## in the observed residuals
ppp_value <- sum(sim_skewness > obs_skewness)/5000
ppp_value

## SKEWNESS STATISTIC ----------------------------------------------------------

library(moments)

obs_skew <- rep(NA, 5000)
sim_skew <- rep(NA, 5000)

## Again, we calculate skewness a thousand times for both the observed
## and simulated residuals
for(i in 1:5000) {
  obs_skew[i] <- skewness(residuals_obs[, i])
  sim_skew[i] <- skewness(residuals_sim[, i])
}

## And again, we calculate the posterior predictive p-value by finding the
## proportion of times that the skewness for simulated residuals is larger
## than the skewness in the observed residuals
comparison_ppp <- sum(sim_skew > obs_skew)/5000
comparison_ppp

## Visual inspection of the residuals in observed dataset
hist(dat$IQ - 118.3201 - 11.92829 * dat$verbal - 0.003478761 * dat$SES)
```

Checking the assumption of normality, we found posterior predictive
*p*-values of \<.001 for both our original and skewness statistic. These
*p*-values mean the following: for our original statistic, it entails
that when using the same sampled coefficients from their posterior
distribution to calculate the residuals, we find a larger difference
between the mean and median of these residuals when using our observed
dataset compared to using a simulated dataset. For the skewness
statistic, it means also means that the calculated residuals in our
observed dataset were practically always more skewed than the residuals
in a simulated dataset.

From these results, we can conclude two things. Firstly, our original
statistic works properly to measure the skewness of a distribution as it
gave the same results as the skewness statistic. Secondly, we can
conclude that the residuals most likely do not follow a normal
distribution and instead are skewed. This result is further strengthened
by visual inspection of the residuals, which shows that the direction of
the skew is to the right.

### Hypothesis support

```{r Bayes Factor Calculations, include=FALSE}
library(MASS)

## To be able to get the multivariate posterior density, we get
## the means and covariance matrix of the posterior distributions 
## from the IV regression coefficients
Means <- c(mean(LR1.dataframe$b1), mean(LR1.dataframe$b2))
Sigma <- cov(LR1.dataframe[2:3])

## Now, we will sample from the posterior and prior distributions

    ## First set a seed
set.seed(3)
    ## Then sample from the posterior, using the above specified means and sigma
posterior <- mvrnorm(100000, mu = Means, Sigma = Sigma)
    
    ## Calculate the fractional value
b <- 6/400 ## 3 regression coefficients: 3 means, 3 variances --> 6

    ## Then sample from the prior, using means = 0 and sigma = sigma/b
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/b)

## Finally, we can calculate the Bayes Factor for all our hypotheses
## by stating the correct statements

## For hypothesis 1: 
## For both the posterior and prior, we want the proportion of sampled values where both
## regression coefficients are higher than 0 and divide them in the correct way
BF.H1 <- (sum(posterior[, 1] > 0 & posterior[, 2] > 0)/100000) /
  (sum(prior[, 1] > 0 & prior[, 2] > 0)/100000)

## For hypothesis 2:
## For both posterior and prior, the first regression coefficient should be higher
## than 0 and the second one should be close to zero (i.e., -.1 < x < .1)
BF.H2 <- (sum(posterior[, 1] > 0 & posterior[, 2] < .1 & posterior[, 2] > -.1)/100000) / 
  (sum(prior[, 1] > 0 & prior[, 2] < .1 & prior[, 2] > -.1)/100000)

## For hypothesis 3:
## Exactly the other way around from hypothesis 2
BF.H3 <- (sum(posterior[, 2] > 0 & posterior[, 1] < .1 & posterior[, 1] > -.1)/100000) / 
  (sum(prior[, 2] > 0 & prior[, 1] < .1 & prior[, 1] > -.1)/100000)

## Sensitivity analysis for H1 and H2
BFH1_values <- rep(NA, 50)

for(i in 1:50) {
  b <- as.numeric(i) / 400
  prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/as.numeric(b))
  BFH1_values[i] <- (sum(posterior[, 1] > 0 & posterior[, 2] > 0)/100000) /
  (sum(prior[, 1] > 0 & prior[, 2] > 0)/100000)
}

plot(x = 1:50, y = BFH1_values, ylim = c(0, 5))

BFH2_values <- rep(NA, 50)

for(i in 1:50) {
  b <- as.numeric(i) / 400
  prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/as.numeric(b))
  BFH2_values[i] <- (sum(posterior[, 1] > 0 & posterior[, 2] < .1 & posterior[, 2] > -.1)/100000) / 
  (sum(prior[, 1] > 0 & prior[, 2] < .1 & prior[, 2] > -.1)/100000)
  
}

plot(x = 1:50, y = BFH2_values)
```

Calculating the support for each hypothesis gave the following results:
$H_1$ had a Bayes' Factor of 1.99. $H_2$ had a Bayes' Factor of 10.83.
$H_3$ had a Bayes' Factor of 0. From these results, we can infer the
following. $H_3$, which stated that there was no effect of Verbal
Ability on IQ, has practically no support. This means that the other
hypotheses also have infinitely more support than $H_3$. In other words,
we can very safely conclude that there most likely is a positive effect
of Verbal Ability on IQ. Furthermore, when comparing $H_1$ to $H_2$ we
discover that $H_2$ has 5.45 times more support than $H_1$. Hence, there
is more evidence that SES has no effect on IQ than that SES has a
positive effect on IQ. However, the sensitivity analyses of the first
two hypotheses should be taken into account. Namely, although the Bayes'
Factor of $H_1$ remains constant, the Bayes' Factor for $H_2$ seems to
be monotone decreasing to a limit of 4 with increasing values of *J*.
Such a result was to be expected due to the fact that $H_2$ uses an
equality constraint, for which the value of *J* matter more. This would
entail that $H_2$ has only around 2 - 2.5 times more support than $H_1$,
which affects the strength of our conclusion. Nonetheless, we can still
state that there most likely is no effect of SES on IQ.

## Discussion

In the current paper, we have investigated whether intelligence can be predicted
by Verbal Ability and self-reported SES in students using Bayesian methods. The results have shown
in multiple ways the following conclusion: Verbal Ability is able to predict IQ, where a
higher Verbal Ability is associated with a higher IQ, whereas self-reported SES 
was not able to predict IQ, as stated in $H_2$.

Now, let us hypothesize what the results would be if we had used Frequentist methods.
A normal linear regression would have shown a non-significant effect of SES and
a significant effect of Verbal Ability. Thus, we would most likely reach a similar
conclusion as we had reached through the Bayesian methods. However, by using
Bayesian methods, we were able to find support for the conclusion in multiple ways.
Namely, through among other things, the size of the credible intervals and the Bayes' factor.
In other words, we would argue that the Bayesian framework allows for much more flexibility.

--

This flexibility extends to multiple things: for example, the analyses 
that can be done with either Gibbs Sampling using either un- or informative
priors or with MH sampler (which can also be done in multiple ways). All these
choices can affect the results and conclusions, however, they also allow you to
finepick them in order to fit your research in the best way possible. 

-- OTHER WAYS BAYESIAN ANALYSIS DIFFERED --

Besides flexibility for gaining evidence for our conclusion, the Bayesian methods
also differ in how we ended up forming the conclusion. The Bayesian framework strafes
away from the dichotomy of significance testing where there either is a significant
effect or there is not. Instead, the Bayesian framework tells us about the
support every hypothesis has and what is most likely, given the data.
--

As stated in the Introduction, Bayesian Statistics is sometimes seen as a possible
answer to the replication crisis. We are unsure, however, whether we agree with 
this statement. Although issues such as *p*-hacking are resolved, other issues could
arise. An example would be choosing a posterior predictive *p*-value that
gives you the result you are looking for. In both cases, the real answer to the
replication crisis would be open science and pre-registration, not the exact field
of statistics you use. Nonetheless, by programming multiple Bayesian methods ourselves
(e.g., Gibbs Sampler or Bayes' Factor), we did come to realize the amount of 
theory that is behind all these methods that we had to understand in
order to be able to use them and answer a frankly quite simple research question. 
We believe that if everybody dove this deep into the methods required, science as a whole would benefit compared
to the current situation in Frequentist Statistics, where a linear regression requires only a single line
of code with a bare minimum amount of understanding.

-   answer of the research question

-   Differences between Bayesian and Frequentist analyses (relevant to
    the context of the research questions 
    --\> praten over hoe het
    programmeren van een simpele regressie mij doet beseffen hoeveel
    theorie en wetenschap er eigenlijk achter zit (ook al word het nu
    overal gebruikt met een simpele line of code / 3 klikjes) --\>
    bayesian kwam op als een soort antwoord op de replicatie crisis in
    meerdere sectoren, maar wellicht zit het antwoord niet per se in
    welke statistiek je doet maar in het goed gebruiken ervan (zie
    hierboven)

\pagebreak

## References


