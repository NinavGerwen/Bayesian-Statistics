---
title: "Final Assignment - Gibbs Sampler"
author: "Nina van Gerwen (1860852)"
date: "3/1/2022"
output: pdf_document
---

```{r}
## First, all priors have to be set

a.prior <- 0.001
b.prior <- 0.001

mu0 <- 1
sigma0 <- 10000

mu1 <- 1
sigma1 <- 10000

mu2 <- 1
sigma2 <- 10000

GibbsSampler <- function(Y, X1, X2, burn.in = 1000, iterations = 10000, initial.values = c(1.5, 2, 0.5)) {
  ## First create space
  b0 <- rep(0, iterations)
  b1 <- rep(0, iterations)
  b2 <- rep(0, iterations)
  vari <- rep(0, iterations)
  
  ## Then assume the specified initial values
  b0[1] <- initial.values[1]
  b1[1] <- initial.values[2]
  b2[1] <- initial.values[3]
  vari[1] <- 1
  N <- length(Y)
  ## In every iteration, the values of the parameters of the conditional
  ## posteriors should be calculated as functions of the prior parameters,
  ## the data and the current values of the other model parameters

  for(i in 2:iterations) {
    
    ## First, we should update the parameters of the intercept (mean and standard deviation) with the following formula:
    ## 10000 because informative prior?
    mean.0.post <- (sum(Y - b1[i - 1]*X1 - b2[i - 1]*X2, na.rm = TRUE)/vari[i - 1] + (mu0 / sigma0)) / ((N / vari[i - 1]) + (mu0 / sigma0))
    sd.0.post <- 1 / ((N / vari[i - 1]) + (mu0 / sigma0))

    ## Then through rnorm we randomly sample from a normal distribution the next value for b0 with the
    ## above calculated mean and standard deviation
    b0[i] <- rnorm(n = 1, mean = mean.0.post, sd = sd.0.post)
 
    ## Now, we can update the mean and standard deviation of the regression coefficient for the first variable
    ## using the updated value of the intercept
    mean.1.post <- (sum(X1*(Y - b0[i] - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1] + (mu1 / sigma1)) / sum(X1^2, na.rm = TRUE)/vari[i - 1] + (mu1 / sigma1)
    sd.1.post <- 1/((sum(X1^2, na.rm = TRUE) / vari[i - 1]) + (mu1 / sigma1))
    
    ## Then again we randomly sample a new b1
    b1[i] <- rnorm(n = 1, mean = mean.1.post, sd = sd.1.post)
    
    ## Afterwards, we update the mean and standard deviation of the regression coefficient for the second variable
    ## using the updated value of both the intercept and b1
    mean.2.post <- (sum(X2*(Y - b0[i] - b1[i]*X1), na.rm = TRUE)/vari[i - 1] + (mu2 / sigma2)) / sum(X2^2, na.rm = TRUE)/vari[i - 1] + (mu2 / sigma2)
    sd.2.post <- 1/((sum(X2^2, na.rm = TRUE) / vari[i - 1]) + (mu2 / sigma2))
   
    ## Then, we randomly sample a new b2
    b2[i] <- rnorm(n = 1, mean = mean.2.post, sd = sd.2.post)
    
    ## Finally, we update the parameters for the distribution of our standard deviation using all above updated values
    a.post <- N / 2 + a.prior
    b.post <- sum((Y - (b0[i] + b1[i]*X1 + b2[i]*X2))^2, na.rm = TRUE) / 2 + b.prior

    ## And randomly sample a new variance again for
    vari[i] <- 1/rgamma(n = 1, shape = a.post, rate = b.post)
  }
  
  b0 <- b0[-c(1:burn.in)]
  b1 <- b1[-c(1:burn.in)]
  b2 <- b2[-c(1:burn.in)]
  vari <- vari[-c(1:burn.in)]
  
  ## Now, we also want the posterior distributions (histograms) of all parameters
  par(mfrow = c(2,2))
  hist(b0)
  abline(v = mean(b0), col = "blue")
  hist(b1)
  abline(v = mean(b1), col = "blue")
  hist(b2)
  abline(v = mean(b2), col = "blue")
  hist(vari)
  
  
  data_frame <- as.data.frame(cbind(b0, b1, b2, vari, iter = 1:length(b0)))
  
  traceplot1 <- ggplot(data_frame, aes(x=iter, y=b0)) + 
    geom_line() + 
    labs(title="b0 trace")
  traceplot1
  
  traceplot2 <- ggplot(data_frame, aes(x=iter, y=b1)) + 
    geom_line() + 
    labs(title="b1 trace")
  traceplot2
  
  traceplot3 <- ggplot(data_frame, aes(x=iter, y=b2)) + 
    geom_line() + 
    labs(title="b2 trace")
  traceplot3
  
  traceplot4 <- ggplot(data_frame, aes(x=iter, y=vari)) + 
    geom_line() + 
    labs(title="var trace")
  traceplot4
  
  
  list_of_output <- list(data_frame, traceplot1, traceplot2, traceplot3, traceplot4)
  return(list_of_output)
}

```

```{r}
## test the sampler
library(haven)
data <- read_sav("Exercise 2 - Data.sav")
str(data)

summary(lm(attitude ~ extraversion + agreeableness, data = data))

GibbsSampler(data$attitude, data$extraversion, data$agreeableness, burn.in = 1000, iterations = 10000)

A <- 1:100
B <- 101:200

cbind(A, B)
```


