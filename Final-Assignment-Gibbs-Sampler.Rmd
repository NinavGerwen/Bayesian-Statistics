---
title: "Final Assignment - Gibbs Sampler"
author: "Nina van Gerwen (1860852)"
date: "3/1/2022"
output: pdf_document
---

## The Gibbs Sampler with an option for Metropolis-Hastings

Because we assume that all predictors are normally distributed, the Gibbs
Sampler can work in the following fashion.

```{r Gibbs + MH Sampler}
## First, all priors have to be set

a.prior <- 0.001
b.prior <- 0.001

mu0 <- 1
sigma0 <- 10000

mu1 <- 1
sigma1 <- 10000

mu2 <- 1
sigma2 <- 10000

## For MH: we need a proposal and posterior distribution, however, since
## it has to be a linear regression, this will always be a normal distribution
## so we can use dnorm() for this, but the proposal distribution should be specified!

## For the input of the Gibbs Sampler, one should provide: 
        ## a vector of continuous variables that serves as your dependent variable
        ## two vectors of continuous variables that serve as your independent variables
        ## a burn.in period (by default 1000 iterations)
        ## a total amount of iterations (by default 10000)
        ## the initial values for every regression coefficient
        ## a method function that specifies per parameter whether they would 
        ## like to have it sampled through a Gibbs Sampler or Metropolis Hastings algorithm
          ## by default, it assumes a Gibbs sampler for every regression coefficient, 
          ## and if you want to redefine it to a MH, you should change the 0 to a 1
          ## for the regression coefficient you want to change
              ## Examples: 
                ## method = c(0, 1, 0) will make it so that b1 is done through MH
GibbsSampler <- function(Y, X1, X2, burn.in = 1000, iterations = 10000, 
                         initial.values = c(1, 1, 1), method = c("0", "0", "0")) {
  ## First, the function should require ggplot2 as it is used to create traceplots later
  library(ggplot2)
  ## First create space
  b0 <- rep(NA, iterations)
  b1 <- rep(NA, iterations)
  b2 <- rep(NA, iterations)
  vari <- rep(NA, iterations)
  
  ## Then assume the specified initial values
  b0[1] <- initial.values[1]
  b1[1] <- initial.values[2]
  b2[1] <- initial.values[3]
  vari[1] <- 1
  N <- length(Y)
 
  ## Now, we will start the sampling, it should start at the second element
  ## because the first element will be the initial values
  for(i in 2:iterations) {
    
    ## In every loop, We first update the intercept:
    ## Check whether the intercept should be sampled through Gibbs or MH
    if(method[1] == "0") {
      
      ## If through Gibbs Sampler:
      
      ## First, we update the parameters of the intercept (mean and standard deviation) with the following formula:
    
      mean.0.post <- (sum(Y - (b1[i - 1]*X1 - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1] + 
                        (mu0 / sigma0)) / ((N / vari[i - 1]) + (1 / sigma0))
      sd.0.post <- 1 / ((N / vari[i - 1]) + (1 / sigma0))

      ## Then through rnorm we randomly sample from a normal distribution the next value for b0 with the
      ## above calculated mean and standard deviation
      b0[i] <- rnorm(n = 1, mean = mean.0.post, sd = sqrt(sd.0.post))
    } else {
      
      ## If through MH:
      ## First, we gain a proposal value
      proposal_value <- 115.45920 + 1.95990 * rt(1, df = 1) ## MOET EEN PLUS ZIJN IPV KEER
      ## Calculate acceptance ratio
      accept_ratio <- (dnorm(proposal_value, mean = 115.45, sd = 19.96)/dnorm(b0[i - 1], mean = 115.45, sd = 19.96)) * 
        (dt(b0[i - 1], df = 1)/dt(proposal_value, df = 1))
      ## Then see whether we accept the proposed value
      if(runif(1,0,1) < accept_ratio) {
        b0[i] <- proposal_value } else { b0[i] <- b0[i - 1] }
    }
 
    ## Then, every loop, we will update the coefficient of the first independent variable
    
    ## Again, check whether Gibbs or MH:
    if(method[2] == "0") {
    
      ## If through Gibbs:
      
      ## We can update the mean and standard deviation of the regression coefficient for the first variable
      ## using the updated value of the intercept
      mean.1.post <- ((sum(X1*(Y - b0[i] - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1]) + 
                        (mu1 / sigma1)) / ((sum((X1^2), na.rm = TRUE)/vari[i - 1]) + (1 / sigma1))
      sd.1.post <- 1/((sum((X1^2), na.rm = TRUE) / vari[i - 1]) + (1 / sigma1))
    
      ## Then again we randomly sample a new b1 through rnorm
      b1[i] <- rnorm(n = 1, mean = mean.1.post, sd = sqrt(sd.1.post))
      
    } else {
      ## If through MH:
      ## First, we gain a proposal value
      proposal_value <- 11.73647 + 1.45593 * rt(1, df = 1)
      ## Calculate acceptance ratio
      accept_ratio <- (dnorm(proposal_value, mean = 11.73, sd = 1.46)/dnorm(b1[i - 1], mean = 11.73, sd = 1.46)) * 
        (dt(b1[i - 1], df = 1)/dt(proposal_value, df = 1))
      ## Then see whether we accept the proposed value
      if(runif(1,0,1) < accept_ratio) {
        b1[i] <- proposal_value } else { b1[i] <- b1[i - 1] }
    }
    
    ## Then, every loop, we update the regression coefficient of the second variable
    
    ## But first check whether Gibbs oR MH:

    if(method[3] == "0") {
      
      ## If through Gibbs:
    
      ## We update the mean and standard deviation of the regression coefficient for the second variable
      ## using the updated value of both the intercept and b1
      mean.2.post <- (sum(X2*(Y - b0[i] - b1[i]*X1), na.rm = TRUE)/vari[i - 1] + 
                        (mu2 / sigma2)) / ((sum(X2^2, na.rm = TRUE)/vari[i - 1]) + (1 / sigma2))
      sd.2.post <- 1/((sum(X2^2, na.rm = TRUE) / vari[i - 1]) + (1 / sigma2))
   
      ## Then, we randomly sample a new b2
      b2[i] <- rnorm(n = 1, mean = mean.2.post, sd = sqrt(sd.2.post))
    } else {
      ## If through MH:
      ## First, we gain a proposal value
      proposal_value <- 0.08399 + 0.05738 * rt(1, df = 1)
      ## Calculate acceptance ratio
      accept_ratio <- (dnorm(proposal_value, mean = 0.08, sd = 0.06)/dnorm(b2[i - 1], mean = 0.08, sd = 0.06)) * 
        (dt(b2[i - 1], df = 1)/dt(proposal_value, df = 1))
      ## Then see whether we accept the proposed value
      if(runif(1,0,1) < accept_ratio) {
        b2[i] <- proposal_value } else { b2[i] <- b2[i - 1] }
    }
    
    ## Finally, we update the parameters for the distribution of our variance using all above updated values
    a.post <- (N / 2) + a.prior
    b.post <- (sum((Y - (b0[i] + b1[i]*X1 + b2[i]*X2))^2, na.rm = TRUE) / 2) + b.prior

    ## And randomly sample a new variance again through rgamma
    vari[i] <- 1/rgamma(n = 1, shape = a.post, rate = b.post)
      ## We use the inverse of the randomly sample because we want the value of the variance, and not the precision.
  }
  
  ## Then, we remove the values of the burn-in
  b0 <- b0[-c(1:burn.in)]
  b1 <- b1[-c(1:burn.in)]
  b2 <- b2[-c(1:burn.in)]
  vari <- vari[-c(1:burn.in)]
  
  ## We also want the posterior distributions (histograms) of all parameters
  par(mfrow = c(2,2))
  hist(b0, breaks = 50)
  abline(v = mean(b0), col = "blue")
  hist(b1, breaks = 50)
  abline(v = mean(b1), col = "blue")
  hist(b2, breaks = 50)
  abline(v = mean(b2), col = "blue")
  hist(vari, breaks = 50)
  
  ## We want a dataframe consisting of all the sampled parameter values
  data_frame <- as.data.frame(cbind(b0, b1, b2, vari, iter = 1:length(b0)))
  
  ## And we want traceplots for all the parameters to assess convergence
  traceplot1 <- ggplot(data_frame, aes(x=iter, y=b0)) + 
    geom_line() + 
    labs(title="b0 trace")
  traceplot1
  
  traceplot2 <- ggplot(data_frame, aes(x=iter, y=b1)) + 
    geom_line() + 
    labs(title="b1 trace")
  traceplot2
  
  traceplot3 <- ggplot(data_frame, aes(x=iter, y=b2)) + 
    geom_line() + 
    labs(title="b2 trace")
  traceplot3
  
  traceplot4 <- ggplot(data_frame, aes(x=iter, y=vari)) + 
    geom_line() + 
    labs(title="var trace")
  traceplot4
  
  ## We create a list of all the output we want
  list_of_output <- list(data_frame, traceplot1, traceplot2, traceplot3, traceplot4)
  
  ## And finally, we ask the function to return this list of output
  return(list_of_output)
}

```

## Autocorrelation plot

Although we already created traceplots to assess convergence, it is always best
to assess it in multiple ways. Hence, I'll make a function that, when given 
all randomly sampled values for a parameter, will return an autocorrelation plot.

```{r Autocor plot}

autocorrelationplot <- function(V, lag = 50) {
  autocors <- rep(NA, lag)
  for(i in 1:lag) {
    V1 <- c(V, rep(0, i))
    V2 <- c(rep(0, i), V)
    matrix <- cbind(V1, V2)
    matrix <- head(matrix, -i)
    matrix <- tail(matrix, -i)
    autocors[i] <- cor(matrix[, 1], matrix[, 2])[1]
  }
  plot(1:lag, autocors)
  return(autocors)
}

autocorrelationplot(V = df$b2)
```

```{r}
## Comparing results to JAGS results
library(haven)
testdat <- read_sav(file = "Week6Data2.sav")
testdat$pass <- as.numeric(testdat$pass)

testdat$verbal <- testdat$verbal - mean(testdat$verbal)

summary(lm(IQ ~ verbal + SES, data = testdat))

set.seed(3)
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("0", "0", "0"))
set.seed(3)
list <- GibbsSampler(testdat$IQ, testdat$verbal, testdat$pass, burn.in = 2500, iterations = 12500, initial.values = c(1, 1, 1))


df <- as.data.frame(list[1])

mean(df$b0)
mean(df$b1)
mean(df$b2)

mean(testdat$IQ)
var(testdat$IQ)
```

Guidelines voor proposal distribution:

niet te breed niet te small... Je kan LM gebruiken om dan de estimate als mean te gebruiken en de standard error te gebruiken als variantie van proposal. (perhaps extra uncertainty toevoegen)

Voor hypothese testen: 

je kan regressie coefficient gebruiken 
(bv. H1: b1 > b2 // H2: b2 < b1 - hier moet je voor echter ze standardizeren)
(bv. H1: b1 > 0 // H2: b2 > 0, H3: b1 & b2 > 0) - ofwel zelf programmeren (met behulp van multivariate normal distribution - posterior en prior) ofwel lm en dan bain

je kan ook meerdere modellen gebruiken: bv een model met 2 predictors, een model met 2 predictors + 1 interactie, etc.

Assessing convergence:

multiple ways, so not just a traceplot (i.e., create autocorrelation plot function)

voor autocorrelation plot:

for-loop t van 1:50
voor elke keer, heb twee vectors van een regressie coefficent,
dan schuif je een vector t plaatsen omhoog (en wellicht n - t'de element
verwijderen OF cor(..., use = 'pairwise.complete.obs')) en dan correlatie calculaten.

Voor eindopdracht:

To convert Rmarkdown to Rfile:

In Rstudio run: knitr::purl.("Mymarkdownfile.Rmd")  

It will create an R file "Mymarkdownfile.R" in your working directory which contains only Rcode. You may want to trim out some markdown junk code that is not needed afterwards. You can also build in that you also produce an R code file  as you knit (see first linkt below).

More info here:
https://bookdown.org/yihui/rmarkdown-cookbook/purl.html

```{r}
set.seed(3)
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("0", "0", "1"))

set.seed(3)
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("0", "1", "0"))

set.seed(3)
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("1", "1", "1"))
```


POSTERIOR PREDICTIVE P-VALUE

```{r}
set.seed(3)

sim.data <- matrix(data = NA, nrow = 400, ncol = 1000)

b0_t <- rep(NA, 1000)
b1_t <- rep(NA, 1000)
b2_t <- rep(NA, 1000)
s2_t <- rep(NA, 1000)

## Generate data
for(i in 1:1000) {
  b0_t[i] <- sample(df$b0, 1)
  b1_t[i] <- sample(df$b1, 1)
  b2_t[i] <- sample(df$b2, 1)
  s2_t[i] <- sample(df$vari, 1)
  
  sim.data[, i] <- rnorm(400, mean = b0_t[i] + b1_t[i]*testdat$verbal + b2_t[i]*testdat$SES, sd = sqrt(s2_t[i]))
}

## Now discrepancy measure
residuals_sim <- matrix(data = NA, nrow = 400, ncol = 1000)

for(i in 1:1000) {
  residuals_sim[, i] <- sim.data[, i] - b0_t[i] - b1_t[i]*testdat$verbal - b2_t[i]*testdat$SES
}

residuals_obs <- matrix(data = NA, nrow = 400, ncol = 1000)

for(i in 1:1000){
  residuals_obs[, i] <- testdat$IQ - b0_t[i] - b1_t[i]*testdat$verbal - b2_t[i]*testdat$SES
}

sim_skewness <- rep(NA, 1000)

## mean - median
for(i in 1:1000) {
  sim_skewness[i] <- (mean(residuals_sim[, i]) - median(residuals_sim[, i]))^2
}

obs_skewness <- rep(NA, 1000)

for(i in 1:1000) {
  obs_skewness[i] <- (mean(residuals_obs[, i])- median(residuals_obs[, i]))^2
}

p_value <- sum(sim_skewness > obs_skewness)/1000
p_value

library(moments)

obs_skew <- rep(NA, 1000)
sim_skew <- rep(NA, 1000)

for(i in 1:1000) {
  obs_skew[i] <- skewness(residuals_obs[, i])
  sim_skew[i] <- skewness(residuals_sim[, i])
}

second_p <- sum(sim_skew > obs_skew)/1000
second_p

```
This means that 24.7% of the time, the mean - median squared of the residuals in a simulated dataset are larger than in the observed dataset. in other words, 24.7% of the time, the residuals in a simulated dataset follow a less normally distributed distribution?

## Bayes Factor

```{r}
library(MASS)

test <- lm(IQ ~ verbal + SES, data = testdat)

test2 <- lm(IQ ~ verbal + SES + verbal*SES, data = testdat)

## Posterior:

set.seed(3)

means <- test$coefficients[2:3]
sigma <- vcov(test)[2:3, 2:3]

post <- mvrnorm(100000, mu = means, Sigma = sigma)

b <- 6/400

prior <- mvrnorm(100000, mu = c(0, 0), Sigma = sigma/b)


BF_H1 <- (sum(post[, 1] > 0)/100000) / (sum(prior[, 1] > 0)/100000)

BF_H2 <- (sum(post[, 2] > 0)/100000) / (sum(prior[, 2] > 0)/100000)

BFH12 <- BF_H1 / BF_H2

(sum(post[, 1] > 0 & post[, 2] < .1 & post[, 2] > -.1)/100000) / (sum(prior[, 1] > 0 & prior[, 2] < .1 & prior[, 2] > -.1)/100000)

(sum(post[, 2] > 0 & post[, 1] < .1 & post[, 1] > -.1)/100000) / (sum(prior[, 2] > 0 & prior[, 1] < .1 & prior[, 1] > -.1)/100000)


```


VRAGEN MORGEN:

- kan je h0: b1 > 0, b2 unconstrained doen?
- sensitivity analysis
- waarom positieve covariance in bayesian, negatieve in lm?
- moet je voor elke bayes factor calculatie opnieuw samplen?

```{r}

BFH2_values <- rep(NA, 50)

set.seed(3)

posterior <- mvrnorm(100000, mu = Means, Sigma = Sigma)

for(i in 1:50) {
  b <- as.numeric(i) / 400
  prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/as.numeric(b))
  BFH2_values[i] <- (sum(posterior[, 1] > 0 & posterior[, 2] < .1 & posterior[, 2] > -.1)/100000) / 
  (sum(prior[, 1] > 0 & prior[, 2] < .1 & prior[, 2] > -.1)/100000)
  
}

plot(x = 1:50, y = BFH2_values)

BFH1_values <- rep(NA,50)

for(i in 1:50) {
  b <- as.numeric(i) / 400
  prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/as.numeric(b))
  BFH1_values[i] <- (sum(posterior[, 1] > 0 & posterior[, 2] > 0)/100000) /
  (sum(prior[, 1] > 0 & prior[, 2] > 0)/100000)
}

plot(x = 1:50, y = BFH1_values, ylim = c(0, 5))
```

