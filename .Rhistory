## Now, as a discrepancy measure, we calculate the residuals for every
## simulated Y values using their corresponding sampled regression coefficients
residuals_sim <- matrix(data = NA, nrow = 400, ncol = 5000)
for(i in 1:5000) {
residuals_sim[, i] <- sim.data[, i] - b0_t[i] - b1_t[i]*dat$verbal - b2_t[i]*dat$SES
}
## For the observed dataset, we also calculate the residuals a 1000 times
## using the sampled regression coefficients
residuals_obs <- matrix(data = NA, nrow = 400, ncol = 5000)
for(i in 1:5000){
residuals_obs[, i] <- dat$IQ - b0_t[i] - b1_t[i]*dat$verbal - b2_t[i]*dat$SES
}
## ORIGINAL TEST STATISTIC -----------------------------------------------------
## Then, for every column of residuals (simulated and observed), we calculate
## our original test statistic
sim_skewness <- rep(NA, 5000)
for(i in 1:5000) {
sim_skewness[i] <- (mean(residuals_sim[, i]) - median(residuals_sim[, i]))^2
}
obs_skewness <- rep(NA, 5000)
for(i in 1:5000) {
obs_skewness[i] <- (mean(residuals_obs[, i]) - median(residuals_obs[, i]))^2
}
## And we calculate the Posterior Predictive P-value by finding the proportion
## of times that our statistic is larger in the simulated residuals than
## in the observed residuals
ppp_value <- sum(sim_skewness > obs_skewness)/5000
ppp_value
## SKEWNESS STATISTIC ----------------------------------------------------------
library(moments)
obs_skew <- rep(NA, 5000)
sim_skew <- rep(NA, 5000)
## Again, we calculate skewness a thousand times for both the observed
## and simulated residuals
for(i in 1:5000) {
obs_skew[i] <- skewness(residuals_obs[, i])
sim_skew[i] <- skewness(residuals_sim[, i])
}
## And again, we calculate the posterior predictive p-value by finding the
## proportion of times that the skewness for simulated residuals is larger
## than the skewness in the observed residuals
comparison_ppp <- sum(sim_skew > obs_skew)/5000
comparison_ppp
## Visual inspection of the residuals in observed dataset
hist(dat$IQ - 118.3201 - 11.92829 * dat$verbal - 0.003478761 * dat$SES)
1/5000
BF1_values <- rep(NA, 50)
BF1_values <- rep(NA, 50)
set.seed(3)
posterior <- mvrnorm(100000, mu = Means, Sigma = Sigma)
for(i in 1:50) {
b <- i / 400
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = sigma/b)
BF1_values[i] <- (sum(posterior[, 1] > 0 & posterior[, 2] < .1 & posterior[, 2] > -.1)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] < .1 & prior[, 2] > -.1)/100000)
}
BF1_values <- rep(NA, 50)
set.seed(3)
posterior <- mvrnorm(100000, mu = Means, Sigma = Sigma)
for(i in 1:50) {
b <- as.numeric(i) / 400
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = sigma/b)
BF1_values[i] <- (sum(posterior[, 1] > 0 & posterior[, 2] < .1 & posterior[, 2] > -.1)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] < .1 & prior[, 2] > -.1)/100000)
}
BF1_values <- rep(NA, 50)
set.seed(3)
posterior <- mvrnorm(100000, mu = Means, Sigma = Sigma)
for(i in 1:50) {
b <- as.numeric(i) / 400
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/as.numeric(b))
BF1_values[i] <- (sum(posterior[, 1] > 0 & posterior[, 2] < .1 & posterior[, 2] > -.1)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] < .1 & prior[, 2] > -.1)/100000)
}
BF1_values
plot(x = 1:50, y = BF1_values)
BF1_values <- rep(NA, 100)
set.seed(3)
posterior <- mvrnorm(100000, mu = Means, Sigma = Sigma)
for(i in 1:100) {
b <- as.numeric(i) / 400
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/as.numeric(b))
BF1_values[i] <- (sum(posterior[, 1] > 0 & posterior[, 2] < .1 & posterior[, 2] > -.1)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] < .1 & prior[, 2] > -.1)/100000)
}
plot(x = 1:100, y = BF1_values)
BFH2_values <- rep(NA, 50)
set.seed(3)
posterior <- mvrnorm(100000, mu = Means, Sigma = Sigma)
for(i in 1:50) {
b <- as.numeric(i) / 400
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/as.numeric(b))
BFH2_values[i] <- (sum(posterior[, 1] > 0 & posterior[, 2] < .1 & posterior[, 2] > -.1)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] < .1 & prior[, 2] > -.1)/100000)
}
plot(x = 1:50, y = BFH2_values)
BFH1_values <- rep(NA,50)
for(i in 1:50) {
b <- as.numeric(i) / 400
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/as.numeric(b))
BFH1_values[i] <- (sum(posterior[, 1] > 0 & posterior[, 2] > 0)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] > 0)/100000)
}
plot(x = 1:50, y = BFH1_values)
plot(x = 1:50, y = BFH1_values, ylim = c(0, 5))
plot(x = 1:50, y = BFH2_values)
library(MASS)
## To be able to get the multivariate posterior density, we get
## the means and covariance matrix of the posterior distributions
## from the IV regression coefficients
Means <- c(mean(LR1.dataframe$b1), mean(LR1.dataframe$b2))
Sigma <- cov(LR1.dataframe[2:3])
## Now, we will sample from the posterior and prior distributions
## First set a seed
set.seed(3)
## Then sample from the posterior, using the above specified means and sigma
posterior <- mvrnorm(100000, mu = Means, Sigma = Sigma)
## Calculate the fractional value
b <- 6/400 ## 3 regression coefficients: 3 means, 3 variances --> 6
## Then sample from the prior, using means = 0 and sigma = sigma/b
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/b)
## Finally, we can calculate the Bayes Factor for all our hypotheses
## by stating the correct statements
## For hypothesis 1:
## For both the posterior and prior, we want the proportion of sampled values where both
## regression coefficients are higher than 0 and divide them in the correct way
BF.H1 <- (sum(posterior[, 1] > 0 & posterior[, 2] > 0)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] > 0)/100000)
## For hypothesis 2:
## For both posterior and prior, the first regression coefficient should be higher
## than 0 and the second one should be close to zero (i.e., -.1 < x < .1)
BF.H2 <- (sum(posterior[, 1] > 0 & posterior[, 2] < .1 & posterior[, 2] > -.1)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] < .1 & prior[, 2] > -.1)/100000)
## For hypothesis 3:
## Exactly the other way around from hypothesis 2
BF.H3 <- (sum(posterior[, 2] > 0 & posterior[, 1] < .1 & posterior[, 1] > -.1)/100000) /
(sum(prior[, 2] > 0 & prior[, 1] < .1 & prior[, 1] > -.1)/100000)
for(i in 1:50) {
b <- as.numeric(i) / 400
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/as.numeric(b))
BFH1_values[i] <- (sum(posterior[, 1] > 0 & posterior[, 2] > 0)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] > 0)/100000)
}
plot(x = 1:50, y = BFH1_values, ylim = c(0, 5))
for(i in 1:50) {
b <- as.numeric(i) / 400
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/as.numeric(b))
BFH2_values[i] <- (sum(posterior[, 1] > 0 & posterior[, 2] < .1 & posterior[, 2] > -.1)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] < .1 & prior[, 2] > -.1)/100000)
}
plot(x = 1:50, y = BFH2_values)
BFH1_values <- rep(NA,50)
plot(x = 1:50, y = BFH1_values, ylim = c(0, 5))
plot(x = 1:50, y = BFH2_values)
plot(x = 1:50, y = BFH1_values, ylim = c(0, 5))
library(MASS)
## To be able to get the multivariate posterior density, we get
## the means and covariance matrix of the posterior distributions
## from the IV regression coefficients
Means <- c(mean(LR1.dataframe$b1), mean(LR1.dataframe$b2))
Sigma <- cov(LR1.dataframe[2:3])
## Now, we will sample from the posterior and prior distributions
## First set a seed
set.seed(3)
## Then sample from the posterior, using the above specified means and sigma
posterior <- mvrnorm(100000, mu = Means, Sigma = Sigma)
## Calculate the fractional value
b <- 6/400 ## 3 regression coefficients: 3 means, 3 variances --> 6
## Then sample from the prior, using means = 0 and sigma = sigma/b
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/b)
## Finally, we can calculate the Bayes Factor for all our hypotheses
## by stating the correct statements
## For hypothesis 1:
## For both the posterior and prior, we want the proportion of sampled values where both
## regression coefficients are higher than 0 and divide them in the correct way
BF.H1 <- (sum(posterior[, 1] > 0 & posterior[, 2] > 0)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] > 0)/100000)
## For hypothesis 2:
## For both posterior and prior, the first regression coefficient should be higher
## than 0 and the second one should be close to zero (i.e., -.1 < x < .1)
BF.H2 <- (sum(posterior[, 1] > 0 & posterior[, 2] < .1 & posterior[, 2] > -.1)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] < .1 & prior[, 2] > -.1)/100000)
## For hypothesis 3:
## Exactly the other way around from hypothesis 2
BF.H3 <- (sum(posterior[, 2] > 0 & posterior[, 1] < .1 & posterior[, 1] > -.1)/100000) /
(sum(prior[, 2] > 0 & prior[, 1] < .1 & prior[, 1] > -.1)/100000)
BFH1_values <- rep(NA,50)
for(i in 1:50) {
b <- as.numeric(i) / 400
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/as.numeric(b))
BFH1_values[i] <- (sum(posterior[, 1] > 0 & posterior[, 2] > 0)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] > 0)/100000)
}
plot(x = 1:50, y = BFH1_values, ylim = c(0, 5))
BFH2_values <- rep(NA, 50)
for(i in 1:50) {
b <- as.numeric(i) / 400
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/as.numeric(b))
BFH2_values[i] <- (sum(posterior[, 1] > 0 & posterior[, 2] < .1 & posterior[, 2] > -.1)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] < .1 & prior[, 2] > -.1)/100000)
}
plot(x = 1:50, y = BFH2_values)
BFH1_values <- rep(NA,50)
plot(x = 1:50, y = BFH2_values)
plot(x = 1:50, y = BFH1_values, ylim = c(0, 5))
plot(x = 1:50, y = BFH1_values)
plot(x = 1:50, y = BFH1_values)
BFH1_values
BFH1_values <- rep(NA,50)
for(i in 1:50) {
b <- as.numeric(i) / 400
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/as.numeric(b))
BFH1_values[i] <- (sum(posterior[, 1] > 0 & posterior[, 2] > 0)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] > 0)/100000)
}
plot(x = 1:50, y = BFH1_values)
plot(x = 1:50, y = BFH1_values, ylim = c(0, 5))
library(MASS)
## To be able to get the multivariate posterior density, we get
## the means and covariance matrix of the posterior distributions
## from the IV regression coefficients
Means <- c(mean(LR1.dataframe$b1), mean(LR1.dataframe$b2))
Sigma <- cov(LR1.dataframe[2:3])
## Now, we will sample from the posterior and prior distributions
## First set a seed
set.seed(3)
## Then sample from the posterior, using the above specified means and sigma
posterior <- mvrnorm(100000, mu = Means, Sigma = Sigma)
## Calculate the fractional value
b <- 6/400 ## 3 regression coefficients: 3 means, 3 variances --> 6
## Then sample from the prior, using means = 0 and sigma = sigma/b
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/b)
## Finally, we can calculate the Bayes Factor for all our hypotheses
## by stating the correct statements
## For hypothesis 1:
## For both the posterior and prior, we want the proportion of sampled values where both
## regression coefficients are higher than 0 and divide them in the correct way
BF.H1 <- (sum(posterior[, 1] > 0 & posterior[, 2] > 0)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] > 0)/100000)
## For hypothesis 2:
## For both posterior and prior, the first regression coefficient should be higher
## than 0 and the second one should be close to zero (i.e., -.1 < x < .1)
BF.H2 <- (sum(posterior[, 1] > 0 & posterior[, 2] < .1 & posterior[, 2] > -.1)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] < .1 & prior[, 2] > -.1)/100000)
## For hypothesis 3:
## Exactly the other way around from hypothesis 2
BF.H3 <- (sum(posterior[, 2] > 0 & posterior[, 1] < .1 & posterior[, 1] > -.1)/100000) /
(sum(prior[, 2] > 0 & prior[, 1] < .1 & prior[, 1] > -.1)/100000)
BFH1_values <- rep(NA,50)
for(i in 1:50) {
b <- as.numeric(i) / 400
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/as.numeric(b))
BFH1_values[i] <- (sum(posterior[, 1] > 0 & posterior[, 2] > 0)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] > 0)/100000)
}
plot(x = 1:50, y = BFH1_values, ylim = c(0, 5))
BFH2_values <- rep(NA, 50)
for(i in 1:50) {
b <- as.numeric(i) / 400
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/as.numeric(b))
BFH2_values[i] <- (sum(posterior[, 1] > 0 & posterior[, 2] < .1 & posterior[, 2] > -.1)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] < .1 & prior[, 2] > -.1)/100000)
}
plot(x = 1:50, y = BFH2_values)
BFH1_values <- rep(NA,50)
plot(x = 1:50, y = BFH1_values, ylim = c(0, 5))
BFH1_values <- rep(NA, 50)
for(i in 1:50) {
b <- as.numeric(i) / 400
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/as.numeric(b))
BFH1_values[i] <- (sum(posterior[, 1] > 0 & posterior[, 2] > 0)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] > 0)/100000)
}
library(MASS)
## To be able to get the multivariate posterior density, we get
## the means and covariance matrix of the posterior distributions
## from the IV regression coefficients
Means <- c(mean(LR1.dataframe$b1), mean(LR1.dataframe$b2))
Sigma <- cov(LR1.dataframe[2:3])
## Now, we will sample from the posterior and prior distributions
## First set a seed
set.seed(3)
## Then sample from the posterior, using the above specified means and sigma
posterior <- mvrnorm(100000, mu = Means, Sigma = Sigma)
## Calculate the fractional value
b <- 6/400 ## 3 regression coefficients: 3 means, 3 variances --> 6
## Then sample from the prior, using means = 0 and sigma = sigma/b
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/b)
## Finally, we can calculate the Bayes Factor for all our hypotheses
## by stating the correct statements
## For hypothesis 1:
## For both the posterior and prior, we want the proportion of sampled values where both
## regression coefficients are higher than 0 and divide them in the correct way
BF.H1 <- (sum(posterior[, 1] > 0 & posterior[, 2] > 0)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] > 0)/100000)
## For hypothesis 2:
## For both posterior and prior, the first regression coefficient should be higher
## than 0 and the second one should be close to zero (i.e., -.1 < x < .1)
BF.H2 <- (sum(posterior[, 1] > 0 & posterior[, 2] < .1 & posterior[, 2] > -.1)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] < .1 & prior[, 2] > -.1)/100000)
## For hypothesis 3:
## Exactly the other way around from hypothesis 2
BF.H3 <- (sum(posterior[, 2] > 0 & posterior[, 1] < .1 & posterior[, 1] > -.1)/100000) /
(sum(prior[, 2] > 0 & prior[, 1] < .1 & prior[, 1] > -.1)/100000)
## Sensitivity analysis for H1 and H2
BFH1_values <- rep(NA, 50)
for(i in 1:50) {
b <- as.numeric(i) / 400
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/as.numeric(b))
BFH1_values[i] <- (sum(posterior[, 1] > 0 & posterior[, 2] > 0)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] > 0)/100000)
}
plot(x = 1:50, y = BFH1_values, ylim = c(0, 5))
BFH2_values <- rep(NA, 50)
for(i in 1:50) {
b <- as.numeric(i) / 400
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/as.numeric(b))
BFH2_values[i] <- (sum(posterior[, 1] > 0 & posterior[, 2] < .1 & posterior[, 2] > -.1)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] < .1 & prior[, 2] > -.1)/100000)
}
plot(x = 1:50, y = BFH2_values)
plot(x = 1:50, y = BFH1_values, ylim = c(0, 5))
plot(x = 1:50, y = BFH2_values)
knitr::opts_chunk$set(echo = TRUE)
library(haven)
dat <- read_sav(file = "Week6Data2.sav")
dat <- dat[, 2:4]
dat$verbal <- dat$verbal - mean(dat$verbal)
## First create space
b0 <- rep(NA, iterations)
dat$verbal[1]
dat$verbal[1* 4]
dat$verbal[1*4]
dat$verbal[1*5]
knitr::opts_chunk$set(echo = TRUE)
library(haven)
dat <- read_sav(file = "Week6Data2.sav")
dat <- dat[, 2:4]
dat$verbal <- dat$verbal - mean(dat$verbal)
## For the input of the Gibbs/MH Sampler, one should provide:
## a vector of continuous variables that serves as your dependent variable
## two vectors of continuous variables that serve as your independent variables
## a burn.in period (by default 1000 iterations)
## a total amount of iterations (by default 10000)
## the initial values for every regression coefficient
## a method function that specifies per parameter whether they would
## like to have it sampled through a Gibbs Sampler or Metropolis Hastings algorithm
## by default, it assumes a Gibbs sampler for every regression coefficient,
## and if you want to redefine it to a MH, you should change the 0 to a 1
## for the corresponding regression coefficient you want to change
## Examples:
## method = c(0, 1, 0) will make it so that b1 is done through MH
GibbsSampler <- function(Y, X1, X2, burn.in = 1000, iterations = 10000,
initial.values = c(1, 1, 1), method = c("0", "0", "0")) {
## First, the function should require ggplot2 as it is used to create traceplots later
library(ggplot2)
## First create space for all the regression coefficients
b0 <- rep(NA, iterations)
b1 <- rep(NA, iterations)
b2 <- rep(NA, iterations)
vari <- rep(NA, iterations)
## Then assume the specified initial values
b0[1] <- initial.values[1]
b1[1] <- initial.values[2]
b2[1] <- initial.values[3]
vari[1] <- 1
N <- length(Y)
## Now, we will start the sampling, it should start at the second element
## because the first element will be the initial values
for(i in 2:iterations) {
## In every loop, We first update the intercept:
## Check whether the intercept should be sampled through Gibbs or MH
if(method[1] == "0") {
## If through Gibbs Sampler:
## First, we update the parameters of the intercept (mean and standard deviation) with the following formula:
mean.0.post <- (sum(Y - (b1[i - 1]*X1 - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1] +
(mu0 / sigma0)) / ((N / vari[i - 1]) + (1 / sigma0))
sd.0.post <- 1 / ((N / vari[i - 1]) + (1 / sigma0))
## Then through rnorm we randomly sample from a normal distribution the next value for b0 with the
## above calculated mean and standard deviation
b0[i] <- rnorm(n = 1, mean = mean.0.post, sd = sqrt(sd.0.post))
} else {
## If through MH:
## First, we gain a proposal value through adding the lm point estimate
## with the standard error of the estimate times
## a random sampled value of a T-distribution with df = 1
proposal_value <- 115.45920 + 1.95990 * rt(1, df = 1)
## Then calculate the acceptance ratio
accept_ratio <- (dnorm(proposal_value, mean = 115.45, sd = 19.96)/dnorm(b0[i - 1], mean = 115.45, sd = 19.96)) *
(dt(b0[i - 1], df = 1)/dt(proposal_value, df = 1))
## Then see whether we accept the proposed value
if(runif(1,0,1) < accept_ratio) {
b0[i] <- proposal_value } else { b0[i] <- b0[i - 1] }
}
## Then, every loop, we will update the coefficient of the first independent variable
## Again, check whether Gibbs or MH:
if(method[2] == "0") {
## If through Gibbs:
## We can update the mean and standard deviation of the regression coefficient for the first variable
## using the updated value of the intercept
mean.1.post <- ((sum(X1*(Y - b0[i] - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1]) +
(mu1 / sigma1)) / ((sum((X1^2), na.rm = TRUE)/vari[i - 1]) + (1 / sigma1))
sd.1.post <- 1/((sum((X1^2), na.rm = TRUE) / vari[i - 1]) + (1 / sigma1))
## Then again we randomly sample a new b1 through rnorm
b1[i] <- rnorm(n = 1, mean = mean.1.post, sd = sqrt(sd.1.post))
} else {
## If through MH:
## First, we gain a proposal value in a similar fashion as above
proposal_value <- 11.73647 + 1.45593 * rt(1, df = 1)
## Calculate acceptance ratio
accept_ratio <- (dnorm(proposal_value, mean = 11.73, sd = 1.46)/dnorm(b1[i - 1], mean = 11.73, sd = 1.46)) *
(dt(b1[i - 1], df = 1)/dt(proposal_value, df = 1))
## Then see whether we accept the proposed value
if(runif(1,0,1) < accept_ratio) {
b1[i] <- proposal_value } else { b1[i] <- b1[i - 1] }
}
## Then, every loop, we update the regression coefficient of the second variable
## First check whether Gibbs or MH:
if(method[3] == "0") {
## If through Gibbs:
## We update the mean and standard deviation of the regression coefficient for the second variable
## using the updated value of both the intercept and b1
mean.2.post <- (sum(X2*(Y - b0[i] - b1[i]*X1), na.rm = TRUE)/vari[i - 1] +
(mu2 / sigma2)) / ((sum(X2^2, na.rm = TRUE)/vari[i - 1]) + (1 / sigma2))
sd.2.post <- 1/((sum(X2^2, na.rm = TRUE) / vari[i - 1]) + (1 / sigma2))
## Then, we randomly sample a new b2
b2[i] <- rnorm(n = 1, mean = mean.2.post, sd = sqrt(sd.2.post))
} else {
## If through MH:
## First, we gain a proposal value
proposal_value <- 0.08399 + 0.05738 * rt(1, df = 1)
## Calculate acceptance ratio
accept_ratio <- (dnorm(proposal_value, mean = 0.08, sd = 0.06)/dnorm(b2[i - 1], mean = 0.08, sd = 0.06)) *
(dt(b2[i - 1], df = 1)/dt(proposal_value, df = 1))
## Then see whether we accept the proposed value
if(runif(1,0,1) < accept_ratio) {
b2[i] <- proposal_value } else { b2[i] <- b2[i - 1] }
}
## Finally, we update the parameters for the distribution of our variance using all above updated values
a.post <- (N / 2) + a.prior
b.post <- (sum((Y - (b0[i] + b1[i]*X1 + b2[i]*X2))^2, na.rm = TRUE) / 2) + b.prior
## And randomly sample a new variance again through rgamma
vari[i] <- 1/rgamma(n = 1, shape = a.post, rate = b.post)
## We use the inverse of the randomly sample because we want the value of the variance, and not the precision.
}
## Then, we remove the values of the burn-in
b0 <- b0[-c(1:burn.in)]
b1 <- b1[-c(1:burn.in)]
b2 <- b2[-c(1:burn.in)]
vari <- vari[-c(1:burn.in)]
## We also want the posterior distributions (histograms) of all parameters
par(mfrow = c(2,2))
hist(b0, breaks = 50)
abline(v = mean(b0), col = "blue")
hist(b1, breaks = 50)
abline(v = mean(b1), col = "blue")
hist(b2, breaks = 50)
abline(v = mean(b2), col = "blue")
hist(vari, breaks = 50)
## We want a dataframe consisting of all the sampled parameter values
data_frame <- as.data.frame(cbind(b0, b1, b2, vari, iter = 1:length(b0)))
## And we want traceplots for all the parameters to assess convergence
traceplot1 <- ggplot(data_frame, aes(x=iter, y=b0)) +
geom_line() +
labs(title="b0 trace")
traceplot1
traceplot2 <- ggplot(data_frame, aes(x=iter, y=b1)) +
geom_line() +
labs(title="b1 trace")
traceplot2
traceplot3 <- ggplot(data_frame, aes(x=iter, y=b2)) +
geom_line() +
labs(title="b2 trace")
traceplot3
traceplot4 <- ggplot(data_frame, aes(x=iter, y=vari)) +
geom_line() +
labs(title="var trace")
traceplot4
## We create a list of all the output we want
list_of_output <- list(data_frame, traceplot1, traceplot2, traceplot3, traceplot4)
## And finally, we ask the function to return this list of output
return(list_of_output)
}
## Note, the above function only works if you first set all priors
## (i.e., a.prior, b.prior, mu0, mu1, mu2, sigma0, sigma1, sigma2)
## Analysis 1: Uninformative prior + Gibbs -------------------------------------
## First set a seed for reproducibility
set.seed(3)
## Then set all priors to uninformative
a.prior <- 0.001
b.prior <- 0.001
mu0 <- 1
sigma0 <- 10000
mu1 <- 1
sigma1 <- 10000
mu2 <- 1
sigma2 <- 10000
## Then run the GibbsSampler function with our data
Analysis.1<- GibbsSampler(dat$IQ, dat$verbal, dat$SES, burn.in = 2500,
iterations = 12500, initial.values = c(1, 1, 1))
## Analysis 2: Independent MH Sampler ------------------------------------------
Analysis.2 <- GibbsSampler(dat$IQ, dat$verbal, dat$SES, burn.in = 5000,
iterations = 25000, initial.values = c(1, 1, 1),
method = c("1", "1", "1"))
LR1.dataframe <- Analysis.1[[1]]
## Now, to get the expected value and credible intervals of every regression coefficient:
mean(LR1.dataframe$b0)
quantile(LR1.dataframe$b0, probs = c(0.025, 0.975))
mean(LR1.dataframe$b1)
quantile(LR1.dataframe$b1, probs = c(0.025, 0.975))
mean(LR1.dataframe$b2)
quantile(LR1.dataframe$b2, probs = c(0.025, 0.975))
