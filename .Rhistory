Analysis.1[[5]]
## AUTOCOR PLOTS
autocorrelationplot(V = LR1.dataframe$b0)
autocorrelationplot(V = LR1.dataframe$b1)
autocorrelationplot(V = LR1.dataframe$b2)
autocorrelationplot(V = LR1.dataframe$vari)
## First, we set a seed for reproducibility
set.seed(3)
## Then, we create an empty matrix with equal rows to our original dataset
## and as many columns as we would like datasets
sim.data <- matrix(data = NA, nrow = 400, ncol = 1000)
## Then, we create space for the regression coefficients that we will sample
## so that we can use these to calculate the residuals later
b0_t <- rep(NA, 1000)
b1_t <- rep(NA, 1000)
b2_t <- rep(NA, 1000)
s2_t <- rep(NA, 1000)
## Now, we can start the data simulation through a for loop
for(i in 1:1000) {
## In every loop, we sample one value from all our regression coefficients
b0_t[i] <- sample(LRX.dataframe$b0, 1)
b1_t[i] <- sample(LRX.dataframe$b1, 1)
b2_t[i] <- sample(LRX.dataframe$b2, 1)
s2_t[i] <- sample(LRX.dataframe$vari, 1)
## Then, we create a column of simulated Y values that use the sampled
## regression coefficients through rnorm, where the mean is their
## expected value and the sd is the sampled variance
sim.data[, i] <- rnorm(400, mean = b0_t[i] + b1_t[i]*testdat$verbal + b2_t[i]*testdat$SES, sd = sqrt(s2_t[i]))
}
library(MASS)
linearmodel <- lm(IQ ~ verbal + SES, data = dat)
vcov(LR1.dataframe)
cov(LR1.dataframe)
var(LR1.dataframe)
var(LR1.dataframe[1:3])
## Create a linear mo
linearmodel <- lm(IQ ~ verbal + SES, data = dat)
vcov(linearmodel)
vcov(linearmodel)[2:3]
vcov(linearmodel)[2:3, 2:3]
## OPTIE 2
Sigma <- cov(LR1.dataframe[2:3])
View(Sigma)
Sigma
library(MASS)
## Create a linear mo
linearmodel <- lm(IQ ~ verbal + SES, data = dat)
## OPTIES VOOR SAMPLING:
## OPTIE 1
Means <- linearmodel$coefficients[2:3]
Sigma <- vcov(linearmodel)[2:3, 2:3]
set.seed(3)
posterior <- mvrnorm(100000, mu = Means, Sigma = sigma)
set.seed(3)
posterior <- mvrnorm(100000, mu = Means, Sigma = sigma)
post <- mvrnorm(100000, mu = means, Sigma = sigma)
set.seed(3)
posterior <- mvrnorm(100000, mu = Means, Sigma = sigma)
set.seed(3)
posterior <- mvrnorm(100000, mu = Means, Sigma = Sigma)
set.seed(3)
posterior <- mvrnorm(100000, mu = Means, Sigma = Sigma)
## Calculate the fractional value
b <- 6/400 ## 3 regression coefficients: 3 means, 3 variances --> 6
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/b)
BF.H1 <- (sum(post[, 1] > 0 & post[, 2] > 0)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] > 0)/100000)
BF.H1 <- (sum(posterior[, 1] > 0 & posterior[, 2] > 0)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] > 0)/100000)
BF.H2 <- (sum(posterior[, 1] > 0 & posterior[, 2] < .1 & posterior[, 2] > -.1)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] < .1 & prior[, 2] > -.1)/100000)
BF.H3 <- (sum(posterior[, 2] > 0 & posterior[, 1] < .1 & posterior[, 1] > -.1)/100000) /
(sum(prior[, 2] > 0 & prior[, 1] < .1 & prior[, 1] > -.1)/100000)
knitr::opts_chunk$set(echo = TRUE)
library(haven)
dat <- read_sav(file = "Week6Data2.sav")
dat <- dat[, 2:4]
dat$verbal <- dat$verbal - mean(dat$verbal)
## For the input of the Gibbs/MH Sampler, one should provide:
## a vector of continuous variables that serves as your dependent variable
## two vectors of continuous variables that serve as your independent variables
## a burn.in period (by default 1000 iterations)
## a total amount of iterations (by default 10000)
## the initial values for every regression coefficient
## a method function that specifies per parameter whether they would
## like to have it sampled through a Gibbs Sampler or Metropolis Hastings algorithm
## by default, it assumes a Gibbs sampler for every regression coefficient,
## and if you want to redefine it to a MH, you should change the 0 to a 1
## for the corresponding regression coefficient you want to change
## Examples:
## method = c(0, 1, 0) will make it so that b1 is done through MH
GibbsSampler <- function(Y, X1, X2, burn.in = 1000, iterations = 10000,
initial.values = c(1, 1, 1), method = c("0", "0", "0")) {
## First, the function should require ggplot2 as it is used to create traceplots later
library(ggplot2)
## First create space for all the regression coefficients
b0 <- rep(NA, iterations)
b1 <- rep(NA, iterations)
b2 <- rep(NA, iterations)
vari <- rep(NA, iterations)
## Then assume the specified initial values
b0[1] <- initial.values[1]
b1[1] <- initial.values[2]
b2[1] <- initial.values[3]
vari[1] <- 1
N <- length(Y)
## Now, we will start the sampling, it should start at the second element
## because the first element will be the initial values
for(i in 2:iterations) {
## In every loop, We first update the intercept:
## Check whether the intercept should be sampled through Gibbs or MH
if(method[1] == "0") {
## If through Gibbs Sampler:
## First, we update the parameters of the intercept (mean and standard deviation) with the following formula:
mean.0.post <- (sum(Y - (b1[i - 1]*X1 - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1] +
(mu0 / sigma0)) / ((N / vari[i - 1]) + (1 / sigma0))
sd.0.post <- 1 / ((N / vari[i - 1]) + (1 / sigma0))
## Then through rnorm we randomly sample from a normal distribution the next value for b0 with the
## above calculated mean and standard deviation
b0[i] <- rnorm(n = 1, mean = mean.0.post, sd = sqrt(sd.0.post))
} else {
## If through MH:
## First, we gain a proposal value through multiplying the lm estimate
## with a random sampled value of a T-distribution with df = 1
proposal_value <- 115.45 * rt(1, df = 1)
## Then calculate the acceptance ratio
accept_ratio <- (dnorm(proposal_value, mean = 115.45, sd = 19.96)/dnorm(b0[i - 1], mean = 115.45, sd = 19.96)) *
(dt(b0[i - 1], df = 1)/dt(proposal_value, df = 1))
## Then see whether we accept the proposed value
if(runif(1,0,1) < accept_ratio) {
b0[i] <- proposal_value } else { b0[i] <- b0[i - 1] }
}
## Then, every loop, we will update the coefficient of the first independent variable
## Again, check whether Gibbs or MH:
if(method[2] == "0") {
## If through Gibbs:
## We can update the mean and standard deviation of the regression coefficient for the first variable
## using the updated value of the intercept
mean.1.post <- ((sum(X1*(Y - b0[i] - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1]) +
(mu1 / sigma1)) / ((sum((X1^2), na.rm = TRUE)/vari[i - 1]) + (1 / sigma1))
sd.1.post <- 1/((sum((X1^2), na.rm = TRUE) / vari[i - 1]) + (1 / sigma1))
## Then again we randomly sample a new b1 through rnorm
b1[i] <- rnorm(n = 1, mean = mean.1.post, sd = sqrt(sd.1.post))
} else {
## If through MH:
## First, we gain a proposal value in a similar fashion as above
proposal_value <- 11.73 * rt(1, df = 1)
## Calculate acceptance ratio
accept_ratio <- (dnorm(proposal_value, mean = 11.73, sd = 1.46)/dnorm(b1[i - 1], mean = 11.73, sd = 1.46)) *
(dt(b1[i - 1], df = 1)/dt(proposal_value, df = 1))
## Then see whether we accept the proposed value
if(runif(1,0,1) < accept_ratio) {
b1[i] <- proposal_value } else { b1[i] <- b1[i - 1] }
}
## Then, every loop, we update the regression coefficient of the second variable
## First check whether Gibbs or MH:
if(method[3] == "0") {
## If through Gibbs:
## We update the mean and standard deviation of the regression coefficient for the second variable
## using the updated value of both the intercept and b1
mean.2.post <- (sum(X2*(Y - b0[i] - b1[i]*X1), na.rm = TRUE)/vari[i - 1] +
(mu2 / sigma2)) / ((sum(X2^2, na.rm = TRUE)/vari[i - 1]) + (1 / sigma2))
sd.2.post <- 1/((sum(X2^2, na.rm = TRUE) / vari[i - 1]) + (1 / sigma2))
## Then, we randomly sample a new b2
b2[i] <- rnorm(n = 1, mean = mean.2.post, sd = sqrt(sd.2.post))
} else {
## If through MH:
## First, we gain a proposal value
proposal_value <- 0.08 * rt(1, df = 1)
## Calculate acceptance ratio
accept_ratio <- (dnorm(proposal_value, mean = 0.08, sd = 0.06)/dnorm(b2[i - 1], mean = 0.08, sd = 0.06)) *
(dt(b2[i - 1], df = 1)/dt(proposal_value, df = 1))
## Then see whether we accept the proposed value
if(runif(1,0,1) < accept_ratio) {
b2[i] <- proposal_value } else { b2[i] <- b2[i - 1] }
}
## Finally, we update the parameters for the distribution of our variance using all above updated values
a.post <- (N / 2) + a.prior
b.post <- (sum((Y - (b0[i] + b1[i]*X1 + b2[i]*X2))^2, na.rm = TRUE) / 2) + b.prior
## And randomly sample a new variance again through rgamma
vari[i] <- 1/rgamma(n = 1, shape = a.post, rate = b.post)
## We use the inverse of the randomly sample because we want the value of the variance, and not the precision.
}
## Then, we remove the values of the burn-in
b0 <- b0[-c(1:burn.in)]
b1 <- b1[-c(1:burn.in)]
b2 <- b2[-c(1:burn.in)]
vari <- vari[-c(1:burn.in)]
## We also want the posterior distributions (histograms) of all parameters
par(mfrow = c(2,2))
hist(b0, breaks = 50)
abline(v = mean(b0), col = "blue")
hist(b1, breaks = 50)
abline(v = mean(b1), col = "blue")
hist(b2, breaks = 50)
abline(v = mean(b2), col = "blue")
hist(vari, breaks = 50)
## We want a dataframe consisting of all the sampled parameter values
data_frame <- as.data.frame(cbind(b0, b1, b2, vari, iter = 1:length(b0)))
## And we want traceplots for all the parameters to assess convergence
traceplot1 <- ggplot(data_frame, aes(x=iter, y=b0)) +
geom_line() +
labs(title="b0 trace")
traceplot1
traceplot2 <- ggplot(data_frame, aes(x=iter, y=b1)) +
geom_line() +
labs(title="b1 trace")
traceplot2
traceplot3 <- ggplot(data_frame, aes(x=iter, y=b2)) +
geom_line() +
labs(title="b2 trace")
traceplot3
traceplot4 <- ggplot(data_frame, aes(x=iter, y=vari)) +
geom_line() +
labs(title="var trace")
traceplot4
## We create a list of all the output we want
list_of_output <- list(data_frame, traceplot1, traceplot2, traceplot3, traceplot4)
## And finally, we ask the function to return this list of output
return(list_of_output)
}
## Note, the above function only works if you first set all priors
## (i.e., a.prior, b.prior, mu0, mu1, mu2, sigma0, sigma1, sigma2)
## Analysis 1: Uninformative prior + Gibbs -------------------------------------
## First set a seed for reproducibility
set.seed(3)
## Then set all priors to uninformative
a.prior <- 0.001
b.prior <- 0.001
mu0 <- 1
sigma0 <- 10000
mu1 <- 1
sigma1 <- 10000
mu2 <- 1
sigma2 <- 10000
## Then run the GibbsSampler function with our data
Analysis.1<- GibbsSampler(dat$IQ, dat$verbal, dat$SES, burn.in = 5000,
iterations = 25000, initial.values = c(1, 1, 1))
## Analysis 2: Informative prior + Gibbs ---------------------------------------
## Set priors to earlier decided upon values
a.prior <- 0.001
b.prior <- 0.001
mu0 <- 1
sigma0 <- 10000
mu1 <- 1
sigma1 <- 10000
mu2 <- 1
sigma2 <- 10000
Analysis.2 <- GibbsSampler(dat$IQ, dat$verbal, dat$SES, burn.in = 5000,
iterations = 25000, initial.values = c(1, 1, 1))
## Analysis 3: Independent MH Sampler ------------------------------------------
Analysis.3 <- GibbsSampler(dat$IQ, dat$verbal, dat$SES, burn.in = 5000,
iterations = 25000, initial.values = c(1, 1, 1),
method = c("1", "1", "1"))
library(MASS)
## Create a linear mo
linearmodel <- lm(IQ ~ verbal + SES, data = dat)
## OPTIES VOOR SAMPLING:
## OPTIE 1
Means <- linearmodel$coefficients[2:3]
Sigma <- vcov(linearmodel)[2:3, 2:3]
## OPTIE 2
Means <- c(mean(LR1.dataframe$b1), mean(LR1.dataframe$b2))
## FIRST ANALYSIS --------------------------------------------------------------
## First, subset the dataframe from the list of output
LR1.dataframe <- Analysis.1[[1]]
## Now, to get the expected value and credible intervals of every regression coefficient:
mean(LR1.dataframe$b0)
quantile(LR1.dataframe$b0, probs = c(0.025, 0.975))
mean(LR1.dataframe$b1)
quantile(LR1.dataframe$b1, probs = c(0.025, 0.975))
mean(LR1.dataframe$b2)
quantile(LR1.dataframe$b2, probs = c(0.025, 0.975))
## SECOND ANALYSIS -------------------------------------------------------------
LR2.dataframe <- Analysis.2[[1]]
## THIRD ANALYSIS --------------------------------------------------------------
LR3.dataframe <- Analysis.3[[1]]
mean(LR3.dataframe$b0)
quantile(LR1.dataframe$b0, probs = c(0.025, 0.975))
mean(LR3.dataframe$b1)
quantile(LR1.dataframe$b1, probs = c(0.025, 0.975))
mean(LR3.dataframe$b2)
quantile(LR1.dataframe$b2, probs = c(0.025, 0.975))
library(MASS)
## Create a linear mo
linearmodel <- lm(IQ ~ verbal + SES, data = dat)
## OPTIES VOOR SAMPLING:
## OPTIE 1
Means <- linearmodel$coefficients[2:3]
Sigma <- vcov(linearmodel)[2:3, 2:3]
## OPTIE 2
Means <- c(mean(LR1.dataframe$b1), mean(LR1.dataframe$b2))
Sigma <- cov(LR1.dataframe[2:3])
## Now, we will sample from the posterior and prior distributions
## First set a seed
set.seed(3)
## Then sample from the posterior, using the above specified means and sigma
posterior <- mvrnorm(100000, mu = Means, Sigma = Sigma)
## Calculate the fractional value
b <- 6/400 ## 3 regression coefficients: 3 means, 3 variances --> 6
## Then sample from the prior, using means = 0 and sigma = sigma/b
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma/b)
## Finally, we can calculate the Bayes Factor for all our hypotheses
BF.H1 <- (sum(posterior[, 1] > 0 & posterior[, 2] > 0)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] > 0)/100000)
BF.H2 <- (sum(posterior[, 1] > 0 & posterior[, 2] < .1 & posterior[, 2] > -.1)/100000) /
(sum(prior[, 1] > 0 & prior[, 2] < .1 & prior[, 2] > -.1)/100000)
BF.H3 <- (sum(posterior[, 2] > 0 & posterior[, 1] < .1 & posterior[, 1] > -.1)/100000) /
(sum(prior[, 2] > 0 & prior[, 1] < .1 & prior[, 1] > -.1)/100000)
vcov(linearmodel)[2:3, 2:3]
cov(LR1.dataframe[2:3])
set.seed(3)
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("0", "0", "1"))
## First, all priors have to be set
a.prior <- 0.001
b.prior <- 0.001
mu0 <- 1
sigma0 <- 10000
mu1 <- 1
sigma1 <- 10000
mu2 <- 1
sigma2 <- 10000
## For MH: we need a proposal and posterior distribution, however, since
## it has to be a linear regression, this will always be a normal distribution
## so we can use dnorm() for this, but the proposal distribution should be specified!
## For the input of the Gibbs Sampler, one should provide:
## a vector of continuous variables that serves as your dependent variable
## two vectors of continuous variables that serve as your independent variables
## a burn.in period (by default 1000 iterations)
## a total amount of iterations (by default 10000)
## the initial values for every regression coefficient
## a method function that specifies per parameter whether they would
## like to have it sampled through a Gibbs Sampler or Metropolis Hastings algorithm
## by default, it assumes a Gibbs sampler for every regression coefficient,
## and if you want to redefine it to a MH, you should change the 0 to a 1
## for the regression coefficient you want to change
## Examples:
## method = c(0, 1, 0) will make it so that b1 is done through MH
GibbsSampler <- function(Y, X1, X2, burn.in = 1000, iterations = 10000,
initial.values = c(1, 1, 1), method = c("0", "0", "0")) {
## First, the function should require ggplot2 as it is used to create traceplots later
library(ggplot2)
## First create space
b0 <- rep(NA, iterations)
b1 <- rep(NA, iterations)
b2 <- rep(NA, iterations)
vari <- rep(NA, iterations)
## Then assume the specified initial values
b0[1] <- initial.values[1]
b1[1] <- initial.values[2]
b2[1] <- initial.values[3]
vari[1] <- 1
N <- length(Y)
## Now, we will start the sampling, it should start at the second element
## because the first element will be the initial values
for(i in 2:iterations) {
## In every loop, We first update the intercept:
## Check whether the intercept should be sampled through Gibbs or MH
if(method[1] == "0") {
## If through Gibbs Sampler:
## First, we update the parameters of the intercept (mean and standard deviation) with the following formula:
mean.0.post <- (sum(Y - (b1[i - 1]*X1 - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1] +
(mu0 / sigma0)) / ((N / vari[i - 1]) + (1 / sigma0))
sd.0.post <- 1 / ((N / vari[i - 1]) + (1 / sigma0))
## Then through rnorm we randomly sample from a normal distribution the next value for b0 with the
## above calculated mean and standard deviation
b0[i] <- rnorm(n = 1, mean = mean.0.post, sd = sqrt(sd.0.post))
} else {
## If through MH:
## First, we gain a proposal value
proposal_value <- 115.45 * rt(1, df = 1)
## Calculate acceptance ratio
accept_ratio <- (dnorm(proposal_value, mean = 115.45, sd = 19.96)/dnorm(b0[i - 1], mean = 115.45, sd = 19.96)) *
(dt(b0[i - 1], df = 1)/dt(proposal_value, df = 1))
## Then see whether we accept the proposed value
if(runif(1,0,1) < accept_ratio) {
b0[i] <- proposal_value } else { b0[i] <- b0[i - 1] }
}
## Then, every loop, we will update the coefficient of the first independent variable
## Again, check whether Gibbs or MH:
if(method[2] == "0") {
## If through Gibbs:
## We can update the mean and standard deviation of the regression coefficient for the first variable
## using the updated value of the intercept
mean.1.post <- ((sum(X1*(Y - b0[i] - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1]) +
(mu1 / sigma1)) / ((sum((X1^2), na.rm = TRUE)/vari[i - 1]) + (1 / sigma1))
sd.1.post <- 1/((sum((X1^2), na.rm = TRUE) / vari[i - 1]) + (1 / sigma1))
## Then again we randomly sample a new b1 through rnorm
b1[i] <- rnorm(n = 1, mean = mean.1.post, sd = sqrt(sd.1.post))
} else {
## If through MH:
## First, we gain a proposal value
proposal_value <- 11.73 * rt(1, df = 1)
## Calculate acceptance ratio
accept_ratio <- (dnorm(proposal_value, mean = 11.73, sd = 1.46)/dnorm(b1[i - 1], mean = 11.73, sd = 1.46)) *
(dt(b1[i - 1], df = 1)/dt(proposal_value, df = 1))
## Then see whether we accept the proposed value
if(runif(1,0,1) < accept_ratio) {
b1[i] <- proposal_value } else { b1[i] <- b1[i - 1] }
}
## Then, every loop, we update the regression coefficient of the second variable
## But first check whether Gibbs oR MH:
if(method[3] == "0") {
## If through Gibbs:
## We update the mean and standard deviation of the regression coefficient for the second variable
## using the updated value of both the intercept and b1
mean.2.post <- (sum(X2*(Y - b0[i] - b1[i]*X1), na.rm = TRUE)/vari[i - 1] +
(mu2 / sigma2)) / ((sum(X2^2, na.rm = TRUE)/vari[i - 1]) + (1 / sigma2))
sd.2.post <- 1/((sum(X2^2, na.rm = TRUE) / vari[i - 1]) + (1 / sigma2))
## Then, we randomly sample a new b2
b2[i] <- rnorm(n = 1, mean = mean.2.post, sd = sqrt(sd.2.post))
} else {
## If through MH:
## First, we gain a proposal value
proposal_value <- 0.08 * rt(1, df = 1)
## Calculate acceptance ratio
accept_ratio <- (dnorm(proposal_value, mean = 0.08, sd = 0.06)/dnorm(b2[i - 1], mean = 0.08, sd = 0.06)) *
(dt(b2[i - 1], df = 1)/dt(proposal_value, df = 1))
## Then see whether we accept the proposed value
if(runif(1,0,1) < accept_ratio) {
b2[i] <- proposal_value } else { b2[i] <- b2[i - 1] }
}
## Finally, we update the parameters for the distribution of our variance using all above updated values
a.post <- (N / 2) + a.prior
b.post <- (sum((Y - (b0[i] + b1[i]*X1 + b2[i]*X2))^2, na.rm = TRUE) / 2) + b.prior
## And randomly sample a new variance again through rgamma
vari[i] <- 1/rgamma(n = 1, shape = a.post, rate = b.post)
## We use the inverse of the randomly sample because we want the value of the variance, and not the precision.
}
## Then, we remove the values of the burn-in
b0 <- b0[-c(1:burn.in)]
b1 <- b1[-c(1:burn.in)]
b2 <- b2[-c(1:burn.in)]
vari <- vari[-c(1:burn.in)]
## We also want the posterior distributions (histograms) of all parameters
par(mfrow = c(2,2))
hist(b0, breaks = 50)
abline(v = mean(b0), col = "blue")
hist(b1, breaks = 50)
abline(v = mean(b1), col = "blue")
hist(b2, breaks = 50)
abline(v = mean(b2), col = "blue")
hist(vari, breaks = 50)
## We want a dataframe consisting of all the sampled parameter values
data_frame <- as.data.frame(cbind(b0, b1, b2, vari, iter = 1:length(b0)))
## And we want traceplots for all the parameters to assess convergence
traceplot1 <- ggplot(data_frame, aes(x=iter, y=b0)) +
geom_line() +
labs(title="b0 trace")
traceplot1
traceplot2 <- ggplot(data_frame, aes(x=iter, y=b1)) +
geom_line() +
labs(title="b1 trace")
traceplot2
traceplot3 <- ggplot(data_frame, aes(x=iter, y=b2)) +
geom_line() +
labs(title="b2 trace")
traceplot3
traceplot4 <- ggplot(data_frame, aes(x=iter, y=vari)) +
geom_line() +
labs(title="var trace")
traceplot4
## We create a list of all the output we want
list_of_output <- list(data_frame, traceplot1, traceplot2, traceplot3, traceplot4)
## And finally, we ask the function to return this list of output
return(list_of_output)
}
## Testing the sampler
library(haven)
data <- read_sav("Exercise 2 - Data.sav")
str(data)
summary(lm(attitude ~ extraversion + agreeableness, data = data))
GibbsSampler(data$attitude, data$extraversion, data$agreeableness, burn.in = 1000, iterations = 10000, initial.values = c(50, .05, .1))
autocorrelationplot <- function(V, lag = 50) {
autocors <- rep(NA, lag)
for(i in 1:lag) {
V1 <- c(V, rep(0, i))
V2 <- c(rep(0, i), V)
matrix <- cbind(V1, V2)
matrix <- head(matrix, -i)
matrix <- tail(matrix, -i)
autocors[i] <- cor(matrix[, 1], matrix[, 2])[1]
}
plot(1:lag, autocors)
return(autocors)
}
autocorrelationplot(V = df$b2)
set.seed(3)
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("0", "0", "1"))
## Comparing results to JAGS results
library(haven)
testdat <- read_sav(file = "Week6Data2.sav")
testdat$pass <- as.numeric(testdat$pass)
testdat$verbal <- testdat$verbal - mean(testdat$verbal)
summary(lm(IQ ~ verbal + SES, data = testdat))
set.seed(3)
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("0", "0", "0"))
list <- GibbsSampler(testdat$IQ, testdat$verbal, testdat$pass, burn.in = 2500, iterations = 12500, initial.values = c(1, 1, 1))
df <- as.data.frame(list[1])
mean(df$b0)
mean(df$b1)
mean(df$b2)
mean(testdat$IQ)
var(testdat$IQ)
set.seed(3)
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("0", "0", "1"))
set.seed(3)
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("0", "1", "0"))
set.seed(3)
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("1", "1", "1"))
rt(100, 1)
?rt
