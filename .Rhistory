mean.0.post <- (sum(Y - b1[i - 1]*X1 - b2[i - 1]*X2, na.rm = TRUE)/vari[i - 1] + (mu0 / sigma0)) / ((N / vari[i - 1]) + (mu0 / sigma0))
sd.0.post <- 1 / ((N / vari[i - 1]) + (mu0 / sigma0))
## Then through rnorm we randomly sample from a normal distribution the next value for b0 with the
## above calculated mean and standard deviation
b0[i] <- rnorm(n = 1, mean = mean.0.post, sd = sd.0.post)
## Now, we can update the mean and standard deviation of the regression coefficient for the first variable
## using the updated value of the intercept
mean.1.post <- (sum(X1*(Y - b0[i] - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1] + (mu1 / sigma1)) / sum(X1^2, na.rm = TRUE)/vari[i - 1] + (mu1 / sigma1)
sd.1.post <- 1/((sum(X1^2, na.rm = TRUE) / vari[i - 1]) + (mu1 / sigma1))
## Then again we randomly sample a new b1
b1[i] <- rnorm(n = 1, mean = mean.1.post, sd = sd.1.post)
## Afterwards, we update the mean and standard deviation of the regression coefficient for the second variable
## using the updated value of both the intercept and b1
mean.2.post <- (sum(X2*(Y - b0[i] - b1[i]*X1), na.rm = TRUE)/vari[i - 1] + (mu2 / sigma2)) / sum(X2^2, na.rm = TRUE)/vari[i - 1] + (mu2 / sigma2)
sd.2.post <- 1/((sum(X2^2, na.rm = TRUE) / vari[i - 1]) + (mu2 / sigma2))
## Then, we randomly sample a new b2
b2[i] <- rnorm(n = 1, mean = mean.2.post, sd = sd.2.post)
## Finally, we update the parameters for the distribution of our standard deviation using all above updated values
a.post <- N / 2 + a.prior
b.post <- sum((Y - (b0[i] + b1[i]*X1 + b2[i]*X2))^2, na.rm = TRUE) / 2 + b.prior
## And randomly sample a new variance again for
vari[i] <- 1/rgamma(n = 1, shape = a.post, rate = b.post)
}
b0 <- b0[-c(1:burn.in)]
b1 <- b1[-c(1:burn.in)]
b2 <- b2[-c(1:burn.in)]
vari <- vari[-c(1:burn.in)]
## Now, we also want the posterior distributions (histograms) of all parameters
par(mfrow = c(2,2))
hist(b0)
abline(v = mean(b0), col = "blue")
hist(b1)
abline(v = mean(b1), col = "blue")
hist(b2)
abline(v = mean(b2), col = "blue")
hist(vari)
data_frame <- as.data.frame(cbind(b0, b1, b2, vari, iter = 1:length(b0)))
traceplot <- ggplot(data_frame, aes(x=iter, y=b0)) +
geom_line(aes(size = 0.00001)) +
labs(title="b0 trace")
traceplot
list_of_output <- list(data_frame, traceplot)
return(list_of_output)
}
GibbsSampler(data$attitude, data$extraversion, data$agreeableness, burn.in = 1000, iterations = 10000)
GibbsSampler <- function(Y, X1, X2, burn.in = 1000, iterations = 10000, initial.values = c(1.5, 2, 0.5)) {
## First create space
b0 <- rep(0, iterations)
b1 <- rep(0, iterations)
b2 <- rep(0, iterations)
vari <- rep(0, iterations)
## Then assume the specified initial values
b0[1] <- initial.values[1]
b1[1] <- initial.values[2]
b2[1] <- initial.values[3]
vari[1] <- 1
N <- length(Y)
## In every iteration, the values of the parameters of the conditional
## posteriors should be calculated as functions of the prior parameters,
## the data and the current values of the other model parameters
for(i in 2:iterations) {
## First, we should update the parameters of the intercept (mean and standard deviation) with the following formula:
## 10000 because informative prior?
mean.0.post <- (sum(Y - b1[i - 1]*X1 - b2[i - 1]*X2, na.rm = TRUE)/vari[i - 1] + (mu0 / sigma0)) / ((N / vari[i - 1]) + (mu0 / sigma0))
sd.0.post <- 1 / ((N / vari[i - 1]) + (mu0 / sigma0))
## Then through rnorm we randomly sample from a normal distribution the next value for b0 with the
## above calculated mean and standard deviation
b0[i] <- rnorm(n = 1, mean = mean.0.post, sd = sd.0.post)
## Now, we can update the mean and standard deviation of the regression coefficient for the first variable
## using the updated value of the intercept
mean.1.post <- (sum(X1*(Y - b0[i] - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1] + (mu1 / sigma1)) / sum(X1^2, na.rm = TRUE)/vari[i - 1] + (mu1 / sigma1)
sd.1.post <- 1/((sum(X1^2, na.rm = TRUE) / vari[i - 1]) + (mu1 / sigma1))
## Then again we randomly sample a new b1
b1[i] <- rnorm(n = 1, mean = mean.1.post, sd = sd.1.post)
## Afterwards, we update the mean and standard deviation of the regression coefficient for the second variable
## using the updated value of both the intercept and b1
mean.2.post <- (sum(X2*(Y - b0[i] - b1[i]*X1), na.rm = TRUE)/vari[i - 1] + (mu2 / sigma2)) / sum(X2^2, na.rm = TRUE)/vari[i - 1] + (mu2 / sigma2)
sd.2.post <- 1/((sum(X2^2, na.rm = TRUE) / vari[i - 1]) + (mu2 / sigma2))
## Then, we randomly sample a new b2
b2[i] <- rnorm(n = 1, mean = mean.2.post, sd = sd.2.post)
## Finally, we update the parameters for the distribution of our standard deviation using all above updated values
a.post <- N / 2 + a.prior
b.post <- sum((Y - (b0[i] + b1[i]*X1 + b2[i]*X2))^2, na.rm = TRUE) / 2 + b.prior
## And randomly sample a new variance again for
vari[i] <- 1/rgamma(n = 1, shape = a.post, rate = b.post)
}
b0 <- b0[-c(1:burn.in)]
b1 <- b1[-c(1:burn.in)]
b2 <- b2[-c(1:burn.in)]
vari <- vari[-c(1:burn.in)]
## Now, we also want the posterior distributions (histograms) of all parameters
par(mfrow = c(2,2))
hist(b0)
abline(v = mean(b0), col = "blue")
hist(b1)
abline(v = mean(b1), col = "blue")
hist(b2)
abline(v = mean(b2), col = "blue")
hist(vari)
data_frame <- as.data.frame(cbind(b0, b1, b2, vari, iter = 1:length(b0)))
traceplot <- ggplot(data_frame, aes(x=iter, y=b0)) +
geom_line() +
labs(title="b0 trace")
traceplot
list_of_output <- list(data_frame, traceplot)
return(list_of_output)
}
GibbsSampler(data$attitude, data$extraversion, data$agreeableness, burn.in = 1000, iterations = 10000)
GibbsSampler <- function(Y, X1, X2, burn.in = 1000, iterations = 10000, initial.values = c(1.5, 2, 0.5)) {
## First create space
b0 <- rep(0, iterations)
b1 <- rep(0, iterations)
b2 <- rep(0, iterations)
vari <- rep(0, iterations)
## Then assume the specified initial values
b0[1] <- initial.values[1]
b1[1] <- initial.values[2]
b2[1] <- initial.values[3]
vari[1] <- 1
N <- length(Y)
## In every iteration, the values of the parameters of the conditional
## posteriors should be calculated as functions of the prior parameters,
## the data and the current values of the other model parameters
for(i in 2:iterations) {
## First, we should update the parameters of the intercept (mean and standard deviation) with the following formula:
## 10000 because informative prior?
mean.0.post <- (sum(Y - b1[i - 1]*X1 - b2[i - 1]*X2, na.rm = TRUE)/vari[i - 1] + (mu0 / sigma0)) / ((N / vari[i - 1]) + (mu0 / sigma0))
sd.0.post <- 1 / ((N / vari[i - 1]) + (mu0 / sigma0))
## Then through rnorm we randomly sample from a normal distribution the next value for b0 with the
## above calculated mean and standard deviation
b0[i] <- rnorm(n = 1, mean = mean.0.post, sd = sd.0.post)
## Now, we can update the mean and standard deviation of the regression coefficient for the first variable
## using the updated value of the intercept
mean.1.post <- (sum(X1*(Y - b0[i] - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1] + (mu1 / sigma1)) / sum(X1^2, na.rm = TRUE)/vari[i - 1] + (mu1 / sigma1)
sd.1.post <- 1/((sum(X1^2, na.rm = TRUE) / vari[i - 1]) + (mu1 / sigma1))
## Then again we randomly sample a new b1
b1[i] <- rnorm(n = 1, mean = mean.1.post, sd = sd.1.post)
## Afterwards, we update the mean and standard deviation of the regression coefficient for the second variable
## using the updated value of both the intercept and b1
mean.2.post <- (sum(X2*(Y - b0[i] - b1[i]*X1), na.rm = TRUE)/vari[i - 1] + (mu2 / sigma2)) / sum(X2^2, na.rm = TRUE)/vari[i - 1] + (mu2 / sigma2)
sd.2.post <- 1/((sum(X2^2, na.rm = TRUE) / vari[i - 1]) + (mu2 / sigma2))
## Then, we randomly sample a new b2
b2[i] <- rnorm(n = 1, mean = mean.2.post, sd = sd.2.post)
## Finally, we update the parameters for the distribution of our standard deviation using all above updated values
a.post <- N / 2 + a.prior
b.post <- sum((Y - (b0[i] + b1[i]*X1 + b2[i]*X2))^2, na.rm = TRUE) / 2 + b.prior
## And randomly sample a new variance again for
vari[i] <- 1/rgamma(n = 1, shape = a.post, rate = b.post)
}
b0 <- b0[-c(1:burn.in)]
b1 <- b1[-c(1:burn.in)]
b2 <- b2[-c(1:burn.in)]
vari <- vari[-c(1:burn.in)]
## Now, we also want the posterior distributions (histograms) of all parameters
par(mfrow = c(2,2))
hist(b0)
abline(v = mean(b0), col = "blue")
hist(b1)
abline(v = mean(b1), col = "blue")
hist(b2)
abline(v = mean(b2), col = "blue")
hist(vari)
data_frame <- as.data.frame(cbind(b0, b1, b2, vari, iter = 1:length(b0)))
traceplot1 <- ggplot(data_frame, aes(x=iter, y=b0)) +
geom_line() +
labs(title="b0 trace")
traceplot1
traceplot2 <- ggplot(data_frame, aes(x=iter, y=b1)) +
geom_line() +
labs(title="b1 trace")
traceplot2
traceplot3 <- ggplot(data_frame, aes(x=iter, y=b2)) +
geom_line() +
labs(title="b2 trace")
traceplot3
traceplot4 <- ggplot(data_frame, aes(x=iter, y=vari)) +
geom_line() +
labs(title="var trace")
traceplot4
list_of_output <- list(data_frame, traceplot1, traceplot2, traceplot3, traceplot4)
return(list_of_output)
}
GibbsSampler(data$attitude, data$extraversion, data$agreeableness, burn.in = 1000, iterations = 10000)
?dnorm
y <- rep(0, 5000)
posterior <- function(y){
return(ifelse(y<0|y>1, 0, 2*y))
}
iter <- 5000
y <- rep(0, 5000)
y[1] <- .8
for(i in 2:iter) {
prop_value <- y[i-1] + runif(1, 0, 1)
random_value <- runif(1, 0, 1)
acceptance_ratio <- posterior(prop_value)/posterior(y[i-1])
If(random_value < acceptance_ratio) {
y[i] <- prop_value
} else {
y[i] <- y[i - 1]
}
}
for(i in 2:iter) {
prop_value <- y[i-1] + runif(1, 0, 1)
random_value <- runif(1, 0, 1)
acceptance_ratio <- posterior(prop_value)/posterior(y[i-1])
If(random_value < acceptance_ratio) {
y[i] <- prop_value
} else {
y[i] <- y[i - 1]
}
}
for(i in 2:iter) {
prop_value <- y[i - 1] + runif(1, 0, 1)
random_value <- runif(1, 0, 1)
acceptance_ratio <- posterior(prop_value)/posterior(y[i - 1])
If(random_value < acceptance_ratio) {
y[i] <- prop_value
} else {
y[i] <- y[i - 1]
}
}
for(i in 2:iter) {
prop_value <- y[i - 1] + runif(1, 0, 1)
random_value <- runif(1, 0, 1)
acceptance_ratio <- posterior(prop_value)/posterior(y[i - 1])
If(random_value < acceptance_ratio) {
y[i] <- prop_value } else { y[i] <- y[i - 1] }
}
?ifelse
for(i in 2:iter) {
prop_value <- y[i - 1] + runif(1, 0, 1)
random_value <- runif(1, 0, 1)
acceptance_ratio <- posterior(prop_value)/posterior(y[i - 1])
if(random_value < acceptance_ratio) {
y[i] <- prop_value } else { y[i] <- y[i - 1] }
}
hist(y)
## Initial value
y[1] <- .1
for(i in 2:iter) {
prop_value <- y[i - 1] + runif(1, 0, 1)
random_value <- runif(1, 0, 1)
acceptance_ratio <- posterior(prop_value)/posterior(y[i - 1])
if(random_value < acceptance_ratio) {
y[i] <- prop_value } else { y[i] <- y[i - 1] }
}
hist(y)
## Initial value
y[1] <- .000001
for(i in 2:iter) {
prop_value <- y[i - 1] + runif(1, 0, 1)
random_value <- runif(1, 0, 1)
acceptance_ratio <- posterior(prop_value)/posterior(y[i - 1])
if(random_value < acceptance_ratio) {
y[i] <- prop_value } else { y[i] <- y[i - 1] }
}
hist(y)
for(i in 2:iter) {
prop_value <- y[i - 1] + runif(1, 0, 1)
random_value <- runif(1, 0, 1)
acceptance_ratio <- posterior(prop_value)/posterior(y[i - 1])
if(random_value < acceptance_ratio) {
y[i] <- prop_value } else { y[i] <- y[i - 1] }
}
hist(y)
## Set number of iterations
iter <- 10000
y <- rep(0, 10000)
## Initial value
y[1] <- .000001
for(i in 2:iter) {
prop_value <- y[i - 1] + runif(1, 0, 1)
random_value <- runif(1, 0, 1)
acceptance_ratio <- posterior(prop_value)/posterior(y[i - 1])
if(random_value < acceptance_ratio) {
y[i] <- prop_value } else { y[i] <- y[i - 1] }
}
hist(y)
plot(y)
remove(ls = ())
remove(ls())
remove(ls = ())
remove()
remove(list = ls())
knitr::opts_chunk$set(echo = TRUE)
bayesdata <- read.csv("Exercise 3 - Data.txt")
bayesdata <- source("Exercise 3 - Data.txt")
View(dat)
View(bayesdata)
source("Exercise 3 - Data.txt")
remove(list = ls())
source("Exercise 3 - Data.txt")
source("Exercise 2 - Data.sav")
source("Exercise 2 - Data.sav")
rawdata <- read.csv("Exercise 2 - Data.sav")
View(rawdata)
library(haven)
rawdata <- read_sav("Exercise 2 - Data.sav")
View(rawdata)
View(dat)
View(rawdata)
View(rawdata)
rm(rawdata)
View(dat)
dat$LD.PE
?dber
??dber
??bernoulli
library(rjags)
knitr::opts_chunk$set(echo = TRUE)
library(haven)
library(rjags)
ex2 <- read_sav("Exercise 2 - Data.sav")
library("rjags")
## Step 1: Data input ----------------------------------------------------------
source('Exercise 1 - Data.txt')
## Step 2: Specifying model ----------------------------------------------------
## This is done in the Exercise 1 - model.txt file
## Step 3: Obtaining initial values --------------------------------------------
## Not necessary in this exercise as this is done automatically
## However, if you have problems with convergence, you might want to manually
## do this part.
## Step 4: Obtaining samples from the posterior distribution of the parameters -
## Defining the model from the file into R
model.def <- jags.model(file = "Exercise 1 - Model.txt", data = dat, n.chains = 2)
## Burn-in iterations
update(object = model.def, n.iter = 1000)
## Set monitors on the parameters of interest and draw a large number of samples from the posterior
## distribution
parameters <- c('theta.PE', 'theta.PC', 'RR')
res <- coda.samples(model = model.def, variable.names = parameters, n.iter = 10000)
## Step 5: Inspecting convergence ----------------------------------------------
## Not done in this exercise. But it is very important! Similar to evaluating
## your assumptions
## Step 6: Substantive interpretation ------------------------------------------
summary(res)
source('Exercise 1 - Data.txt')
source('Exercise 1 - Data.txt')
setwd("~/GitHub/Bayesian-Statistics")
source('Exercise 1 - Data.txt')
## Defining the model from the file into R
model.def <- jags.model(file = "Exercise 1 - Model.txt", data = dat, n.chains = 2)
## Burn-in iterations
update(object = model.def, n.iter = 1000)
parameters <- c('theta.PE', 'theta.PC', 'RR')
res <- coda.samples(model = model.def, variable.names = parameters, n.iter = 10000)
summary(res)
source("Exercise 3 - Data.txt")
library(rjags)
dat$LD.PE
model.def <- jags.model(file = "Exercise 2 - Model.txt", data = ex2,
n.chains = 2,
inits = list(theta.PC = .45, theta.PE = .55))
model.def <- jags.model(file = "Exercise 3 - Model_template.txt", data = dat,
n.chains = 2,
inits = list(theta.PC = .45, theta.PE = .55))
?dbern
??dbern
dbernoulli(1)
?dbernoulli
??dbernoulli
model.def <- jags.model(file = "Exercise 3 - Model_template.txt", data = dat,
n.chains = 2,
inits = list(theta.PC = .45, theta.PE = .55))
model.def <- jags.model(file = "Exercise 3 - Model_template.txt", data = dat,
n.chains = 2,
inits = list(theta.PC = .45, theta.PE = .55))
?dbin
model.def <- jags.model(file = "Exercise 3 - Model_template.txt", data = dat,
n.chains = 2,
inits = list(theta.PC = .45, theta.PE = .55))
model.def <- jags.model(file = "Exercise 3 - Model_template.txt", data = dat,
n.chains = 2,
inits = list(theta.PC = .45, theta.PE = .55))
?dbinom
model.def <- jags.model(file = "Exercise 3 - Model_template.txt", data = dat,
n.chains = 2,
inits = list(theta.PC = .45, theta.PE = .55))
model.def <- jags.model(file = "Exercise 3 - Model_template.txt", data = dat,
n.chains = 2,
inits = list(theta.PC = .45, theta.PE = .55))
model.def <- jags.model(file = "Exercise 3 - Model_template.txt", data = dat,
n.chains = 2,
inits = list(theta.PC = .45, theta.PE = .55))
update(model.def, n.iter = 2500)
parameters <- c('theta.PE', 'theta.PC', 'RR')
set.seed(3)
results <- coda.samples(model = model.def, variable.names = parameters, n.iter = 10000)
summary(results)
View(res)
summary(results)
summary(res)
plot(results)
## Autocorrelation plots
autocorr.plot(results)
## Gelman-Rubin diagnostic plot
gelman.plot(results)
proportion.half1.PE <- sum(dat$LD.PE[1:70])/70
proportion.half2.PE <- sum(dat$LD.PE[71:141])/71
diff.PE <- proportion.half1.PE - proportion.half2.PE
# test statistic for the PC condition
proportion.half1.PC <- sum(dat$LD.PC[1:71])/71
proportion.half2.PC <- sum(dat$LD.PC[72:143])/72
diff.PC <- proportion.half1.PC - proportion.half2.PC
# Extract the parameter estimates
theta.PE.chain1 <- samples[[1]][,"theta.PE"]
# Extract the parameter estimates
theta.PE.chain1 <- results[[1]][,"theta.PE"]
theta.PE.chain1 <- results[[1]][,"theta.PE"]
theta.PE.chain2 <- results[[2]][,"theta.PE"]
theta.PC.chain1 <- results[[1]][,"theta.PC"]
theta.PC.chain2 <- results[[2]][,"theta.PC"]
library(LaplacesDemon)
library(LaplacesDemon)
# Storage room (each row is a replicated dataset) :
replicated.PE.chain1 <- array(data = NA, dim = c(length(theta.PE.chain1), dat$n.PE))
replicated.PE.chain2 <- array(data = NA, dim = c(length(theta.PE.chain2), dat$n.PE))
replicated.PC.chain1 <- array(data = NA, dim = c(length(theta.PC.chain1), dat$n.PC))
replicated.PC.chain2 <- array(data = NA, dim = c(length(theta.PC.chain2), dat$n.PC))
# Sample replicated datasets
# For each parameter estimate...
for(t in 1:length(theta.PE.chain1)) {
# ... sample a replicated dataset by sampling n times from the Bernoulli distribution
# with as probability of success on each trial the parameter estimate
replicated.PE.chain1[t,] <- rbern(n = dat$n.PE, prob = theta.PE.chain1[t])
replicated.PE.chain2[t,] <- rbern(n = dat$n.PE, prob = theta.PE.chain2[t])
replicated.PC.chain1[t,] <- rbern(n = dat$n.PC, prob = theta.PC.chain1[t])
replicated.PC.chain2[t,] <- rbern(n = dat$n.PC, prob = theta.PC.chain2[t])
}
View(replicated.PE.chain2)
View(replicated.PC.chain1)
View(replicated.PC.chain2)
?colleng
View(replicated.PC.chain1)
test.statistics.PE <- rep(NA, ncol(replicated.PE.chain1) + ncol(replicated.PE.chain2))
View(replicated.PC.chain1)
for(i in 1:ncol(replicated.PE.chain1)){
half1 <- sum(replicated.PE.chain1[1:(nrow(replicated.PE.chain1)/2), i])/(nrow(replicated.PE.chain1)/nrow(replicated.PE.chain1)/2)
half2 <- sum(replicated.PE.chain1[(nrow(replicated.PE.chain1)/2 + 1):nrow(replicated.PE.chain1), i])/(nrow(replicated.PE.chain1)/2 + 1)
test.statistics.PE[i] <- half1 - half2
}
test.statistics.PE
for(i in 1:ncol(replicated.PE.chain1)){
half1 <- sum(replicated.PE.chain1[1:(nrow(replicated.PE.chain1)/2), i])/(nrow(replicated.PE.chain1)/2)
half2 <- sum(replicated.PE.chain1[(nrow(replicated.PE.chain1)/2 + 1):nrow(replicated.PE.chain1), i])/(nrow(replicated.PE.chain1)/2 + 1)
test.statistics.PE[i] <- half1 - half2
}
ncol(replicated.PE.chain1)
for(i in 1:ncol(replicated.PE.chain2)) {
half1 <- sum(replicated.PE.chain2[1:(nrow(replicated.PE.chain2)/2), i])/(nrow(replicated.PE.chain2)/2)
half2 <- sum(replicated.PE.chain2[(nrow(replicated.PE.chain2)/2 + 1):nrow(replicated.PE.chain2), i])/(nrow(replicated.PE.chain2)/2 + 1)
test.statistics.PE[i + ncol(replicated.PE.chain1)] <- half1 - half2
}
test.statistics.PE
test.statistics.PC <- rep(NA, ncol(replicated.PC.chain1) + ncol(replicated.PC.chain2))
for(i in 1:ncol(replicated.PC.chain1)){
half1 <- sum(replicated.PC.chain1[1:(nrow(replicated.PC.chain1)/2), i])/(nrow(replicated.PC.chain1)/2)
half2 <- sum(replicated.PC.chain1[(nrow(replicated.PC.chain1)/2 + 1):nrow(replicated.PC.chain1), i])/(nrow(replicated.PC.chain1)/2 + 1)
test.statistics.PC[i] <- half1 - half2
}
for(i in 1:ncol(replicated.PC.chain2)) {
half1 <- sum(replicated.PC.chain2[1:(nrow(replicated.PC.chain2)/2), i])/(nrow(replicated.PC.chain2)/2)
half2 <- sum(replicated.PC.chain2[(nrow(replicated.PC.chain2)/2 + 1):nrow(replicated.PC.chain2), i])/(nrow(replicated.PC.chain2)/2 + 1)
test.statistics.PC[i + ncol(replicated.PC.chain1)] <- half1 - half2
}
test.statistics.PC
library(tidyverse)
## For PC group:
p.value.prop <- rep(NA, 286)
## For PC group:
p.value.prop <- rep(NA, 286)
ifelse(abs(test.statistics.PC) > diff.PC, p.value.prop = 1, p.value.prop = 0)
?ifelse
ifelse(abs(test.statistics.PC) > diff.PC, p.value.prop <- 1, p.value.prop <- 0)
ifelse(abs(test.statistics.PC) > abs(diff.PC), p.value.prop <- 1, p.value.prop <- 0)
## For PC group:
p.value.prop <- rep(NA, 286)
ifelse(abs(test.statistics.PC) > abs(diff.PC), p.value.prop <- 1, p.value.prop <- 0)
ifelse(test.statistics.PC > diff.PC, p.value.prop <- 1, p.value.prop <- 0)
ifelse(test.statistics.PC < diff.PC, p.value.prop <- 1, p.value.prop <- 0)
ifelse(abs(test.statistics.PC) < abs(diff.PC), p.value.prop <- 1, p.value.prop <- 0)
p.value.prop
## For PC group:
p.value.prop <- rep(NA, 286)
ifelse(abs(test.statistics.PC) < abs(diff.PC), p.value.prop <- 1, p.value.prop <- 0)
ifelse(abs(test.statistics.PC) < abs(diff.PC), p.value.prop <- 1, p.value.prop <- 0)
ifelse(abs(test.statistics.PC) > abs(diff.PC), p.value.prop <- 1, p.value.prop <- 0)
ifelse(abs(test.statistics.PC) > abs(diff.PC), p.value.prop <- 1, p.value.prop <- 0) %>% mean(.)
ifelse(abs(test.statistics.PC) < abs(diff.PC), p.value.prop <- 1, p.value.prop <- 0) %>% mean(.)
## For PC group:
ifelse(abs(test.statistics.PC) > abs(diff.PC), p.value.prop <- 1, p.value.prop <- 0) %>% mean(.)
## For PC group:
ifelse(abs(test.statistics.PC) > abs(diff.PC), 1, 0) %>% mean(.)
## For PE group:
ifelse(abs(test.statistics.PE) > abs(diff.PE), 1, 0) %>% mean(.)
library(tidyverse)
## For PC group:
ifelse(abs(test.statistics.PC) > abs(diff.PC), 1, 0) %>% mean(.)
## For PE group:
ifelse(abs(test.statistics.PE) > abs(diff.PE), 1, 0) %>% mean(.)
## For PE group:
ifelse(abs(test.statistics.PE) < abs(diff.PE), 1, 0) %>% mean(.)
library(tidyverse)
## For PC group:
ifelse(abs(test.statistics.PC) > abs(diff.PC), 1, 0) %>% mean(.)
## For PE group:
ifelse(abs(test.statistics.PE) > abs(diff.PE), 1, 0) %>% mean(.)
test.statistics.PE
## For PE group:
ifelse(abs(test.statistics.PE) > abs(diff.PE), 1, 0) %>% mean(.)
ifelse(test.statistics.PC < diff.PC, 1, 0) %>% mean(.)
ifelse(test.statistics.PC > diff.PC, 1, 0) %>% mean(.)
