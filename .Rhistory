## like to have it sampled through a Gibbs Sampler or Metropolis Hastings algorithm
## by default, it assumes a Gibbs sampler for every regression coefficient,
## and if you want to redefine it to a MH, you should change the 0 to a 1
## for the regression coefficient you want to change
## Examples:
## method = c(0, 1, 0) will make it so that b1 is done through MH
GibbsSampler <- function(Y, X1, X2, burn.in = 1000, iterations = 10000, initial.values = c(1, 1, 1), method = c(0, 0, 0)) {
## First, the function should require ggplot2 as it is used to create traceplots later
library(ggplot2)
## First create space
b0 <- rep(0, iterations)
b1 <- rep(0, iterations)
b2 <- rep(0, iterations)
vari <- rep(0, iterations)
## Then assume the specified initial values
b0[1] <- initial.values[1]
b1[1] <- initial.values[2]
b2[1] <- initial.values[3]
vari[1] <- 1
N <- length(Y)
## In every iteration, the values of the parameters of the conditional
## posteriors should be calculated as functions of the prior parameters,
## the data and the current values of the other model parameters
for(i in 2:iterations) {
## First, we should update the parameters of the intercept (mean and standard deviation) with the following formula:
## 10000 because informative prior?
if(method[1] == 0) {
mean.0.post <- (sum(Y - b1[i - 1]*X1 - b2[i - 1]*X2, na.rm = TRUE)/vari[i - 1] + (mu0 / sigma0)) / ((N / vari[i - 1]) + (mu0 / sigma0))
sd.0.post <- 1 / ((N / vari[i - 1]) + (mu0 / sigma0))
## Then through rnorm we randomly sample from a normal distribution the next value for b0 with the
## above calculated mean and standard deviation
b0[i] <- rnorm(n = 1, mean = mean.0.post, sd = sd.0.post)
} else {
## intercept through MH
}
## Now, we can update the mean and standard deviation of the regression coefficient for the first variable
## using the updated value of the intercept
if(method[2] == 0) {
mean.1.post <- (sum(X1*(Y - b0[i] - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1] + (mu1 / sigma1)) / sum(X1^2, na.rm = TRUE)/vari[i - 1] + (mu1 / sigma1)
sd.1.post <- 1/((sum(X1^2, na.rm = TRUE) / vari[i - 1]) + (mu1 / sigma1))
## Then again we randomly sample a new b1
b1[i] <- rnorm(n = 1, mean = mean.1.post, sd = sd.1.post)
} else {
## b1 through MH
}
## Afterwards, we update the mean and standard deviation of the regression coefficient for the second variable
## using the updated value of both the intercept and b1
if(method[3] == 0) {
mean.2.post <- (sum(X2*(Y - b0[i] - b1[i]*X1), na.rm = TRUE)/vari[i - 1] + (mu2 / sigma2)) / sum(X2^2, na.rm = TRUE)/vari[i - 1] + (mu2 / sigma2)
sd.2.post <- 1/((sum(X2^2, na.rm = TRUE) / vari[i - 1]) + (mu2 / sigma2))
## Then, we randomly sample a new b2
b2[i] <- rnorm(n = 1, mean = mean.2.post, sd = sd.2.post)
} else {
## b2 through MH
}
## Finally, we update the parameters for the distribution of our standard deviation using all above updated values
a.post <- N / 2 + a.prior
b.post <- sum((Y - (b0[i] + b1[i]*X1 + b2[i]*X2))^2, na.rm = TRUE) / 2 + b.prior
## And randomly sample a new variance again for
vari[i] <- 1/rgamma(n = 1, shape = a.post, rate = b.post)
}
b0 <- b0[-c(1:burn.in)]
b1 <- b1[-c(1:burn.in)]
b2 <- b2[-c(1:burn.in)]
vari <- vari[-c(1:burn.in)]
## Now, we also want the posterior distributions (histograms) of all parameters
par(mfrow = c(2,2))
hist(b0, breaks = 50)
abline(v = mean(b0), col = "blue")
hist(b1, breaks = 50)
abline(v = mean(b1), col = "blue")
hist(b2, breaks = 50)
abline(v = mean(b2), col = "blue")
hist(vari, breaks = 50)
data_frame <- as.data.frame(cbind(b0, b1, b2, vari, iter = 1:length(b0)))
traceplot1 <- ggplot(data_frame, aes(x=iter, y=b0)) +
geom_line() +
labs(title="b0 trace")
traceplot1
traceplot2 <- ggplot(data_frame, aes(x=iter, y=b1)) +
geom_line() +
labs(title="b1 trace")
traceplot2
traceplot3 <- ggplot(data_frame, aes(x=iter, y=b2)) +
geom_line() +
labs(title="b2 trace")
traceplot3
traceplot4 <- ggplot(data_frame, aes(x=iter, y=vari)) +
geom_line() +
labs(title="var trace")
traceplot4
list_of_output <- list(data_frame, traceplot1, traceplot2, traceplot3, traceplot4)
return(list_of_output)
}
GibbsSampler(data$attitude, data$extraversion, data$agreeableness, burn.in = 1000, iterations = 10000)
rm(list=ls)
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
source("Exercise 3 - Data.txt")
library(rjags)
model.def <- jags.model(file = "Exercise 3 - Model_template.txt", data = dat,
n.chains = 2,
inits = list(theta.PC = .45, theta.PE = .55))
update(model.def, n.iter = 2500)
parameters <- c('theta.PE', 'theta.PC', 'RR')
set.seed(3)
results <- coda.samples(model = model.def, variable.names = parameters, n.iter = 12500)
summary(results)
## Assessing convergence
plot(results)
## Autocorrelation plots
autocorr.plot(results)
## Gelman-Rubin diagnostic plot
gelman.plot(results)
## Calculating the above defined test statistic
# test statistic for the PE condition
proportion.half1.PE <- sum(dat$LD.PE[1:70])/70
proportion.half2.PE <- sum(dat$LD.PE[71:141])/71
diff.PE <- proportion.half1.PE - proportion.half2.PE
# test statistic for the PC condition
proportion.half1.PC <- sum(dat$LD.PC[1:71])/71
proportion.half2.PC <- sum(dat$LD.PC[72:143])/72
diff.PC <- proportion.half1.PC - proportion.half2.PC
# Extract the parameter estimates
theta.PE.chain1 <- results[[1]][,"theta.PE"]
theta.PE.chain2 <- results[[2]][,"theta.PE"]
theta.PC.chain1 <- results[[1]][,"theta.PC"]
theta.PC.chain2 <- results[[2]][,"theta.PC"]
library(LaplacesDemon)
# Storage room (each row is a replicated dataset) :
replicated.PE.chain1 <- array(data = NA, dim = c(length(theta.PE.chain1), dat$n.PE))
replicated.PE.chain2 <- array(data = NA, dim = c(length(theta.PE.chain2), dat$n.PE))
replicated.PC.chain1 <- array(data = NA, dim = c(length(theta.PC.chain1), dat$n.PC))
replicated.PC.chain2 <- array(data = NA, dim = c(length(theta.PC.chain2), dat$n.PC))
# Sample replicated datasets
# For each parameter estimate...
for(t in 1:length(theta.PE.chain1)) {
# ... sample a replicated dataset by sampling n times from the Bernoulli distribution
# with as probability of success on each trial the parameter estimate
replicated.PE.chain1[t,] <- rbern(n = dat$n.PE, prob = theta.PE.chain1[t])
replicated.PE.chain2[t,] <- rbern(n = dat$n.PE, prob = theta.PE.chain2[t])
replicated.PC.chain1[t,] <- rbern(n = dat$n.PC, prob = theta.PC.chain1[t])
replicated.PC.chain2[t,] <- rbern(n = dat$n.PC, prob = theta.PC.chain2[t])
}
test.statistics.PE <- rep(NA, nrow(replicated.PE.chain1) + nrow(replicated.PE.chain2))
test.statistics.PC <- rep(NA, nrow(replicated.PC.chain1) + nrow(replicated.PC.chain2))
## First, we calculate the statistics for the PE treatment
for(i in 1:nrow(replicated.PE.chain1)){
half1 <- sum(replicated.PE.chain1[i, 1:(ncol(replicated.PE.chain1)/2)])/(ncol(replicated.PE.chain1)/2)
half2 <- sum(replicated.PE.chain1[i, (ncol(replicated.PE.chain1)/2 + 1):ncol(replicated.PE.chain1), i])/(ncol(replicated.PE.chain1)/2 + 1)
test.statistics.PE[i] <- half1 - half2
}
test.statistics.PE <- rep(NA, nrow(replicated.PE.chain1) + nrow(replicated.PE.chain2))
test.statistics.PC <- rep(NA, nrow(replicated.PC.chain1) + nrow(replicated.PC.chain2))
## First, we calculate the statistics for the PE treatment
for(i in 1:nrow(replicated.PE.chain1)){
half1 <- sum(replicated.PE.chain1[i, 1:(ncol(replicated.PE.chain1)/2)])/(ncol(replicated.PE.chain1)/2)
half2 <- sum(replicated.PE.chain1[i, (ncol(replicated.PE.chain1)/2 + 1):ncol(replicated.PE.chain1)])/(ncol(replicated.PE.chain1)/2 + 1)
test.statistics.PE[i] <- half1 - half2
}
for(i in 1:nrow(replicated.PE.chain2)) {
half1 <- sum(replicated.PE.chain2[i, 1:(ncol(replicated.PE.chain2)/2)])/(ncol(replicated.PE.chain2)/2)
half2 <- sum(replicated.PE.chain2[i, (ncol(replicated.PE.chain2)/2 + 1):ncol(replicated.PE.chain2)])/(ncol(replicated.PE.chain2)/2 + 1)
test.statistics.PE[i + nrow(replicated.PE.chain1)] <- half1 - half2
}
## Then the statistic for the PC treatment
for(i in 1:nrow(replicated.PC.chain1)){
half1 <- sum(replicated.PC.chain1[i, 1:(ncol(replicated.PC.chain1)/2)])/(ncol(replicated.PC.chain1)/2)
half2 <- sum(replicated.PC.chain1[i, (ncol(replicated.PC.chain1)/2 + 1):ncol(replicated.PC.chain1)])/(ncol(replicated.PC.chain1)/2 + 1)
test.statistics.PC[i] <- half1 - half2
}
for(i in 1:nrow(replicated.PC.chain2)) {
half1 <- sum(replicated.PC.chain2[i, 1:(ncol(replicated.PC.chain2)/2)])/(ncol(replicated.PC.chain2)/2)
half2 <- sum(replicated.PC.chain2[i, (ncol(replicated.PC.chain2)/2 + 1):ncol(replicated.PC.chain2)])/(ncol(replicated.PC.chain2)/2 + 1)
test.statistics.PC[i + nrow(replicated.PC.chain1)] <- half1 - half2
}
test.statistics.PE
test.statistics.PC
library(tidyverse)
## For PC group:
ifelse(abs(test.statistics.PC) > abs(diff.PC), 1, 0) %>% mean(.)
## For PE group:
ifelse(abs(test.statistics.PE) > abs(diff.PE), 1, 0) %>% mean(.)
ifelse(abs(test.statistics.PC) > abs(diff.PC), 1, 0) %>% mean(.)
## For PE group:
ifelse(abs(test.statistics.PE) > abs(diff.PE), 1, 0) %>% mean(.)
library(tidyverse)
## For PC group:
ifelse(abs(test.statistics.PC) > abs(diff.PC), 1, 0) %>% mean(.)
## For PE group:
ifelse(abs(test.statistics.PE) > abs(diff.PE), 1, 0) %>% mean(.)
?pnorm
pnorm(1)
pnorm(1, 20)
pnorm(9)
pnorm(-1)
pnorm(0)
load('dataEx1Lab3.Rdata')
knitr::opts_chunk$set(echo = TRUE)
load('dataEx1Lab3.Rdata')
studyh_marg <- lm(studyhours~IQ,data=study_succes_data)
summary(studyh_marg)
mean(study_succes_data$studyhours)
studyh_cond <- lm(studyhours~IQ+studysucces,data=study_succes_data)
summary(studyh_cond)
hist(study_succes_data$studysucces)
hist(study_succes_data$studysucces)
abline(v=95, col="red", lwd=3)
honors_data = study_succes_data[which(study_succes_data$studysucces>95),]
hist(study_succes_data$studysucces)
abline(v=95, col="red", lwd=3)
honors_data = study_succes_data[which(study_succes_data$studysucces>95),]
studyh_honors <- lm(studyhours~IQ,data=honors_data)
summary(studyh_honors)
plot(study_succes_data$IQ, study_succes_data$studyhours, main = "Relatioship IQ and Study Success", ylab="Study Succes", xlab="IQ", pch=19, col="grey15")
abline(a = studyh_marg$coefficients[1] , b = studyh_marg$coefficients[2], col = "black",lwd=3)
points(honors_data$IQ, honors_data$studyhours, pch=8,col="blue",cex=1.2)
abline(a = studyh_honors $coefficients[1] , b = studyh_honors $coefficients[2], col = "blue", lwd=2)
set.seed(113221701) # set the seed for comparison
n <- 600          # total sample size
IQ <- rnorm(n,115,7)
studyhours<- rnorm(n,30,5)
studysucces<- 25 + .25*IQ + 1.25*studyhours + rnorm(n,0,5)
IQ <- 1
EVstudyhours_IQ1 <- 30
EVstudysucces <- 25 + .25*IQ + 1.25*EVstudyhours_IQ1
###IQ set to 1###
IQ <- 0
EVstudyhours_IQ0<- 30
EVstudysucces <- 25 + .25*IQ + 1.25*EVstudyhours_IQ0
EVstudyhours_IQ1-EVstudyhours_IQ0
library(dagitty)
library(ggdag)
library(qgraph)
kiddag <- dagify(
HP ~ Tr + KSz
Tr ~ KSz
kiddag <- dagify(
HP ~ Tr + KSz,
Tr ~ KSz,
exposure = Tr,
outcome = HP
)
kiddag <- dagify(
HP ~ TR + KSz,
TR ~ KSz,
exposure = TR,
outcome = HP
)
ggdag_status(kiddag) + theme_dag()
kiddag <- dagify(
TR ~ KSz,
HP ~ TR + KSz,
exposure = TR,
outcome = HP
)
kiddag <- dagify(
TR ~ KSz,
HP ~ TR + KSz,
exposure = "TR",
outcome = "HP"
)
ggdag_status(kiddag) + theme_dag()
wellb_marg <- lm(wellbeing~treatment,data=kidneystone_data)
summary(wellb_marg)
wellb_marg <- lm(wellbeing~treatment,data=kidneystone_data)
summary(wellb_marg)
wellb_cond <- lm(wellbeing~treatment+stonesize,data=kidneystone_data)
summary(wellb_cond)
plot(kidneystone_data$treatment, kidneystone_data$wellbeing, main = "Relationship treatment and Wellbeing", ylab="Wellbeing", xlab="treatment", pch=19, col="grey15")
abline(a = wellb_marg$coefficients[1] , b = wellb_marg$coefficients[2], col = "black",lwd=3)
points(larges_data$treatment, larges_data$wellbeing, pch=8,col="blue",cex=1.2)
plot(kidneystone_data$treatment, kidneystone_data$wellbeing, main = "Relationship treatment and Wellbeing", ylab="Wellbeing", xlab="treatment", pch=19, col="grey15")
abline(a = wellb_marg$coefficients[1] , b = wellb_marg$coefficients[2], col = "black",lwd=3)
points(larges_data$treatment, larges_data$wellbeing, pch=8,col="blue",cex=1.2)
plot(kidneystone_data$treatment, kidneystone_data$wellbeing, main = "Relationship treatment and Wellbeing", ylab="Wellbeing", xlab="treatment", pch=19, col="grey15")
abline(a = wellb_marg$coefficients[1] , b = wellb_marg$coefficients[2], col = "black",lwd=3)
points(larges_data$treatment, larges_data$wellbeing, pch=8,col="blue",cex=1.2)
plot(kidneystone_data$treatment, kidneystone_data$wellbeing, main = "Relationship treatment and Wellbeing", ylab="Wellbeing", xlab="treatment", library(ggplot2)
# Gradient color
ggplot(kidneystone_data, aes(x = treatment, y = wellbeing, colour = stonesize)) +
library(ggplot2)
library(viridis)
library(ggplot2)
library(viridis)
# Gradient color
ggplot(kidneystone_data, aes(x = treatment, y = wellbeing, colour = stonesize)) +
geom_point()+
scale_color_viridis(option = "C")
set.seed(133222032) # set the seed for comparison
n <- 500          # total sample size
stonesize <- rnorm(n,8,2) # averge stone size of 8 mm (yikes)
treatment=rep(0,n)
treatment[which(stonesize>9)] <- 1  #Treatment is assigned deterministically (no probability involved here) completely based on stone size. People with large stones (>9) get treatment '1'.
wellbeing <- 30 + 2*treatment - 2*stonesize + rnorm(n,0,1)
## treatment = 1
stonesize_Average = 8
treatment = 1
wellbeing_treat1 <- 30 + 2*treatment - 2*stonesize_Average
## treatment = 1
treatment = 0
wellbeing_treat0 <- 30 + 2*treatment - 2*stonesize_Average
wellbeing_treat1 - wellbeing_treat0
ggdag_status(flowerdag) + theme_dag()
flowerdag <- daggify(
PR ~ SL,
BA ~ SL,
exposure = "SL",
outcome = "PR"
)
flowerdag <- dagify(
PR ~ SL,
BA ~ SL,
exposure = "SL",
outcome = "PR"
)
ggdag_status(flowerdag) + theme_dag()
flowerdag <- dagify(
PR ~ SL + BA,
BA ~ SL,
exposure = "SL",
outcome = "PR"
)
ggdag_status(flowerdag) + theme_dag()
pollen_marg <- lm(pollen~sepal_length,data=pollen_data)
summary(pollen_marg)
pollen_cond <- lm(pollen~sepal_length+bloom_attract,data=pollen_data)
summary(pollen_cond)
plot(pollen_data$sepal_length, pollen_data$pollen, main = "Relationship sepal_length and pollen", ylab="pollen", xlab="sepal_length", pch=19, col="grey15")
abline(a = pollen_marg$coefficients[1] , b = pollen_marg$coefficients[2], col = "black",lwd=3)
points(highatt_data$sepal_length, highatt_data$pollen, pch=8,col="blue",cex=1.2)
plot(pollen_data$sepal_length, pollen_data$pollen, main = "Relationship sepal_length and pollen", ylab="pollen", xlab="sepal_length", pch=19, col="grey15")
abline(a = pollen_marg$coefficients[1] , b = pollen_marg$coefficients[2], col = "black",lwd=3)
points(highatt_data$sepal_length, highatt_data$pollen, pch=8,col="blue",cex=1.2)
set.seed(133222058) # set the seed for comparison
n <- 500          # total sample size
sepal_length <- rnorm(n,5,1)
bloom_attract= 25+5*sepal_length + rnorm(n,0,5)
pollen <- 0 + .02*bloom_attract + .01*sepal_length + rnorm(n,0,.05)
## sepal length is 1
sepal_length <- 1
bloom_attract <- 25*5*sepal_length
pollen1 <- 0 + .02 * bloom_attract + .01 * sepal_length
## sepal length is 0
sepal_length <- 0
bloom_attract <- 25*5*sepal_length
pollen0 <- 0 + .02 * bloom_attract + .01 * sepal_length
pollen1 - pollen0
## sepal length is 1
sepal_length <- 1
bloom_attract <- 25+5*sepal_length
pollen1 <- 0 + .02 * bloom_attract + .01 * sepal_length
## sepal length is 0
sepal_length <- 0
bloom_attract <- 25+5*sepal_length
pollen0 <- 0 + .02 * bloom_attract + .01 * sepal_length
pollen1 - pollen0
set.seed(482) # set the seed for comparison
N <- 500          # total sample size
# For Y1
mY1 <- 115    # mean on pretest
sdY1 <- 20    # sd on pretest
Y1 <- rnorm(N,mY1,sdY1)   # simulate Y1
# For X
X <- rbinom(N,1,0.5)      # simulate treatment
# For Y2
b0 <- 70          # intercept
b1 <- 20          # causal effect
b2 <- .4          # covariate effect
sdE2 <- 5         # within-group residual sd
Y2 <- b0 + b1*X + b2*Y1 + rnorm(N,0,sdE2)
dat1 <- data.frame(Y1,Y2,X)
round(cor(dat1),3)
# For plotting it may be useful to find the overall minimum
# and maximum across the pretest and posttest, to make the two
# axes comparable
minY <- min(c(Y1,Y2))
maxY <- max(c(Y1,Y2))
plot(x=dat1$Y1, y=dat1$Y2, col = (dat1$X+1),
xlim=c(minY,maxY),ylim=c(minY,maxY),
xlab="Y1", ylab="Y2")
# Means of:
# - pretest non-treatment group (mY10)
# - pretest treatment group (mY11)
# Note that both should be  about mY1=115 (value used in simulation)
mY10 <- mean(Y1[X==0])
mY11 <- mean(Y1[X==1])
# Means of:
# - posttest non-treatement group (mY20)
# - posttest treatment group (mY21)
# Note that the first should be about:
# mY20 = b0 + b2*mY10 = 70 + 0.4*115 = 116
# and the second should be about:
# mY21 = b0 + b1 + b2*mY11 = 70 + 20 + 0.4*115 = 136
mY20 <- mean(Y2[X==0])
mY21 <- mean(Y2[X==1])
# Gather means on both occasions per treatment condition
mYX0 <- c(mY10,mY20)
mYX1 <- c(mY11,mY21)
minY <- min(mYX0,mYX1)
maxY <- max(mYX0,mYX1)
plot(c(1,2), mYX0, type="l",
xlim=c(0.7,2.3),
ylim=c(minY-5,maxY+5),xaxt="n",
xlab="time",
ylab="Y")
lines(c(1,2),mYX1,col="red")
points(c(1,2),mYX1,pch=19,cex=1.3,col="red")
points(c(1,2),mYX0,pch=19,cex=1.3)
axis(side=1, at=seq(1, 2, by=1))
ANCOVA <- lm(Y2 ~ X + Y1, data=dat1)
summary(ANCOVA)
CSA <- lm((Y2-Y1) ~ X, data=dat1)
summary(CSA)
Y2onX <- lm((Y2) ~ X, data=dat1)
summary(Y2onX)
set.seed(934) # set the seed for comparison
N <- 5000 # sample size
U <- rnorm(N)  # Unmeasured time-invariant confounder
# Create a treatment variable that is dependent on the unmeasured confounder
z <- -1.1*(U)           # linear combination with noise
pr <- 1/(1+exp(-z))               # pass through an inv-logit function
X <- rbinom(N,1,pr)               # bernoulli treatment variable
cor(X,U)                        # check the correlation
# Create the pre-test and the post-test
Y1 <- 10 + 0.7*U + rnorm(N)
Y2 <- 11 + 0.7*U + 0.5*X + rnorm(N)
dat5 <- data.frame(U,X,Y1,Y2)
round(cor(dat5),2)
set.seed(934) # set the seed for comparison
N <- 5000 # sample size
U <- rnorm(N)  # Unmeasured time-invariant confounder
# Create a treatment variable that is dependent on the unmeasured confounder
z <- -1.1*(U)           # linear combination with noise
pr <- 1/(1+exp(-z))               # pass through an inv-logit function
X <- rbinom(N,1,pr)               # bernoulli treatment variable
cor(X,U)                        # check the correlation
# Create the pre-test and the post-test
Y1 <- 10 + 0.7*U + rnorm(N)
Y2 <- 11 + 0.7*U + 0.5*X + rnorm(N)
dat5 <- data.frame(U,X,Y1,Y2)
round(cor(dat5),2)
# Create the pre-test and the post-test
Y1 <- 10 + 0.7*U + rnorm(N)
Y2 <- 11 + 0.7*U + 0.5*X + rnorm(N)
dat5 <- data.frame(U,X,Y1,Y2)
round(cor(dat5),2)
## Marginal model:
Y2onX <- lm(Y2 ~ X, data=dat5)
summary(Y2onX)
## ANCOVA model:
ANCOVA <- lm(Y2 ~ X + Y1, data=dat5)
summary(ANCOVA)
## CS model:
CSA <- lm((Y2-Y1) ~ X, data=dat5)
summary(CSA)
library("rjags")
source('Exercise 1 - Data.txt')
## Defining the model from the file into R
model.def <- jags.model(file = "Exercise 1 - Model.txt", data = dat, n.chains = 2)
## Burn-in iterations
update(object = model.def, n.iter = 1000)
parameters <- c('theta.PE', 'theta.PC', 'RR')
res <- coda.samples(model = model.def, variable.names = parameters, n.iter = 10000)
summary(res)
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
dat <- read.table("Lab-2.dat", header = TRUE)
library(recoder)
for(i in 5:10) {
dat[, i] <- recoder(dat[, i],"1:4;2:3;3:2;4:1")
}
cor(dat[, 5:12], use = "pairwise")
library(lavaan)
model <- "f1=~item3+item5+item6+item8
f2=~item1+item2+item4+item7"
factor.analysis <- cfa(model = model, data = dat)
summary(factor.analysis, fit.measures = TRUE)
variance.factor.analysis <- cfa(model = model, data = dat, std.lv = TRUE)
summary(variance.factor.analysis, fit.measures = TRUE)
theta <- inspect(variance.factor.analysis, "est")$theta
sigma <- fitted(variance.factor.analysis)$cov
R <- 1 - sum(diag(theta))/sum(diag(sigma))
R
lambda <- inspect(variance.factor.analysis, "est")$lambda
psi <- inspect(variance.factor.analysis, "est")$psi
psi_1 <- diag(psi)[1]
psi_2 <- diag(psi)[2]
weights_1 <- c(1,1,1,1,0,0,0,0)
weights_2 <- c(0,0,0,0,1,1,1,1)
e_1 <- c(1, 0)
e_2 <- c(0, 1)
validity_S1_C1 <- (t(weights_1) %*% lambda %*% psi %*% e_1)^2 / (t(weights_1) %*% sigma %*% weights_1 %*% psi_1)
validity_S1_C1
validity_S2_C2 <- (t(weights_2) %*% lambda %*% psi %*% e_2)^2 / (t(weights_2) %*% sigma %*% weights_2 %*% psi_2)
validity_S2_C2
thur_scores <- psi %*% t(lambda) %*% solve(sigma)
weights_thur1 <- thur_scores[1 ,]
weights_thur2 <- thur_scores[2 ,]
## Thurstone scores for f1
## FOR C1
(t(weights_thur1) %*% lambda %*% psi %*% e_1)^2 / (t(weights_thur1) %*% sigma %*% weights_thur1 %*% psi_1)
## FOR C2
(t(weights_thur1) %*% lambda %*% psi %*% e_2)^2 / (t(weights_thur1) %*% sigma %*% weights_thur1 %*% psi_2)
## Thrstone scores for f2
## FOR C1
(t(weights_thur2) %*% lambda %*% psi %*% e_1)^2 / (t(weights_thur2) %*% sigma %*% weights_thur2 %*% psi_1)
## FOR C2
(t(weights_thur2) %*% lambda %*% psi %*% e_2)^2 / (t(weights_thur2) %*% sigma %*% weights_thur2 %*% psi_2)
## Centered data matrix:
cent.dat <- scale(dat[, 5:12])
test <- cent.dat %*% solve(sigma) %*% lambda %*% psi
