b1 <- rep(0, iterations)
b2 <- rep(0, iterations)
vari <- rep(0, iterations)
## Then assume the specified initial values
b0[1] <- initial.values[1]
b1[1] <- initial.values[2]
b2[1] <- initial.values[3]
vari[1] <- 1
N <- length(Y)
## In every iteration, the values of the parameters of the conditional
## posteriors should be calculated as functions of the prior parameters,
## the data and the current values of the other model parameters
for(i in 2:iterations) {
## First, we should update the parameters of the intercept (mean and standard deviation) with the following formula:
## 10000 because informative prior?
if(method[1] = 0) {
## a burn.in period (by default 1000 iterations)
## a total amount of iterations (by default 10000)
## the initial values for every regression coefficient
## a method function that specifies per parameter whether they would
## like to have it sampled through a Gibbs Sampler or Metropolis Hastings algorithm
## by default, it assumes a Gibbs sampler for every regression coefficient,
## and if you want to redefine it to a MH, you should change the 0 to a 1
## for the regression coefficient you want to change
## Examples:
## method = c(0, 1, 0) will make it so that b1 is done through MH
GibbsSampler <- function(Y, X1, X2, burn.in = 1000, iterations = 10000, initial.values = c(1, 1, 1), method = c(0, 0, 0)) {
## First create space
b0 <- rep(0, iterations)
b1 <- rep(0, iterations)
b2 <- rep(0, iterations)
vari <- rep(0, iterations)
## Then assume the specified initial values
b0[1] <- initial.values[1]
b1[1] <- initial.values[2]
b2[1] <- initial.values[3]
vari[1] <- 1
N <- length(Y)
## In every iteration, the values of the parameters of the conditional
## posteriors should be calculated as functions of the prior parameters,
## the data and the current values of the other model parameters
for(i in 2:iterations) {
## First, we should update the parameters of the intercept (mean and standard deviation) with the following formula:
## 10000 because informative prior?
if(method[1] == 0) {
mean.0.post <- (sum(Y - b1[i - 1]*X1 - b2[i - 1]*X2, na.rm = TRUE)/vari[i - 1] + (mu0 / sigma0)) / ((N / vari[i - 1]) + (mu0 / sigma0))
sd.0.post <- 1 / ((N / vari[i - 1]) + (mu0 / sigma0))
## Then through rnorm we randomly sample from a normal distribution the next value for b0 with the
## above calculated mean and standard deviation
b0[i] <- rnorm(n = 1, mean = mean.0.post, sd = sd.0.post)
} else {
## intercept through MH
}
## Now, we can update the mean and standard deviation of the regression coefficient for the first variable
## using the updated value of the intercept
if(method[2] == 0) {
mean.1.post <- (sum(X1*(Y - b0[i] - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1] + (mu1 / sigma1)) / sum(X1^2, na.rm = TRUE)/vari[i - 1] + (mu1 / sigma1)
sd.1.post <- 1/((sum(X1^2, na.rm = TRUE) / vari[i - 1]) + (mu1 / sigma1))
## Then again we randomly sample a new b1
b1[i] <- rnorm(n = 1, mean = mean.1.post, sd = sd.1.post)
} else {
## b1 through MH
}
## Afterwards, we update the mean and standard deviation of the regression coefficient for the second variable
## using the updated value of both the intercept and b1
if(method[3] == 0) {
mean.2.post <- (sum(X2*(Y - b0[i] - b1[i]*X1), na.rm = TRUE)/vari[i - 1] + (mu2 / sigma2)) / sum(X2^2, na.rm = TRUE)/vari[i - 1] + (mu2 / sigma2)
sd.2.post <- 1/((sum(X2^2, na.rm = TRUE) / vari[i - 1]) + (mu2 / sigma2))
## Then, we randomly sample a new b2
b2[i] <- rnorm(n = 1, mean = mean.2.post, sd = sd.2.post)
} else {
## b2 through MH
}
## Finally, we update the parameters for the distribution of our standard deviation using all above updated values
a.post <- N / 2 + a.prior
b.post <- sum((Y - (b0[i] + b1[i]*X1 + b2[i]*X2))^2, na.rm = TRUE) / 2 + b.prior
## And randomly sample a new variance again for
vari[i] <- 1/rgamma(n = 1, shape = a.post, rate = b.post)
}
b0 <- b0[-c(1:burn.in)]
b1 <- b1[-c(1:burn.in)]
b2 <- b2[-c(1:burn.in)]
vari <- vari[-c(1:burn.in)]
## Now, we also want the posterior distributions (histograms) of all parameters
par(mfrow = c(2,2))
hist(b0)
abline(v = mean(b0), col = "blue")
hist(b1)
abline(v = mean(b1), col = "blue")
hist(b2)
abline(v = mean(b2), col = "blue")
hist(vari)
data_frame <- as.data.frame(cbind(b0, b1, b2, vari, iter = 1:length(b0)))
traceplot1 <- ggplot(data_frame, aes(x=iter, y=b0)) +
geom_line() +
labs(title="b0 trace")
traceplot1
traceplot2 <- ggplot(data_frame, aes(x=iter, y=b1)) +
geom_line() +
labs(title="b1 trace")
traceplot2
traceplot3 <- ggplot(data_frame, aes(x=iter, y=b2)) +
geom_line() +
labs(title="b2 trace")
traceplot3
traceplot4 <- ggplot(data_frame, aes(x=iter, y=vari)) +
geom_line() +
labs(title="var trace")
traceplot4
list_of_output <- list(data_frame, traceplot1, traceplot2, traceplot3, traceplot4)
return(list_of_output)
}
GibbsSampler(data$attitude, data$extraversion, data$agreeableness, burn.in = 1000, iterations = 10000)
## a burn.in period (by default 1000 iterations)
## a total amount of iterations (by default 10000)
## the initial values for every regression coefficient
## a method function that specifies per parameter whether they would
## like to have it sampled through a Gibbs Sampler or Metropolis Hastings algorithm
## by default, it assumes a Gibbs sampler for every regression coefficient,
## and if you want to redefine it to a MH, you should change the 0 to a 1
## for the regression coefficient you want to change
## Examples:
## method = c(0, 1, 0) will make it so that b1 is done through MH
GibbsSampler <- function(Y, X1, X2, burn.in = 1000, iterations = 10000, initial.values = c(1, 1, 1), method = c(0, 0, 0)) {
## First, the function should require ggplot2 as it is used to create traceplots later
library(ggplot2)
## First create space
b0 <- rep(0, iterations)
b1 <- rep(0, iterations)
b2 <- rep(0, iterations)
vari <- rep(0, iterations)
## Then assume the specified initial values
b0[1] <- initial.values[1]
b1[1] <- initial.values[2]
b2[1] <- initial.values[3]
vari[1] <- 1
N <- length(Y)
## In every iteration, the values of the parameters of the conditional
## posteriors should be calculated as functions of the prior parameters,
## the data and the current values of the other model parameters
for(i in 2:iterations) {
## First, we should update the parameters of the intercept (mean and standard deviation) with the following formula:
## 10000 because informative prior?
if(method[1] == 0) {
mean.0.post <- (sum(Y - b1[i - 1]*X1 - b2[i - 1]*X2, na.rm = TRUE)/vari[i - 1] + (mu0 / sigma0)) / ((N / vari[i - 1]) + (mu0 / sigma0))
sd.0.post <- 1 / ((N / vari[i - 1]) + (mu0 / sigma0))
## Then through rnorm we randomly sample from a normal distribution the next value for b0 with the
## above calculated mean and standard deviation
b0[i] <- rnorm(n = 1, mean = mean.0.post, sd = sd.0.post)
} else {
## intercept through MH
}
## Now, we can update the mean and standard deviation of the regression coefficient for the first variable
## using the updated value of the intercept
if(method[2] == 0) {
mean.1.post <- (sum(X1*(Y - b0[i] - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1] + (mu1 / sigma1)) / sum(X1^2, na.rm = TRUE)/vari[i - 1] + (mu1 / sigma1)
sd.1.post <- 1/((sum(X1^2, na.rm = TRUE) / vari[i - 1]) + (mu1 / sigma1))
## Then again we randomly sample a new b1
b1[i] <- rnorm(n = 1, mean = mean.1.post, sd = sd.1.post)
} else {
## b1 through MH
}
## Afterwards, we update the mean and standard deviation of the regression coefficient for the second variable
## using the updated value of both the intercept and b1
if(method[3] == 0) {
mean.2.post <- (sum(X2*(Y - b0[i] - b1[i]*X1), na.rm = TRUE)/vari[i - 1] + (mu2 / sigma2)) / sum(X2^2, na.rm = TRUE)/vari[i - 1] + (mu2 / sigma2)
sd.2.post <- 1/((sum(X2^2, na.rm = TRUE) / vari[i - 1]) + (mu2 / sigma2))
## Then, we randomly sample a new b2
b2[i] <- rnorm(n = 1, mean = mean.2.post, sd = sd.2.post)
} else {
## b2 through MH
}
## Finally, we update the parameters for the distribution of our standard deviation using all above updated values
a.post <- N / 2 + a.prior
b.post <- sum((Y - (b0[i] + b1[i]*X1 + b2[i]*X2))^2, na.rm = TRUE) / 2 + b.prior
## And randomly sample a new variance again for
vari[i] <- 1/rgamma(n = 1, shape = a.post, rate = b.post)
}
b0 <- b0[-c(1:burn.in)]
b1 <- b1[-c(1:burn.in)]
b2 <- b2[-c(1:burn.in)]
vari <- vari[-c(1:burn.in)]
## Now, we also want the posterior distributions (histograms) of all parameters
par(mfrow = c(2,2))
hist(b0)
abline(v = mean(b0), col = "blue")
hist(b1)
abline(v = mean(b1), col = "blue")
hist(b2)
abline(v = mean(b2), col = "blue")
hist(vari)
data_frame <- as.data.frame(cbind(b0, b1, b2, vari, iter = 1:length(b0)))
traceplot1 <- ggplot(data_frame, aes(x=iter, y=b0)) +
geom_line() +
labs(title="b0 trace")
traceplot1
traceplot2 <- ggplot(data_frame, aes(x=iter, y=b1)) +
geom_line() +
labs(title="b1 trace")
traceplot2
traceplot3 <- ggplot(data_frame, aes(x=iter, y=b2)) +
geom_line() +
labs(title="b2 trace")
traceplot3
traceplot4 <- ggplot(data_frame, aes(x=iter, y=vari)) +
geom_line() +
labs(title="var trace")
traceplot4
list_of_output <- list(data_frame, traceplot1, traceplot2, traceplot3, traceplot4)
return(list_of_output)
}
GibbsSampler(data$attitude, data$extraversion, data$agreeableness, burn.in = 1000, iterations = 10000)
## a burn.in period (by default 1000 iterations)
## a total amount of iterations (by default 10000)
## the initial values for every regression coefficient
## a method function that specifies per parameter whether they would
## like to have it sampled through a Gibbs Sampler or Metropolis Hastings algorithm
## by default, it assumes a Gibbs sampler for every regression coefficient,
## and if you want to redefine it to a MH, you should change the 0 to a 1
## for the regression coefficient you want to change
## Examples:
## method = c(0, 1, 0) will make it so that b1 is done through MH
GibbsSampler <- function(Y, X1, X2, burn.in = 1000, iterations = 10000, initial.values = c(1, 1, 1), method = c(0, 0, 0)) {
## First, the function should require ggplot2 as it is used to create traceplots later
library(ggplot2)
## First create space
b0 <- rep(0, iterations)
b1 <- rep(0, iterations)
b2 <- rep(0, iterations)
vari <- rep(0, iterations)
## Then assume the specified initial values
b0[1] <- initial.values[1]
b1[1] <- initial.values[2]
b2[1] <- initial.values[3]
vari[1] <- 1
N <- length(Y)
## In every iteration, the values of the parameters of the conditional
## posteriors should be calculated as functions of the prior parameters,
## the data and the current values of the other model parameters
for(i in 2:iterations) {
## First, we should update the parameters of the intercept (mean and standard deviation) with the following formula:
## 10000 because informative prior?
if(method[1] == 0) {
mean.0.post <- (sum(Y - b1[i - 1]*X1 - b2[i - 1]*X2, na.rm = TRUE)/vari[i - 1] + (mu0 / sigma0)) / ((N / vari[i - 1]) + (mu0 / sigma0))
sd.0.post <- 1 / ((N / vari[i - 1]) + (mu0 / sigma0))
## Then through rnorm we randomly sample from a normal distribution the next value for b0 with the
## above calculated mean and standard deviation
b0[i] <- rnorm(n = 1, mean = mean.0.post, sd = sd.0.post)
} else {
## intercept through MH
}
## Now, we can update the mean and standard deviation of the regression coefficient for the first variable
## using the updated value of the intercept
if(method[2] == 0) {
mean.1.post <- (sum(X1*(Y - b0[i] - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1] + (mu1 / sigma1)) / sum(X1^2, na.rm = TRUE)/vari[i - 1] + (mu1 / sigma1)
sd.1.post <- 1/((sum(X1^2, na.rm = TRUE) / vari[i - 1]) + (mu1 / sigma1))
## Then again we randomly sample a new b1
b1[i] <- rnorm(n = 1, mean = mean.1.post, sd = sd.1.post)
} else {
## b1 through MH
}
## Afterwards, we update the mean and standard deviation of the regression coefficient for the second variable
## using the updated value of both the intercept and b1
if(method[3] == 0) {
mean.2.post <- (sum(X2*(Y - b0[i] - b1[i]*X1), na.rm = TRUE)/vari[i - 1] + (mu2 / sigma2)) / sum(X2^2, na.rm = TRUE)/vari[i - 1] + (mu2 / sigma2)
sd.2.post <- 1/((sum(X2^2, na.rm = TRUE) / vari[i - 1]) + (mu2 / sigma2))
## Then, we randomly sample a new b2
b2[i] <- rnorm(n = 1, mean = mean.2.post, sd = sd.2.post)
} else {
## b2 through MH
}
## Finally, we update the parameters for the distribution of our standard deviation using all above updated values
a.post <- N / 2 + a.prior
b.post <- sum((Y - (b0[i] + b1[i]*X1 + b2[i]*X2))^2, na.rm = TRUE) / 2 + b.prior
## And randomly sample a new variance again for
vari[i] <- 1/rgamma(n = 1, shape = a.post, rate = b.post)
}
b0 <- b0[-c(1:burn.in)]
b1 <- b1[-c(1:burn.in)]
b2 <- b2[-c(1:burn.in)]
vari <- vari[-c(1:burn.in)]
## Now, we also want the posterior distributions (histograms) of all parameters
par(mfrow = c(2,2))
hist(b0, breaks = 50)
abline(v = mean(b0), col = "blue")
hist(b1)
abline(v = mean(b1), col = "blue")
hist(b2)
abline(v = mean(b2), col = "blue")
hist(vari)
data_frame <- as.data.frame(cbind(b0, b1, b2, vari, iter = 1:length(b0)))
traceplot1 <- ggplot(data_frame, aes(x=iter, y=b0)) +
geom_line() +
labs(title="b0 trace")
traceplot1
traceplot2 <- ggplot(data_frame, aes(x=iter, y=b1)) +
geom_line() +
labs(title="b1 trace")
traceplot2
traceplot3 <- ggplot(data_frame, aes(x=iter, y=b2)) +
geom_line() +
labs(title="b2 trace")
traceplot3
traceplot4 <- ggplot(data_frame, aes(x=iter, y=vari)) +
geom_line() +
labs(title="var trace")
traceplot4
list_of_output <- list(data_frame, traceplot1, traceplot2, traceplot3, traceplot4)
return(list_of_output)
}
GibbsSampler(data$attitude, data$extraversion, data$agreeableness, burn.in = 1000, iterations = 10000)
## a burn.in period (by default 1000 iterations)
## a total amount of iterations (by default 10000)
## the initial values for every regression coefficient
## a method function that specifies per parameter whether they would
## like to have it sampled through a Gibbs Sampler or Metropolis Hastings algorithm
## by default, it assumes a Gibbs sampler for every regression coefficient,
## and if you want to redefine it to a MH, you should change the 0 to a 1
## for the regression coefficient you want to change
## Examples:
## method = c(0, 1, 0) will make it so that b1 is done through MH
GibbsSampler <- function(Y, X1, X2, burn.in = 1000, iterations = 10000, initial.values = c(1, 1, 1), method = c(0, 0, 0)) {
## First, the function should require ggplot2 as it is used to create traceplots later
library(ggplot2)
## First create space
b0 <- rep(0, iterations)
b1 <- rep(0, iterations)
b2 <- rep(0, iterations)
vari <- rep(0, iterations)
## Then assume the specified initial values
b0[1] <- initial.values[1]
b1[1] <- initial.values[2]
b2[1] <- initial.values[3]
vari[1] <- 1
N <- length(Y)
## In every iteration, the values of the parameters of the conditional
## posteriors should be calculated as functions of the prior parameters,
## the data and the current values of the other model parameters
for(i in 2:iterations) {
## First, we should update the parameters of the intercept (mean and standard deviation) with the following formula:
## 10000 because informative prior?
if(method[1] == 0) {
mean.0.post <- (sum(Y - b1[i - 1]*X1 - b2[i - 1]*X2, na.rm = TRUE)/vari[i - 1] + (mu0 / sigma0)) / ((N / vari[i - 1]) + (mu0 / sigma0))
sd.0.post <- 1 / ((N / vari[i - 1]) + (mu0 / sigma0))
## Then through rnorm we randomly sample from a normal distribution the next value for b0 with the
## above calculated mean and standard deviation
b0[i] <- rnorm(n = 1, mean = mean.0.post, sd = sd.0.post)
} else {
## intercept through MH
}
## Now, we can update the mean and standard deviation of the regression coefficient for the first variable
## using the updated value of the intercept
if(method[2] == 0) {
mean.1.post <- (sum(X1*(Y - b0[i] - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1] + (mu1 / sigma1)) / sum(X1^2, na.rm = TRUE)/vari[i - 1] + (mu1 / sigma1)
sd.1.post <- 1/((sum(X1^2, na.rm = TRUE) / vari[i - 1]) + (mu1 / sigma1))
## Then again we randomly sample a new b1
b1[i] <- rnorm(n = 1, mean = mean.1.post, sd = sd.1.post)
} else {
## b1 through MH
}
## Afterwards, we update the mean and standard deviation of the regression coefficient for the second variable
## using the updated value of both the intercept and b1
if(method[3] == 0) {
mean.2.post <- (sum(X2*(Y - b0[i] - b1[i]*X1), na.rm = TRUE)/vari[i - 1] + (mu2 / sigma2)) / sum(X2^2, na.rm = TRUE)/vari[i - 1] + (mu2 / sigma2)
sd.2.post <- 1/((sum(X2^2, na.rm = TRUE) / vari[i - 1]) + (mu2 / sigma2))
## Then, we randomly sample a new b2
b2[i] <- rnorm(n = 1, mean = mean.2.post, sd = sd.2.post)
} else {
## b2 through MH
}
## Finally, we update the parameters for the distribution of our standard deviation using all above updated values
a.post <- N / 2 + a.prior
b.post <- sum((Y - (b0[i] + b1[i]*X1 + b2[i]*X2))^2, na.rm = TRUE) / 2 + b.prior
## And randomly sample a new variance again for
vari[i] <- 1/rgamma(n = 1, shape = a.post, rate = b.post)
}
b0 <- b0[-c(1:burn.in)]
b1 <- b1[-c(1:burn.in)]
b2 <- b2[-c(1:burn.in)]
vari <- vari[-c(1:burn.in)]
## Now, we also want the posterior distributions (histograms) of all parameters
par(mfrow = c(2,2))
hist(b0, breaks = 50)
abline(v = mean(b0), col = "blue")
hist(b1, breaks = 50)
abline(v = mean(b1), col = "blue")
hist(b2, breaks = 50)
abline(v = mean(b2), col = "blue")
hist(vari, breaks = 50)
data_frame <- as.data.frame(cbind(b0, b1, b2, vari, iter = 1:length(b0)))
traceplot1 <- ggplot(data_frame, aes(x=iter, y=b0)) +
geom_line() +
labs(title="b0 trace")
traceplot1
traceplot2 <- ggplot(data_frame, aes(x=iter, y=b1)) +
geom_line() +
labs(title="b1 trace")
traceplot2
traceplot3 <- ggplot(data_frame, aes(x=iter, y=b2)) +
geom_line() +
labs(title="b2 trace")
traceplot3
traceplot4 <- ggplot(data_frame, aes(x=iter, y=vari)) +
geom_line() +
labs(title="var trace")
traceplot4
list_of_output <- list(data_frame, traceplot1, traceplot2, traceplot3, traceplot4)
return(list_of_output)
}
GibbsSampler(data$attitude, data$extraversion, data$agreeableness, burn.in = 1000, iterations = 10000)
rm(list=ls)
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
source("Exercise 3 - Data.txt")
library(rjags)
model.def <- jags.model(file = "Exercise 3 - Model_template.txt", data = dat,
n.chains = 2,
inits = list(theta.PC = .45, theta.PE = .55))
update(model.def, n.iter = 2500)
parameters <- c('theta.PE', 'theta.PC', 'RR')
set.seed(3)
results <- coda.samples(model = model.def, variable.names = parameters, n.iter = 12500)
summary(results)
## Assessing convergence
plot(results)
## Autocorrelation plots
autocorr.plot(results)
## Gelman-Rubin diagnostic plot
gelman.plot(results)
## Calculating the above defined test statistic
# test statistic for the PE condition
proportion.half1.PE <- sum(dat$LD.PE[1:70])/70
proportion.half2.PE <- sum(dat$LD.PE[71:141])/71
diff.PE <- proportion.half1.PE - proportion.half2.PE
# test statistic for the PC condition
proportion.half1.PC <- sum(dat$LD.PC[1:71])/71
proportion.half2.PC <- sum(dat$LD.PC[72:143])/72
diff.PC <- proportion.half1.PC - proportion.half2.PC
# Extract the parameter estimates
theta.PE.chain1 <- results[[1]][,"theta.PE"]
theta.PE.chain2 <- results[[2]][,"theta.PE"]
theta.PC.chain1 <- results[[1]][,"theta.PC"]
theta.PC.chain2 <- results[[2]][,"theta.PC"]
library(LaplacesDemon)
# Storage room (each row is a replicated dataset) :
replicated.PE.chain1 <- array(data = NA, dim = c(length(theta.PE.chain1), dat$n.PE))
replicated.PE.chain2 <- array(data = NA, dim = c(length(theta.PE.chain2), dat$n.PE))
replicated.PC.chain1 <- array(data = NA, dim = c(length(theta.PC.chain1), dat$n.PC))
replicated.PC.chain2 <- array(data = NA, dim = c(length(theta.PC.chain2), dat$n.PC))
# Sample replicated datasets
# For each parameter estimate...
for(t in 1:length(theta.PE.chain1)) {
# ... sample a replicated dataset by sampling n times from the Bernoulli distribution
# with as probability of success on each trial the parameter estimate
replicated.PE.chain1[t,] <- rbern(n = dat$n.PE, prob = theta.PE.chain1[t])
replicated.PE.chain2[t,] <- rbern(n = dat$n.PE, prob = theta.PE.chain2[t])
replicated.PC.chain1[t,] <- rbern(n = dat$n.PC, prob = theta.PC.chain1[t])
replicated.PC.chain2[t,] <- rbern(n = dat$n.PC, prob = theta.PC.chain2[t])
}
test.statistics.PE <- rep(NA, nrow(replicated.PE.chain1) + nrow(replicated.PE.chain2))
test.statistics.PC <- rep(NA, nrow(replicated.PC.chain1) + nrow(replicated.PC.chain2))
## First, we calculate the statistics for the PE treatment
for(i in 1:nrow(replicated.PE.chain1)){
half1 <- sum(replicated.PE.chain1[i, 1:(ncol(replicated.PE.chain1)/2)])/(ncol(replicated.PE.chain1)/2)
half2 <- sum(replicated.PE.chain1[i, (ncol(replicated.PE.chain1)/2 + 1):ncol(replicated.PE.chain1), i])/(ncol(replicated.PE.chain1)/2 + 1)
test.statistics.PE[i] <- half1 - half2
}
test.statistics.PE <- rep(NA, nrow(replicated.PE.chain1) + nrow(replicated.PE.chain2))
test.statistics.PC <- rep(NA, nrow(replicated.PC.chain1) + nrow(replicated.PC.chain2))
## First, we calculate the statistics for the PE treatment
for(i in 1:nrow(replicated.PE.chain1)){
half1 <- sum(replicated.PE.chain1[i, 1:(ncol(replicated.PE.chain1)/2)])/(ncol(replicated.PE.chain1)/2)
half2 <- sum(replicated.PE.chain1[i, (ncol(replicated.PE.chain1)/2 + 1):ncol(replicated.PE.chain1)])/(ncol(replicated.PE.chain1)/2 + 1)
test.statistics.PE[i] <- half1 - half2
}
for(i in 1:nrow(replicated.PE.chain2)) {
half1 <- sum(replicated.PE.chain2[i, 1:(ncol(replicated.PE.chain2)/2)])/(ncol(replicated.PE.chain2)/2)
half2 <- sum(replicated.PE.chain2[i, (ncol(replicated.PE.chain2)/2 + 1):ncol(replicated.PE.chain2)])/(ncol(replicated.PE.chain2)/2 + 1)
test.statistics.PE[i + nrow(replicated.PE.chain1)] <- half1 - half2
}
## Then the statistic for the PC treatment
for(i in 1:nrow(replicated.PC.chain1)){
half1 <- sum(replicated.PC.chain1[i, 1:(ncol(replicated.PC.chain1)/2)])/(ncol(replicated.PC.chain1)/2)
half2 <- sum(replicated.PC.chain1[i, (ncol(replicated.PC.chain1)/2 + 1):ncol(replicated.PC.chain1)])/(ncol(replicated.PC.chain1)/2 + 1)
test.statistics.PC[i] <- half1 - half2
}
for(i in 1:nrow(replicated.PC.chain2)) {
half1 <- sum(replicated.PC.chain2[i, 1:(ncol(replicated.PC.chain2)/2)])/(ncol(replicated.PC.chain2)/2)
half2 <- sum(replicated.PC.chain2[i, (ncol(replicated.PC.chain2)/2 + 1):ncol(replicated.PC.chain2)])/(ncol(replicated.PC.chain2)/2 + 1)
test.statistics.PC[i + nrow(replicated.PC.chain1)] <- half1 - half2
}
test.statistics.PE
test.statistics.PC
library(tidyverse)
## For PC group:
ifelse(abs(test.statistics.PC) > abs(diff.PC), 1, 0) %>% mean(.)
## For PE group:
ifelse(abs(test.statistics.PE) > abs(diff.PE), 1, 0) %>% mean(.)
ifelse(abs(test.statistics.PC) > abs(diff.PC), 1, 0) %>% mean(.)
## For PE group:
ifelse(abs(test.statistics.PE) > abs(diff.PE), 1, 0) %>% mean(.)
library(tidyverse)
## For PC group:
ifelse(abs(test.statistics.PC) > abs(diff.PC), 1, 0) %>% mean(.)
## For PE group:
ifelse(abs(test.statistics.PE) > abs(diff.PE), 1, 0) %>% mean(.)
?pnorm
pnorm(1)
pnorm(1, 20)
pnorm(9)
pnorm(-1)
pnorm(0)
