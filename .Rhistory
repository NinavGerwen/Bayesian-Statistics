} else {
## If through MH:
## First, we gain a proposal value
proposal_value <- 11.73 * rt(1, df = 1)
## Calculate acceptance ratio
accept_ratio <- (dnorm(proposal_value, mean = 11.73, sd = 1.46)/dnorm(b1[i - 1], mean = 11.73, sd = 1.46)) *
(dt(b1[i - 1], df = 1)/dt(proposal_value, df = 1))
## Then see whether we accept the proposed value
if(runif(1,0,1) < accept_ratio) {
b1[i] <- proposal_value } else { b1[i] <- b1[i - 1] }
}
## Then, every loop, we update the regression coefficient of the second variable
## But first check whether Gibbs oR MH:
if(method[3] == "0") {
## If through Gibbs:
## We update the mean and standard deviation of the regression coefficient for the second variable
## using the updated value of both the intercept and b1
mean.2.post <- (sum(X2*(Y - b0[i] - b1[i]*X1), na.rm = TRUE)/vari[i - 1] +
(mu2 / sigma2)) / ((sum(X2^2, na.rm = TRUE)/vari[i - 1]) + (1 / sigma2))
sd.2.post <- 1/((sum(X2^2, na.rm = TRUE) / vari[i - 1]) + (1 / sigma2))
## Then, we randomly sample a new b2
b2[i] <- rnorm(n = 1, mean = mean.2.post, sd = sqrt(sd.2.post))
} else {
## If through MH:
## First, we gain a proposal value
proposal_value <- 0.08 * rt(1, df = 1)
## Calculate acceptance ratio
accept_ratio <- (dnorm(proposal_value, mean = 0.08, sd = 0.06)/dnorm(b2[i - 1], mean = 0.08, sd = 0.06)) *
(dt(b2[i - 1], df = 1)/dt(proposal_value, df = 1))
## Then see whether we accept the proposed value
if(runif(1,0,1) < accept_ratio) {
b2[i] <- proposal_value } else { b2[i] <- b2[i - 1] }
}
## Finally, we update the parameters for the distribution of our variance using all above updated values
a.post <- (N / 2) + a.prior
b.post <- (sum((Y - (b0[i] + b1[i]*X1 + b2[i]*X2))^2, na.rm = TRUE) / 2) + b.prior
## And randomly sample a new variance again through rgamma
vari[i] <- 1/rgamma(n = 1, shape = a.post, rate = b.post)
## We use the inverse of the randomly sample because we want the value of the variance, and not the precision.
}
## Then, we remove the values of the burn-in
b0 <- b0[-c(1:burn.in)]
b1 <- b1[-c(1:burn.in)]
b2 <- b2[-c(1:burn.in)]
vari <- vari[-c(1:burn.in)]
## We also want the posterior distributions (histograms) of all parameters
par(mfrow = c(2,2))
hist(b0, breaks = 50)
abline(v = mean(b0), col = "blue")
hist(b1, breaks = 50)
abline(v = mean(b1), col = "blue")
hist(b2, breaks = 50)
abline(v = mean(b2), col = "blue")
hist(vari, breaks = 50)
## We want a dataframe consisting of all the sampled parameter values
data_frame <- as.data.frame(cbind(b0, b1, b2, vari, iter = 1:length(b0)))
## And we want traceplots for all the parameters to assess convergence
traceplot1 <- ggplot(data_frame, aes(x=iter, y=b0)) +
geom_line() +
labs(title="b0 trace")
traceplot1
traceplot2 <- ggplot(data_frame, aes(x=iter, y=b1)) +
geom_line() +
labs(title="b1 trace")
traceplot2
traceplot3 <- ggplot(data_frame, aes(x=iter, y=b2)) +
geom_line() +
labs(title="b2 trace")
traceplot3
traceplot4 <- ggplot(data_frame, aes(x=iter, y=vari)) +
geom_line() +
labs(title="var trace")
traceplot4
## We create a list of all the output we want
list_of_output <- list(data_frame, traceplot1, traceplot2, traceplot3, traceplot4)
## And finally, we ask the function to return this list of output
return(list_of_output)
}
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("0", "0", "1"))
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("0", "1", "0"))
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("1", "0", "0"))
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("0", "1", "0"))
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("1", "0", "0"))
## First, all priors have to be set
a.prior <- 0.001
b.prior <- 0.001
mu0 <- 1
sigma0 <- 10000
mu1 <- 1
sigma1 <- 10000
mu2 <- 1
sigma2 <- 10000
## For MH: we need a proposal and posterior distribution, however, since
## it has to be a linear regression, this will always be a normal distribution
## so we can use dnorm() for this, but the proposal distribution should be specified!
## For the input of the Gibbs Sampler, one should provide:
## a vector of continuous variables that serves as your dependent variable
## two vectors of continuous variables that serve as your independent variables
## a burn.in period (by default 1000 iterations)
## a total amount of iterations (by default 10000)
## the initial values for every regression coefficient
## a method function that specifies per parameter whether they would
## like to have it sampled through a Gibbs Sampler or Metropolis Hastings algorithm
## by default, it assumes a Gibbs sampler for every regression coefficient,
## and if you want to redefine it to a MH, you should change the 0 to a 1
## for the regression coefficient you want to change
## Examples:
## method = c(0, 1, 0) will make it so that b1 is done through MH
GibbsSampler <- function(Y, X1, X2, burn.in = 1000, iterations = 10000,
initial.values = c(1, 1, 1), method = c("0", "0", "0")) {
## First, the function should require ggplot2 as it is used to create traceplots later
library(ggplot2)
## First create space
b0 <- rep(NA, iterations)
b1 <- rep(NA, iterations)
b2 <- rep(NA, iterations)
vari <- rep(NA, iterations)
## Then assume the specified initial values
b0[1] <- initial.values[1]
b1[1] <- initial.values[2]
b2[1] <- initial.values[3]
vari[1] <- 1
N <- length(Y)
## Now, we will start the sampling, it should start at the second element
## because the first element will be the initial values
for(i in 2:iterations) {
## In every loop, We first update the intercept:
## Check whether the intercept should be sampled through Gibbs or MH
if(method[1] == "0") {
## If through Gibbs Sampler:
## First, we update the parameters of the intercept (mean and standard deviation) with the following formula:
mean.0.post <- (sum(Y - (b1[i - 1]*X1 - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1] +
(mu0 / sigma0)) / ((N / vari[i - 1]) + (1 / sigma0))
sd.0.post <- 1 / ((N / vari[i - 1]) + (1 / sigma0))
## Then through rnorm we randomly sample from a normal distribution the next value for b0 with the
## above calculated mean and standard deviation
b0[i] <- rnorm(n = 1, mean = mean.0.post, sd = sqrt(sd.0.post))
} else {
## If through MH:
## First, we gain a proposal value
proposal_value <- 115.45 * rt(1, df = 1)
## Calculate acceptance ratio
accept_ratio <- (dnorm(proposal_value, mean = 115.45, sd = 19.96)/dnorm(b0[i - 1], mean = 115.45, sd = 19.96)) *
(dt(b0[i - 1], df = 1)/dt(proposal_value, df = 1))
## Then see whether we accept the proposed value
if(runif(1,0,1) < accept_ratio) {
b0[i] <- proposal_value } else { b0[i] <- b0[i - 1] }
}
## Then, every loop, we will update the coefficient of the first independent variable
## Again, check whether Gibbs or MH:
if(method[2] == "0") {
## If through Gibbs:
## We can update the mean and standard deviation of the regression coefficient for the first variable
## using the updated value of the intercept
mean.1.post <- ((sum(X1*(Y - b0[i] - b2[i - 1]*X2), na.rm = TRUE)/vari[i - 1]) +
(mu1 / sigma1)) / ((sum((X1^2), na.rm = TRUE)/vari[i - 1]) + (1 / sigma1))
sd.1.post <- 1/((sum((X1^2), na.rm = TRUE) / vari[i - 1]) + (1 / sigma1))
## Then again we randomly sample a new b1 through rnorm
b1[i] <- rnorm(n = 1, mean = mean.1.post, sd = sqrt(sd.1.post))
} else {
## If through MH:
## First, we gain a proposal value
proposal_value <- 11.73 * rt(1, df = 1)
## Calculate acceptance ratio
accept_ratio <- (dnorm(proposal_value, mean = 11.73, sd = 1.46)/dnorm(b1[i - 1], mean = 11.73, sd = 1.46)) *
(dt(b1[i - 1], df = 1)/dt(proposal_value, df = 1))
## Then see whether we accept the proposed value
if(runif(1,0,1) < accept_ratio) {
b1[i] <- proposal_value } else { b1[i] <- b1[i - 1] }
}
## Then, every loop, we update the regression coefficient of the second variable
## But first check whether Gibbs oR MH:
if(method[3] == "0") {
## If through Gibbs:
## We update the mean and standard deviation of the regression coefficient for the second variable
## using the updated value of both the intercept and b1
mean.2.post <- (sum(X2*(Y - b0[i] - b1[i]*X1), na.rm = TRUE)/vari[i - 1] +
(mu2 / sigma2)) / ((sum(X2^2, na.rm = TRUE)/vari[i - 1]) + (1 / sigma2))
sd.2.post <- 1/((sum(X2^2, na.rm = TRUE) / vari[i - 1]) + (1 / sigma2))
## Then, we randomly sample a new b2
b2[i] <- rnorm(n = 1, mean = mean.2.post, sd = sqrt(sd.2.post))
} else {
## If through MH:
## First, we gain a proposal value
proposal_value <- 0.08 * rt(1, df = 1)
## Calculate acceptance ratio
accept_ratio <- (dnorm(proposal_value, mean = 0.08, sd = 0.06)/dnorm(b2[i - 1], mean = 0.08, sd = 0.06)) *
(dt(b2[i - 1], df = 1)/dt(proposal_value, df = 1))
## Then see whether we accept the proposed value
if(runif(1,0,1) < accept_ratio) {
b2[i] <- proposal_value } else { b2[i] <- b2[i - 1] }
}
## Finally, we update the parameters for the distribution of our variance using all above updated values
a.post <- (N / 2) + a.prior
b.post <- (sum((Y - (b0[i] + b1[i]*X1 + b2[i]*X2))^2, na.rm = TRUE) / 2) + b.prior
## And randomly sample a new variance again through rgamma
vari[i] <- 1/rgamma(n = 1, shape = a.post, rate = b.post)
## We use the inverse of the randomly sample because we want the value of the variance, and not the precision.
}
## Then, we remove the values of the burn-in
b0 <- b0[-c(1:burn.in)]
b1 <- b1[-c(1:burn.in)]
b2 <- b2[-c(1:burn.in)]
vari <- vari[-c(1:burn.in)]
## We also want the posterior distributions (histograms) of all parameters
par(mfrow = c(2,2))
hist(b0, breaks = 50)
abline(v = mean(b0), col = "blue")
hist(b1, breaks = 50)
abline(v = mean(b1), col = "blue")
hist(b2, breaks = 50)
abline(v = mean(b2), col = "blue")
hist(vari, breaks = 50)
## We want a dataframe consisting of all the sampled parameter values
data_frame <- as.data.frame(cbind(b0, b1, b2, vari, iter = 1:length(b0)))
## And we want traceplots for all the parameters to assess convergence
traceplot1 <- ggplot(data_frame, aes(x=iter, y=b0)) +
geom_line() +
labs(title="b0 trace")
traceplot1
traceplot2 <- ggplot(data_frame, aes(x=iter, y=b1)) +
geom_line() +
labs(title="b1 trace")
traceplot2
traceplot3 <- ggplot(data_frame, aes(x=iter, y=b2)) +
geom_line() +
labs(title="b2 trace")
traceplot3
traceplot4 <- ggplot(data_frame, aes(x=iter, y=vari)) +
geom_line() +
labs(title="var trace")
traceplot4
## We create a list of all the output we want
list_of_output <- list(data_frame, traceplot1, traceplot2, traceplot3, traceplot4)
## And finally, we ask the function to return this list of output
return(list_of_output)
}
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("1", "0", "0"))
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("1", "0", "0"))
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("0", "0", "1"))
library(haven)
testdat <- read_sav(file = "Week6Data2.sav")
testdat$pass <- as.numeric(testdat$pass)
testdat$verbal <- testdat$verbal - mean(testdat$verbal)
set.seed(3)
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("0", "0", "1"))
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("1", "0", "0"))
set.seed(3)
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("1", "0", "0"))
set.seed(3)
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("0", "1", "0"))
set.seed(3)
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("0", "0", "1"))
set.seed(3)
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("1", "1", "1"))
View(df)
## Comparing results to JAGS results
library(haven)
testdat <- read_sav(file = "Week6Data2.sav")
testdat$pass <- as.numeric(testdat$pass)
testdat$verbal <- testdat$verbal - mean(testdat$verbal)
summary(lm(IQ ~ verbal + SES, data = testdat))
set.seed(3)
GibbsSampler(testdat$IQ, testdat$verbal, testdat$SES, burn.in = 5000, iterations = 25000, initial.values = c(1, 1, 1), method = c("0", "0", "0"))
list <- GibbsSampler(testdat$IQ, testdat$verbal, testdat$pass, burn.in = 2500, iterations = 12500, initial.values = c(1, 1, 1))
df <- as.data.frame(list[1])
mean(df$b0)
mean(df$b1)
mean(df$b2)
mean(testdat$IQ)
var(testdat$IQ)
?sample
sample(1)
sample(100)
sample(df$b0, 1)
set.seed(3)
sim.data <- matrix(data = NA, nrow = 400, ncol = 1000)
for(i in 1:1000) {
b0 <- sample(df$b0, 1)
b1 <- sample(df$b1, 1)
b2 <- sample(df$b2, 1)
s2 <- sample(df$vari, 1)
sim.data[, i] <- b0 + b1*testdat$verbal + b2*testdat$SES + rnorm(1, mean = 0, sd = s2)
}
View(sim.data)
set.seed(3)
sim.data <- matrix(data = NA, nrow = 400, ncol = 1000)
for(i in 1:1000) {
b0 <- sample(df$b0, 1)
b1 <- sample(df$b1, 1)
b2 <- sample(df$b2, 1)
s2 <- sample(df$vari, 1)
sim.data[, i] <- rnorm(1000, mean = b0 + b1*testdat$verbal + b2*testdat$SES, sd = s2)
}
set.seed(3)
sim.data <- matrix(data = NA, nrow = 400, ncol = 1000)
for(i in 1:1000) {
b0 <- sample(df$b0, 1)
b1 <- sample(df$b1, 1)
b2 <- sample(df$b2, 1)
s2 <- sample(df$vari, 1)
sim.data[, i] <- rnorm(400, mean = b0 + b1*testdat$verbal + b2*testdat$SES, sd = s2)
}
View(sim.data)
set.seed(3)
sim.data <- matrix(data = NA, nrow = 400, ncol = 1000)
b0_t <- rep(NA, 1000)
b1_t <- rep(NA, 1000)
b2_t <- rep(NA, 1000)
s2_t <- rep(NA, 1000)
for(i in 1:1000) {
b0_t[i] <- sample(df$b0, 1)
b1_t[i] <- sample(df$b1, 1)
b2_t[i] <- sample(df$b2, 1)
s2_t[i] <- sample(df$vari, 1)
sim.data[, i] <- rnorm(400, mean = b0[i] + b1[i]*testdat$verbal + b2[i]*testdat$SES, sd = s2[1])
}
## Now discrepancy measure
set.seed(3)
sim.data <- matrix(data = NA, nrow = 400, ncol = 1000)
b0_t <- rep(NA, 1000)
b1_t <- rep(NA, 1000)
b2_t <- rep(NA, 1000)
s2_t <- rep(NA, 1000)
for(i in 1:1000) {
b0_t[i] <- sample(df$b0, 1)
b1_t[i] <- sample(df$b1, 1)
b2_t[i] <- sample(df$b2, 1)
s2_t[i] <- sample(df$vari, 1)
sim.data[, i] <- rnorm(400, mean = b0_t[i] + b1_t[i]*testdat$verbal + b2_t[i]*testdat$SES, sd = s2_t[1])
}
## Now discrepancy measure
View(sim.data)
set.seed(3)
sim.data <- matrix(data = NA, nrow = 400, ncol = 1000)
b0_t <- rep(NA, 1000)
b1_t <- rep(NA, 1000)
b2_t <- rep(NA, 1000)
s2_t <- rep(NA, 1000)
for(i in 1:1000) {
b0_t[i] <- sample(df$b0, 1)
b1_t[i] <- sample(df$b1, 1)
b2_t[i] <- sample(df$b2, 1)
s2_t[i] <- sample(df$vari, 1)
sim.data[, i] <- rnorm(400, mean = b0_t[i] + b1_t[i]*testdat$verbal + b2_t[i]*testdat$SES, sd = sqrt(s2_t[1]))
}
## Now discrepancy measure
View(sim.data)
b0_t
b1_t
set.seed(3)
sim.data <- matrix(data = NA, nrow = 400, ncol = 1000)
b0_t <- rep(NA, 1000)
b1_t <- rep(NA, 1000)
b2_t <- rep(NA, 1000)
s2_t <- rep(NA, 1000)
for(i in 1:1000) {
b0_t[i] <- sample(df$b0, 1)
b1_t[i] <- sample(df$b1, 1)
b2_t[i] <- sample(df$b2, 1)
s2_t[i] <- sample(df$vari, 1)
sim.data[, i] <- rnorm(400, mean = b0_t[i] + b1_t[i]*testdat$verbal + b2_t[i]*testdat$SES, sd = sqrt(s2_t[1]))
}
## Now discrepancy measure
View(sim.data)
hist(sim.data[, 1])
hist(sim.data[, 99])
hist(sim.data[, 76])
hist(sim.data[, 600])
hist(sim.data[, 986])
s2_t
## Now discrepancy measure
residuals_sim <- matrix(data = NA, nrow = 400, ncol = 1000)
residuals_sim[, i] <- sim.data[, i] - b0_t[i] - b1_t[i]*testdat$verbal - b2_t[i]*testdat$SES
## Now discrepancy measure
residuals_sim <- matrix(data = NA, nrow = 400, ncol = 1000)
for(i in 1:1000) {
residuals_sim[, i] <- sim.data[, i] - b0_t[i] - b1_t[i]*testdat$verbal - b2_t[i]*testdat$SES
}
View(residuals_sim)
residuals_obs <- matrix(data = NA, nrow = 400, ncol = 1000)
for(i in 1:1000){
residuals_obs[, i] <- testdat$IQ - b0_t[i] - b1_t[i]*testdat$verbal - b2_t[i]*testdat$SES
}
View(residuals_obs)
sim_skewness <- rep(NA, 1000)
?median
?mode
sim_skewness <- rep(NA, 1000)
for(i in 1:1000) {
sim_skewness[i] <- (mean(residuals_obs[, i] - median(residuals_obs[, i])))^2
}
sim_skewness
obs_skewness <- rep(NA, 1000)
set.seed(3)
sim.data <- matrix(data = NA, nrow = 400, ncol = 1000)
b0_t <- rep(NA, 1000)
b1_t <- rep(NA, 1000)
b2_t <- rep(NA, 1000)
s2_t <- rep(NA, 1000)
for(i in 1:1000) {
b0_t[i] <- sample(df$b0, 1)
b1_t[i] <- sample(df$b1, 1)
b2_t[i] <- sample(df$b2, 1)
s2_t[i] <- sample(df$vari, 1)
sim.data[, i] <- rnorm(400, mean = b0_t[i] + b1_t[i]*testdat$verbal + b2_t[i]*testdat$SES, sd = sqrt(s2_t[1]))
}
## Now discrepancy measure
residuals_sim <- matrix(data = NA, nrow = 400, ncol = 1000)
for(i in 1:1000) {
residuals_sim[, i] <- sim.data[, i] - b0_t[i] - b1_t[i]*testdat$verbal - b2_t[i]*testdat$SES
}
residuals_obs <- matrix(data = NA, nrow = 400, ncol = 1000)
for(i in 1:1000){
residuals_obs[, i] <- testdat$IQ - b0_t[i] - b1_t[i]*testdat$verbal - b2_t[i]*testdat$SES
}
sim_skewness <- rep(NA, 1000)
for(i in 1:1000) {
sim_skewness[i] <- (mean(residuals_sim[, i] - median(residuals_sim[, i])))^2
}
obs_skewness <- rep(NA, 1000)
for(i in 1:1000) {
obs_skewness[i] <- (mean(residuals_obs[, i] - median(residuals_obs[, i])))^2
}
p_value <- sum(sim_skewness > obs_skewness)
p_value <- sum(sim_skewness > obs_skewness)/400
p_value
?rnorm
View(sim.data)
sim_skewness > obs_skewness
summary(lm(IQ ~ verbal + SES, data = testdat))
test <- lm(IQ ~ verbal + SES, data = testdat)
View(test)
vcov(test)
library(MASS)
?mvnorm
?mvrnorm
mean(test)
test$coefficients
mvrnorm(10000, mu = c(115.45920379, 11.73647120, 0.08399327), Sigma = vcov(test))
post <- mvrnorm(10000, mu = c(115.45920379, 11.73647120, 0.08399327), Sigma = vcov(test))
?dmvrnorm
View(post)
plot(post)
View(post)
View(post)
sum(post[,2] > 0)/10000
prior <- mvrnorm(10000, mu = c(115.45920379, 11.73647120, 0.08399327), Sigma = vcov(test)/5)
sum(prior[,2] > 0)
sum(prior[,2] > 0)/10000
plot(prior)
plot(post)
b <- 5/400
prior <- mvrnorm(10000, mu = c(115.45920379, 11.73647120, 0.08399327), Sigma = vcov(test)/b)
sum(prior[,2] > 0)/10000
1/0.8152
sum(post[,3] > 0)/10000
b <- 5/400
prior <- mvrnorm(10000, mu = c(115.45920379, 11.73647120, 0.08399327), Sigma = vcov(test)/b)
sum(prior[,3] > 0)/10000
0.9255 / 0.5624
View(post)
b <- 2/400
prior <- mvrnorm(10000, mu = c(115.45920379, 11.73647120, 0.08399327), Sigma = vcov(test)/b)
sum(prior[,3] > 0)/10000
sum(post[,3] > 0)/10000
b <- 2/400
sum(prior[,3] > 0)/10000
set.seed(3)
post <- mvrnorm(10000, mu = c(115.45920379, 11.73647120, 0.08399327), Sigma = vcov(test))
sum(post[,3] > 0)/10000
b <- 5/400
prior <- mvrnorm(10000, mu = c(0, 0, 0), Sigma = vcov(test)/b)
sum(prior[,3] > 0)/10000
View(prior)
sum(post[,2] > 0)/10000
sum(prior[,2] > 0)/10000
1/0.4984
0.92/0.5009
View(prior)
vcov(test)
vcov(test[, 1:2])
vcov(test[, 2])
vcov(test)
sigma <- vcov(test)
sigma[1]
sigma[2:3, 2:3]
test$coefficients
test$coefficients[2:3]
vcov(test)[2:3, 2:3]
means <- test$coefficients[2:3]
sigma <- vcov(test)[2:3, 2:3]
post <- mvrnorm(10000, mu = means, Sigma = sigma)
View(post)
sum(post[,2] > 0)/10000
View(post)
b <- 5/400
prior <- mvrnorm(10000, mu = c(0, 0, 0), Sigma = sigma/b)
prior <- mvrnorm(10000, mu = c(0, 0), Sigma = sigma/b)
sum(prior[,2] > 0)/10000
sum(post[, 1] > 0)/10000
sum(prior[, 1] > 0)/10000
plot(post)
post <- mvrnorm(100000, mu = means, Sigma = sigma)
sum(post[, 1] > 0)/10000
b <- 5/400
sum(post[, 1] > 0)/100000
b <- 5/400
prior <- mvrnorm(100000, mu = c(0, 0), Sigma = sigma/b)
sum(prior[, 1] > 0)/100000
sum(post[, 2] > 0)/100000
sum(prior[, 2] > 0)/100000
